<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>曦</title>
  <icon>https://bigdata-yx.github.io/imgs/avatar.webp</icon>
  <subtitle>曦</subtitle>
  <link href="https://bigdata-yx.github.io/atom.xml" rel="self"/>
  
  <link href="https://bigdata-yx.github.io/"/>
  <updated>2025-01-13T08:12:05.000Z</updated>
  <id>https://bigdata-yx.github.io/</id>
  
  <author>
    <name>曦</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>集群搭建学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1081.html"/>
    <id>https://bigdata-yx.github.io/posts/1081.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据集群搭建学习路线"><a href="#大数据集群搭建学习路线" class="headerlink" title="大数据集群搭建学习路线"></a>大数据集群搭建学习路线</h1><h3 id="前提：熟练使用Linux的命令及其操作"><a href="#前提：熟练使用Linux的命令及其操作" class="headerlink" title="前提：熟练使用Linux的命令及其操作"></a>前提：熟练使用Linux的命令及其操作</h3><h3 id="0-CentosLinux的基操"><a href="#0-CentosLinux的基操" class="headerlink" title="0.CentosLinux的基操"></a>0.CentosLinux的基操</h3><p>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="1-Mysql数据库的入门学习"><a href="#1-Mysql数据库的入门学习" class="headerlink" title="1.Mysql数据库的入门学习"></a>1.Mysql数据库的入门学习</h3><p>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="2-Hadoop集群的搭建及其使用和Hive的使用"><a href="#2-Hadoop集群的搭建及其使用和Hive的使用" class="headerlink" title="2.Hadoop集群的搭建及其使用和Hive的使用"></a>2.Hadoop集群的搭建及其使用和Hive的使用</h3><p>【黑马程序员大数据入门到实战教程，大数据开发必会的Hadoop、Hive，云平台实战项目全套一网打尽】<a href="https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="3-Java的基础学习（上部）"><a href="#3-Java的基础学习（上部）" class="headerlink" title="3.Java的基础学习（上部）"></a>3.Java的基础学习（上部）</h3><p>【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="4-scala语言的学习"><a href="#4-scala语言的学习" class="headerlink" title="4.scala语言的学习"></a>4.scala语言的学习</h3><p>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="5-Python基础的学习"><a href="#5-Python基础的学习" class="headerlink" title="5.Python基础的学习"></a>5.Python基础的学习</h3><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="6-zookeeper组件的学习"><a href="#6-zookeeper组件的学习" class="headerlink" title="6.zookeeper组件的学习"></a>6.zookeeper组件的学习</h3><p>【黑马程序员Zookeeper视频教程，快速入门zookeeper技术】<a href="https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="7-kafka组件的利用"><a href="#7-kafka组件的利用" class="headerlink" title="7.kafka组件的利用"></a>7.kafka组件的利用</h3><p>【尚硅谷Kafka教程，2024新版kafka视频，零基础入门到实战】<a href="https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="8-Flume的使用"><a href="#8-Flume的使用" class="headerlink" title="8.Flume的使用"></a>8.Flume的使用</h3><p>【【海牛大数据】Flume教程（学Flume看这个一套就够了，从基础到扩展到实战案例到组件配合全都有）】<a href="https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="9-Sqoop的运用"><a href="#9-Sqoop的运用" class="headerlink" title="9.Sqoop的运用"></a>9.Sqoop的运用</h3><p>【【海牛大数据】Sqoop教程（命令详解、各组件融合、实战案例）】<a href="https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="10-Spark的搭建"><a href="#10-Spark的搭建" class="headerlink" title="10.Spark的搭建"></a>10.Spark的搭建</h3><h4 id="Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）"><a href="#Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）" class="headerlink" title="Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）"></a>Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）</h4><p><a href="https://blog.csdn.net/JunLeon/article/details/123625680">(超详细) Spark环境搭建（Local模式、 StandAlone模式、Spark On Yarn模式）-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_44480968/article/details/119580363">Spark配置高可用（HA）_spark ha-CSDN博客</a></p><h3 id="11-Flink搭建及其使用-由于python版的使用还不是特别完整"><a href="#11-Flink搭建及其使用-由于python版的使用还不是特别完整" class="headerlink" title="11.Flink搭建及其使用(由于python版的使用还不是特别完整)"></a>11.Flink搭建及其使用(由于python版的使用还不是特别完整)</h3><h4 id="scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="scala版：【【尚硅谷】Flink1.13教程（Scala版）】https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>scala版：【【尚硅谷】Flink1.13教程（Scala版）】<a href="https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h4 id="java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】<a href="https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h3 id="11-Redis的搭建及其使用"><a href="#11-Redis的搭建及其使用" class="headerlink" title="11.Redis的搭建及其使用"></a>11.Redis的搭建及其使用</h3><p>【黑马程序员Redis入门到实战教程，深度透析redis底层原理+redis分布式锁+企业解决方案+黑马点评实战项目】<a href="https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="12-Hbase的搭建及其使用"><a href="#12-Hbase的搭建及其使用" class="headerlink" title="12.Hbase的搭建及其使用"></a>12.Hbase的搭建及其使用</h3><p>【【好程序员】大数据全新分布式存储HBase精品课程】<a href="https://www.bilibili.com/video/BV1RM411i7XM?p=15&vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1RM411i7XM?p=15&amp;vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="13-Azkaban的搭建及其使用"><a href="#13-Azkaban的搭建及其使用" class="headerlink" title="13.Azkaban的搭建及其使用"></a>13.Azkaban的搭建及其使用</h3><p>【尚硅谷大数据Azkaban 3.x教程（全新发布）】<a href="https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据集群搭建学习路线&quot;&gt;&lt;a href=&quot;#大数据集群搭建学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据集群搭建学习路线&quot;&gt;&lt;/a&gt;大数据集群搭建学习路线&lt;/h1&gt;&lt;h3 id=&quot;前提：熟练使用Linux的命令及其操作&quot;&gt;&lt;a href=</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>数据处理学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1082.html"/>
    <id>https://bigdata-yx.github.io/posts/1082.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据数据处理学习路线"><a href="#大数据数据处理学习路线" class="headerlink" title="大数据数据处理学习路线"></a>大数据数据处理学习路线</h1><h2 id="第一阶段：基础部分"><a href="#第一阶段：基础部分" class="headerlink" title="第一阶段：基础部分"></a>第一阶段：基础部分</h2><ul><li>语言基础：python、java基础</li><li>数据库基础：mysql</li><li>linux：命令基础</li></ul><h2 id="第二部分：简单工具使用"><a href="#第二部分：简单工具使用" class="headerlink" title="第二部分：简单工具使用"></a>第二部分：简单工具使用</h2><ul><li>python第三方库：pandas、numpy、requests、bs4、jieba、snownlp</li><li>excel：函数使用以及操作</li><li>hadoop：hadoop的基本使用命令以及组件查看</li></ul><h2 id="第三部分：集群工具使用"><a href="#第三部分：集群工具使用" class="headerlink" title="第三部分：集群工具使用"></a>第三部分：集群工具使用</h2><ul><li>scala：scala语言基础</li><li>mapreduce：了解基本使用以及自定义方法</li><li>spark：掌握sparkcore sparksql</li><li>hive：了解hive命令以及udf自定义函数</li></ul><h2 id="第四部分：深入学习"><a href="#第四部分：深入学习" class="headerlink" title="第四部分：深入学习"></a>第四部分：深入学习</h2><ul><li>机器学习：了解机器学习原理以及sklearn使用</li><li>spark streaming：了解流式数据</li><li>项目制作：尝试制作大数据项目</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据数据处理学习路线&quot;&gt;&lt;a href=&quot;#大数据数据处理学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据数据处理学习路线&quot;&gt;&lt;/a&gt;大数据数据处理学习路线&lt;/h1&gt;&lt;h2 id=&quot;第一阶段：基础部分&quot;&gt;&lt;a href=&quot;#第一阶段：基础部</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop</title>
    <link href="https://bigdata-yx.github.io/posts/3983.html"/>
    <id>https://bigdata-yx.github.io/posts/3983.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    
    
    
    <category term="demo" scheme="https://bigdata-yx.github.io/categories/demo/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="demo" scheme="https://bigdata-yx.github.io/tags/demo/"/>
    
  </entry>
  
  <entry>
    <title>Spark RDD</title>
    <link href="https://bigdata-yx.github.io/posts/fa63.html"/>
    <id>https://bigdata-yx.github.io/posts/fa63.html</id>
    <published>2025-01-13T07:59:03.000Z</published>
    <updated>2025-01-13T07:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心"><a href="#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心" class="headerlink" title="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心"></a><strong>RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心</strong></h3><h4 id><a href="#" class="headerlink" title></a></h4><h4 id="Dataset-数据集"><a href="#Dataset-数据集" class="headerlink" title="Dataset:(数据集)"></a><strong>Dataset</strong><strong>:(数据集)</strong></h4><ul><li><strong>RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数；</strong></li><li><strong>RDD 也可以缓存起来, 相当于存储具体数据。</strong></li></ul><h4 id="Distributed-："><a href="#Distributed-：" class="headerlink" title="Distributed****："></a><strong>Distributed****：</strong></h4><h4 id="RDD-支持分区-可以运行在集群中。"><a href="#RDD-支持分区-可以运行在集群中。" class="headerlink" title="RDD 支持分区, 可以运行在集群中。"></a><strong>RDD 支持分区, 可以运行在集群中。</strong></h4><h4 id="Resilient-："><a href="#Resilient-：" class="headerlink" title="Resilient****："></a><strong>Resilient****：</strong></h4><ul><li><strong>RDD 支持高效的容错；</strong></li><li><strong>RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中。</strong></li></ul><h3 id="1-RDD的特点："><a href="#1-RDD的特点：" class="headerlink" title="1.RDD的特点："></a><strong>1.RDD的特点：</strong></h3><ul><li><p>弹性</p><ul><li>容错的弹性:数据丢失可以自动恢复;</li><li>存储的弹性:内存与磁盘的自动切换;</li><li>计算的弹性:计算出错重试机制;</li><li>分片的弹性:可根据需要重新分片。</li></ul></li><li><p>分布式:数据存储在集群不同节点上&#x2F;计算分布式。</p></li><li><p>数据集: RDD封装了计算逻辑，并不保存数据。</p></li><li><p>数据抽象: RDD是一个抽象类，需要子类具体实现。</p></li><li><p>不可变: RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑。</p></li><li><p>可分区、并行计算。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心&quot;&gt;&lt;a href=&quot;#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里</summary>
      
    
    
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/categories/Spark/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark on yarn</title>
    <link href="https://bigdata-yx.github.io/posts/fa64.html"/>
    <id>https://bigdata-yx.github.io/posts/fa64.html</id>
    <published>2025-01-13T07:59:03.000Z</published>
    <updated>2025-01-13T07:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-on-Yarn"><a href="#Spark-on-Yarn" class="headerlink" title="Spark on Yarn"></a>Spark on Yarn</h1><h3 id="SparkOnYarn本质"><a href="#SparkOnYarn本质" class="headerlink" title="SparkOnYarn本质"></a><strong>SparkOnYarn本质</strong></h3><p>master角色由yarn的Resourcemanager担任</p><p>worker角色由yarn的nodemanager担任</p><p>deiver角色运行在Yarn容器内或提交任务的客户端进程中</p><p>真正干活的Executor运行在yarn提供的容器内</p><h3 id="部署："><a href="#部署：" class="headerlink" title="部署："></a><strong>部署：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在spark-env.sh上只要添加这两个即可</span></span><br><span class="line">HADOOP_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line"><span class="comment"># 添加环境变量之后还要添加一个</span></span><br><span class="line"><span class="built_in">which</span> python</span><br><span class="line"><span class="built_in">export</span> SPARK_PYTHON=<span class="built_in">which</span> python</span><br><span class="line"><span class="comment"># 然后启动，bin/pyspark --master yarn</span></span><br></pre></td></tr></table></figure><h3 id="Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"><a href="#Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式" class="headerlink" title="Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"></a><strong>Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式</strong></h3><h3 id="这两种模式的区别就是Driver运行的位置"><a href="#这两种模式的区别就是Driver运行的位置" class="headerlink" title="这两种模式的区别就是Driver运行的位置"></a><strong>这两种模式的区别就是Driver运行的位置</strong></h3><h3 id="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"><a href="#集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内" class="headerlink" title="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"></a><strong>集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内</strong></h3><h3 id="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"><a href="#客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中" class="headerlink" title="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"></a><strong>客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端模式</span></span><br><span class="line">SPARK HOME=/export/server/spark<span class="variable">$&#123;SPARK HOME&#125;</span>/bin/spark-submit\--master yarn</span><br><span class="line">--deploy-mode client \（默认是客户端模式，不加也可以）</span><br><span class="line"><span class="comment"># 下面的参数可加可不加</span></span><br><span class="line">--driver-memory 512m \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line"><span class="variable">$&#123;SPARK HOME&#125;</span>/examples/src/main/python/pi.py 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群模式</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Spark-on-Yarn&quot;&gt;&lt;a href=&quot;#Spark-on-Yarn&quot; class=&quot;headerlink&quot; title=&quot;Spark on Yarn&quot;&gt;&lt;/a&gt;Spark on Yarn&lt;/h1&gt;&lt;h3 id=&quot;SparkOnYarn本质&quot;&gt;&lt;a hre</summary>
      
    
    
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/categories/Spark/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark搭建</title>
    <link href="https://bigdata-yx.github.io/posts/fa65.html"/>
    <id>https://bigdata-yx.github.io/posts/fa65.html</id>
    <published>2025-01-13T07:59:03.000Z</published>
    <updated>2025-01-13T07:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark安装部署"><a href="#spark安装部署" class="headerlink" title="spark安装部署"></a>spark安装部署</h1><h3 id="先安装anacondea3然后再解压spark"><a href="#先安装anacondea3然后再解压spark" class="headerlink" title="先安装anacondea3然后再解压spark"></a><strong>先安装anacondea3然后再解压spark</strong></h3><h3 id="然后追加以下内容至-x2F-root-x2F-condarc"><a href="#然后追加以下内容至-x2F-root-x2F-condarc" class="headerlink" title="然后追加以下内容至&#x2F;root&#x2F; .condarc"></a><strong>然后追加以下内容至&#x2F;root&#x2F; .condarc</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - defaults</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/fastai/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br></pre></td></tr></table></figure><h3 id="配置pyspark"><a href="#配置pyspark" class="headerlink" title="配置pyspark"></a><strong>配置pyspark</strong></h3><h3 id="conda-create-n-pyspark-python-x3D-版本号然后回车就下载了"><a href="#conda-create-n-pyspark-python-x3D-版本号然后回车就下载了" class="headerlink" title="conda create -n pyspark python&#x3D;版本号然后回车就下载了"></a><strong>conda create -n pyspark python&#x3D;版本号然后回车就下载了</strong></h3><h3 id="安装完之后conda-activate-pyspark切换虚拟环境"><a href="#安装完之后conda-activate-pyspark切换虚拟环境" class="headerlink" title="安装完之后conda activate pyspark切换虚拟环境"></a><strong>安装完之后conda activate pyspark切换虚拟环境</strong></h3><h1 id="Local环境部署"><a href="#Local环境部署" class="headerlink" title="Local环境部署"></a><strong>Local环境部署</strong></h1><h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a><strong>添加环境变量</strong></h3><h2 id="如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath"><a href="#如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath" class="headerlink" title="如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)"></a><strong>如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)</strong></h2><h3 id="然后就可以启动了pysparkspark-shell"><a href="#然后就可以启动了pysparkspark-shell" class="headerlink" title="然后就可以启动了pysparkspark shell"></a><strong>然后就可以启动了pysparkspark shell</strong></h3><h3 id="运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动"><a href="#运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动" class="headerlink" title="运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动"></a><strong>运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动</strong></h3><h3 id="Spark集群搭建"><a href="#Spark集群搭建" class="headerlink" title="Spark集群搭建"></a>Spark集群搭建</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br><span class="line">    <span class="comment"># 指定 Java Home</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/export/servers/jdk1.8.0_221</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定 Spark Master 地址</span></span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_HOST=node01</span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> works.template works</span><br><span class="line">    将Localhost删了换成master slave1 slave2</span><br><span class="line">    </span><br><span class="line">配置 HistoryServer</span><br><span class="line"><span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vi spark-defaults.conf (将这两个前面<span class="comment">#去掉)</span></span><br><span class="line">    spark.eventLog.enabled  <span class="literal">true</span></span><br><span class="line">    spark.eventLog.<span class="built_in">dir</span>      hdfs://node01:8020/spark_log</span><br><span class="line">vi spark-env.sh末尾添加</span><br><span class="line">    <span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:8020/spark_log&quot;</span></span><br><span class="line"> 创建hdfs目录</span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /spark_log</span><br><span class="line"></span><br><span class="line">修改log4j2.properties.template改名为log4j2.properties然后将19行的info改为WARN</span><br><span class="line"></span><br><span class="line">然后分发</span><br><span class="line">启动历史服务器：sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">最后启动</span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">连接，在webui的那个</span><br><span class="line">pyspark --master spark://master:7077</span><br></pre></td></tr></table></figure><h2 id="spark基于zookeeper实现HA"><a href="#spark基于zookeeper实现HA" class="headerlink" title="spark基于zookeeper实现HA"></a><strong>spark基于zookeeper实现HA</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前提确保zookeeper和hdfs均已启动</span><br><span class="line">vi spark-env.sh文件</span><br><span class="line">将<span class="built_in">export</span> SPARK_MASTER_HOST=master和他的那个端口注释了</span><br><span class="line">追加SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181,hadoop03:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span><br><span class="line">然后重新启动spark并且再启动另外一台或者多台机的start-master.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;spark安装部署&quot;&gt;&lt;a href=&quot;#spark安装部署&quot; class=&quot;headerlink&quot; title=&quot;spark安装部署&quot;&gt;&lt;/a&gt;spark安装部署&lt;/h1&gt;&lt;h3 id=&quot;先安装anacondea3然后再解压spark&quot;&gt;&lt;a href=&quot;#先安</summary>
      
    
    
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/categories/Spark/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Flume source的type</title>
    <link href="https://bigdata-yx.github.io/posts/fe55.html"/>
    <id>https://bigdata-yx.github.io/posts/fe55.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="不支持断点续传"><a href="#不支持断点续传" class="headerlink" title="不支持断点续传"></a>不支持断点续传</h5><h1 id="spooling-directory-source"><a href="#spooling-directory-source" class="headerlink" title="spooling directory source"></a>spooling directory source</h1><h2 id="监听某一个目录，只要目录下有文件，文件中的数据就会收集"><a href="#监听某一个目录，只要目录下有文件，文件中的数据就会收集" class="headerlink" title="监听某一个目录，只要目录下有文件，文件中的数据就会收集"></a>监听某一个目录，只要目录下有文件，文件中的数据就会收集</h2><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; spooldir a1.sources.r1.spoolDir &#x3D; &#x2F;root&#x2F;spool</p><p>a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 10000 a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>注意Dir大写</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h5 id=&quot;不支持断点续传&quot;&gt;&lt;a href=&quot;#不支持断点续传&quot; class=&quot;headerlink&quot; title=&quot;不支持断点续传&quot;&gt;&lt;/a&gt;不支持断点续传&lt;/h5&gt;&lt;h1 id=&quot;spooling-directory-source&quot;&gt;&lt;a href=&quot;#spooling</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume Put事务和Take事务</title>
    <link href="https://bigdata-yx.github.io/posts/fe67.html"/>
    <id>https://bigdata-yx.github.io/posts/fe67.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">在source和chanel的传输中是批量过去的，channels传输到sinks也是批量的</span><br><span class="line"></span><br><span class="line">在source到channel中呢就有一个缓冲区</span><br><span class="line"></span><br><span class="line">1.doPut操作，浆皮数据写入到临时缓冲区 Putlist中</span><br><span class="line"></span><br><span class="line">2.doCommit操作：检查channel队列中是否有足够空间用来存放数据 有的话就会执行doCommit操作</span><br><span class="line"></span><br><span class="line">3.如果channel空间不够就会执行回滚数据的操作（doRollBack）</span><br><span class="line"></span><br><span class="line">在channel到sink中的事务叫take事务，也是批量过去的</span><br><span class="line"></span><br><span class="line">1.doTake操作，拉取一批channel的数据，然后进入缓冲区</span><br><span class="line"></span><br><span class="line">2.takeList操作，临时缓冲</span><br><span class="line"></span><br><span class="line">3.doCommit操作，将这一批数据发送出去，发送失败就会回滚</span><br><span class="line"></span><br><span class="line">4.doRollBack发送失败就进行回滚</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>案例：Flume avro sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe65.html"/>
    <id>https://bigdata-yx.github.io/posts/fe65.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="avro-sink"><a href="#avro-sink" class="headerlink" title="avro sink"></a>avro sink</h2><p>需要配置的：type &#x3D; avro</p><p>hostname &#x3D; 主机名</p><p>port  &#x3D; 端口</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; avro<br>a1.sources.r1.bind &#x3D; 192.168.1.122<br>a1.sources.r1.port &#x3D; 55555&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1 c2<br>a1.sinks &#x3D; k1 k2</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100<br>a1.channels.c2.type &#x3D; memory<br>a1.channels.c2.capacity &#x3D; 10000<br>a1.channels.c2.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; avro<br>a1.sinks.k1.hostname &#x3D; 192.168.1.123<br>a1.sinks.k1.port &#x3D; 55555<br>a1.sinks.k2.type &#x3D; avro<br>=&#x3D;a1.sinks.k2.hostname &#x3D; 192.168.1.124<br>a1.sinks.k2.port &#x3D; 55555</p><p>a1.sources.r1.channels &#x3D; c1 c2<br>a1.sinks.k1.channel &#x3D; c1<br>a1.sinks.k2.channel &#x3D; c2</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;avro-sink&quot;&gt;&lt;a href=&quot;#avro-sink&quot; class=&quot;headerlink&quot; title=&quot;avro sink&quot;&gt;&lt;/a&gt;avro sink&lt;/h2&gt;&lt;p&gt;需要配置的：type &amp;#x3D; avro&lt;/p&gt;
&lt;p&gt;hostname &amp;#x</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>案例：Flume file channel</title>
    <link href="https://bigdata-yx.github.io/posts/fe63.html"/>
    <id>https://bigdata-yx.github.io/posts/fe63.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="file-channel"><a href="#file-channel" class="headerlink" title="file channel"></a>file channel</h2><p>需要的参数 type &#x3D; file</p><p>dataDirs &#x3D; &#x2F;roort</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.channels.c1.type &#x3D; file&#x3D;&#x3D;<br>a1.channels.c1.capaciry&#x3D;10000<br>a1.channels.c1.transactioncapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;file-channel&quot;&gt;&lt;a href=&quot;#file-channel&quot; class=&quot;headerlink&quot; title=&quot;file channel&quot;&gt;&lt;/a&gt;file channel&lt;/h2&gt;&lt;p&gt;需要的参数 type &amp;#x3D; file&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>案例：Flume exec sources监听命令</title>
    <link href="https://bigdata-yx.github.io/posts/fe64.html"/>
    <id>https://bigdata-yx.github.io/posts/fe64.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="exec-sources"><a href="#exec-sources" class="headerlink" title="exec sources"></a>exec sources</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; exec</p><p>a1.sources.r1.command &#x3D; tail -f &#x2F;root&#x2F;exec.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>那个命令就是需要监听的命令</p><p>他是不断的去监听你文件添加了什么</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;exec-sources&quot;&gt;&lt;a href=&quot;#exec-sources&quot; class=&quot;headerlink&quot; title=&quot;exec sources&quot;&gt;&lt;/a&gt;exec sources&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;a1.sources &amp;#x3D;</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume file_roll sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe62.html"/>
    <id>https://bigdata-yx.github.io/posts/fe62.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="File-roll-sink"><a href="#File-roll-sink" class="headerlink" title="File_roll sink"></a>File_roll sink</h2><p>必须的：type &#x3D; file_roll</p><p>sink.directory 保存在那个目录&amp;#x20;</p><p>非必须：sink.rollInterval &#x3D; 30 就是每过30s就会生成一个新的文件用来存储数据</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; file_roll</p><p>a1.sinks.k1.sink.directory &#x3D; &#x2F;root&#x2F;file_roll&amp;#x20;</p><p>a1.sinks.k1.sink.rollInterval &#x3D; 10</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;File-roll-sink&quot;&gt;&lt;a href=&quot;#File-roll-sink&quot; class=&quot;headerlink&quot; title=&quot;File_roll sink&quot;&gt;&lt;/a&gt;File_roll sink&lt;/h2&gt;&lt;p&gt;必须的：type &amp;#x3D; file_r</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume hdfs sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe60.html"/>
    <id>https://bigdata-yx.github.io/posts/fe60.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hdfs-sink"><a href="#hdfs-sink" class="headerlink" title="hdfs sink"></a>hdfs sink</h2><p>hdfs sink是将flume收集到的数据写入到hdfs中，方便数据可靠的保存</p><p>sink输出到hdfs中，默认每10个event生成一个hdfs文件，hdfs文件目录会根据hdfs.path的配置自动创建</p><p>配置参数：</p><p>hdfs.pathhdfs目录路径</p><p>hdsf.filePrefix文件前缀，默认值是FlumeData</p><p>hdfs.fileSuffix文件后缀</p><p>hdfs.rollnterval就是滚动，设置多长时间创建一个新文件进行存储，默认是30s</p><p>hdfs.rollSize文件大小超过一定值后，然后再创建一个新文件进行存储，默认是1024</p><p>hdfs.rollCount写入了多少个事件然后再创建一个新文件进行存储，默认是10个，设置为0的话表示不基于事件个数</p><p>hdfs.file.Type文件格式，有三种格式可选择：SequenceFile（默认，二进制），DataStream(不压缩,以文本的形式【方便观察】)，CompressedStream（可压缩）</p><p>hdfs.batchSize批数次，HDFS Sink每次从Channel中拿的事件个数。默认值100（就是每100个事件就从sink中传到hdfs上一次）## 一般不设置</p><p>hdfs.maxOpenFiles允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭 ## 一般不设置</p><p>hdfs.callTimeouthdfs允许操作的事件，比如hdfs文件的open, write, flush, close操作，单位是ms，默认值10000</p><p>hdfs.codeC压缩编解码器。以下之一：gzip, bzip2, lzo, lzop, snappy</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; hdfs <br>a1.sinks.k1.hdfs.path &#x3D; &#x2F;data&#x2F;hkjcpdd&#x2F;%Y-%m-%d <br>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true <br>a1.sinks.k1.hdfs.rollInterval&#x3D; 10 <br>a1.sinks.k1.hdfs.fileType &#x3D; DataStream</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true这个是使用本地的事件戳</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;hdfs-sink&quot;&gt;&lt;a href=&quot;#hdfs-sink&quot; class=&quot;headerlink&quot; title=&quot;hdfs sink&quot;&gt;&lt;/a&gt;hdfs sink&lt;/h2&gt;&lt;p&gt;hdfs sink是将flume收集到的数据写入到hdfs中，方便数据可靠的保存&lt;/</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume http source 监听Http</title>
    <link href="https://bigdata-yx.github.io/posts/fe59.html"/>
    <id>https://bigdata-yx.github.io/posts/fe59.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="http-source-监听Http"><a href="#http-source-监听Http" class="headerlink" title="http source 监听Http"></a>http source 监听Http</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; http<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试办法curl -X POST -d ‘[{“headers”:{“key”:”Flume”},”body”:”TestEvent1”}]’ <a href="http://master:4141/">http://master:4141/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;http-source-监听Http&quot;&gt;&lt;a href=&quot;#http-source-监听Http&quot; class=&quot;headerlink&quot; title=&quot;http source 监听Http&quot;&gt;&lt;/a&gt;http source 监听Http&lt;/h2&gt;&lt;blockquo</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume kafka channel</title>
    <link href="https://bigdata-yx.github.io/posts/fe58.html"/>
    <id>https://bigdata-yx.github.io/posts/fe58.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kafka-channel"><a href="#kafka-channel" class="headerlink" title="kafka channel"></a>kafka channel</h2><p>需要指定的</p><blockquote><p>type &#x3D; org.apache.flume.channel.kafka.KafkaChannel</p><p>kafka.bootstrap.server &#x3D; hostname:port</p><p>kafka.topic &#x3D; hkjcpdd</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; org.apache.flume.channel.kafka.KafkaChannel <br>a1.channels.c1.kafka.bootstrap.servers &#x3D; master:9092</p><p>a1.channels.c1.kafka.topic&#x3D;hkjmjj</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;kafka-channel&quot;&gt;&lt;a href=&quot;#kafka-channel&quot; class=&quot;headerlink&quot; title=&quot;kafka channel&quot;&gt;&lt;/a&gt;kafka channel&lt;/h2&gt;&lt;p&gt;需要指定的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume Sink Processors sink处理器</title>
    <link href="https://bigdata-yx.github.io/posts/fe66.html"/>
    <id>https://bigdata-yx.github.io/posts/fe66.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sink-Processors-sink处理器"><a href="#Sink-Processors-sink处理器" class="headerlink" title="Sink Processors sink处理器"></a>Sink Processors sink处理器</h2><h3 id="failover-sink-Processor故障转移处理器"><a href="#failover-sink-Processor故障转移处理器" class="headerlink" title="failover sink Processor故障转移处理器"></a><em><strong>failover sink Processor故障转移处</strong>理器</em></h3><h4 id="amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"><a href="#amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力" class="headerlink" title="&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"></a>&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力</h4><p>&amp;#x9;2)需要指定: processor.type  &#x3D; failover</p><p>&amp;#x9;processor.priority.&lt;sinkName&gt; &#x3D; 数字（就是优先级，数字越大优先级越高）</p><p>&amp;#x9;processor.maxpenalty &#x3D; 10000（就是允许你宕机以后给你重连的时间）</p><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>负载平衡处理器提供了在多个sink负载平衡流量的能力。支持两种模式：round<em>robin and random。round</em>_robin可以将数据负载均衡到多个sink上，random支持随机分发到不同的sink上</p><p>第一种就是随机给你发送（random随机）</p><p>第二种就是负载均衡（robin负载均衡）</p><blockquote><p>a1.sources&#x3D;r1 <br>a1.sinks&#x3D;k1 k2 <br>a1.channels&#x3D;c1 <br><br>a1.sources.r1.type&#x3D;netcat <br>a1.sources.r1.bind&#x3D;worker-1 <br>a1.sources.r1.port&#x3D;44444 <br><br>a1.channels.c1.type&#x3D;memory <br>a1.channels.c1.capacity&#x3D;100000 <br>a1.channels.c1.transactionCapacity&#x3D;100 <br><br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-1<br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-2 <br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinkgroups &#x3D; g1 <br>a1.sinkgroups.g1.sinks &#x3D; k1 k2 <br>a1.sinkgroups.g1.processor.type &#x3D; failover(故障转移)   or    load_<em>balance(随机)<br>a1.sinkgroups.g1.processor.selector &#x3D; random or    round</em>_<em>robin# 默认是故障转移的不写就是故障转移，写了就是随机random是随机的意思，而round</em>_robin是轮询的意思<br><br>a1.sources.r1.channels&#x3D;c1 <br>a1.sinks.k1.channel&#x3D;c1 <br>a1.sinks.k2.channel&#x3D;c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Sink-Processors-sink处理器&quot;&gt;&lt;a href=&quot;#Sink-Processors-sink处理器&quot; class=&quot;headerlink&quot; title=&quot;Sink Processors sink处理器&quot;&gt;&lt;/a&gt;Sink Processors s</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume kafka sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe57.html"/>
    <id>https://bigdata-yx.github.io/posts/fe57.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kafka-sink"><a href="#kafka-sink" class="headerlink" title="kafka sink"></a>kafka sink</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; org.apache.flume.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.server &#x3D; master<br>a1.sinks.k1.kafka.topic &#x3D; hkjcpdd&#x3D;&#x3D;</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>flume监听nginx的access.log文件给kafka消费</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; taildir<br>a1.sources.r1.filegroups &#x3D; f1<br>a1.sources.r1.filegroups.f1 &#x3D; &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;access.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.servers &#x3D; master:9092<br>a1.sinks.k1.kafka.topic&#x3D;hkjcpdd</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;kafka-sink&quot;&gt;&lt;a href=&quot;#kafka-sink&quot; class=&quot;headerlink&quot; title=&quot;kafka sink&quot;&gt;&lt;/a&gt;kafka sink&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;a1.sources &amp;#x3D; r1&lt;br&gt;a</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume source的type</title>
    <link href="https://bigdata-yx.github.io/posts/fe56.html"/>
    <id>https://bigdata-yx.github.io/posts/fe56.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。</p><p>spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文件，并在新文件处显示从新文件中解析出来。 与exec source不同，spooling directory source是可靠的， 即使flume重新启动或被kill，也不会丢失数据，同时作为这种可靠的代价，指定目录中的被手机的文件必须是不可变的、唯一命名的。flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常；</p><p>taildir source 监控指定的一些文件，并在检测新的一行数据残生的时候几乎实时的读取他们，如果新的一行数据还没写完，taildir source 等到这行写完后读取</p><p>kafka source 就是一个apache kafka消费者， 他从kafka的topic中读取消息，如果运行了多个Kafka source 则可以把他们配置到同一个消费者组，以便每个source都读取一组唯一的topic分区</p><p>syslog sources 针对系统日志</p><p>http sources 发送get协议请求的</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。&lt;/p&gt;
&lt;p&gt;spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume</title>
    <link href="https://bigdata-yx.github.io/posts/fe69.html"/>
    <id>https://bigdata-yx.github.io/posts/fe69.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume Channel Selectors通道选择器</title>
    <link href="https://bigdata-yx.github.io/posts/fe69.html"/>
    <id>https://bigdata-yx.github.io/posts/fe69.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Channel-Selectors通道选择器"><a href="#Channel-Selectors通道选择器" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><h3 id="Channel-Selectors通道选择器-1"><a href="#Channel-Selectors通道选择器-1" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><p>多路复用通道选择器，source是通过event header来决定传输到哪一个channel。source是通过event header来决定传输到哪一个channel</p><p><img src="https://pic1.imgdb.cn/item/6784c37dd0e0a243d4f3d664.png"></p><p><strong>replicating type是复制选择器</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Channel-Selectors通道选择器&quot;&gt;&lt;a href=&quot;#Channel-Selectors通道选择器&quot; class=&quot;headerlink&quot; title=&quot;Channel Selectors通道选择器&quot;&gt;&lt;/a&gt;&lt;strong&gt;Channel Sele</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume的netcat 监听端口</title>
    <link href="https://bigdata-yx.github.io/posts/fe61.html"/>
    <id>https://bigdata-yx.github.io/posts/fe61.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="netcat-agent"><a href="#netcat-agent" class="headerlink" title="netcat agent"></a>netcat agent</h1><p>配置文件如下</p><blockquote><p>a1.sources &#x3D; r1&amp;#x20;</p><p>a1.channels &#x3D; c1&amp;#x20;</p><p>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat&amp;#x20;</p><p>=&#x3D;a1.sources.r1.bind &#x3D; localhost &#x3D;&#x3D;</p><p>=&#x3D;a1.sources.r1.port &#x3D; 44444&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory&amp;#x20;</p><p>a1.channels.c1.capacity &#x3D; 10000&amp;#x20;</p><p>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试方法：telnet localhost 44444</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;netcat-agent&quot;&gt;&lt;a href=&quot;#netcat-agent&quot; class=&quot;headerlink&quot; title=&quot;netcat agent&quot;&gt;&lt;/a&gt;netcat agent&lt;/h1&gt;&lt;p&gt;配置文件如下&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a1.</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume Interceptor</title>
    <link href="https://bigdata-yx.github.io/posts/fe68.html"/>
    <id>https://bigdata-yx.github.io/posts/fe68.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"><a href="#就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理" class="headerlink" title="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"></a>就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理</h4><p>timestamp Interceptor给event的头信息中添加时间戳</p><p>Static Interceptor 给event的头信息中添加自定义键值</p><p>Host Interceptor给event的头信息中添加主机名或者ip信息</p><p>Search and Replace Interceptor拦截信息进行匹配和替换</p><p>Regex File</p><h2 id="timestamp-interceptor-添加时间戳"><a href="#timestamp-interceptor-添加时间戳" class="headerlink" title="timestamp interceptor(添加时间戳)"></a>timestamp interceptor(添加时间戳)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; timestamp&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="host-interceptor（添加主机信息）"><a href="#host-interceptor（添加主机信息）" class="headerlink" title="host interceptor（添加主机信息）"></a>host interceptor（添加主机信息）</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1 &#x3D;&#x3D;<br>=&#x3D;a1.sources.r1.interceptors.i1.type &#x3D; host&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Static-Interceptor"><a href="#Static-Interceptor" class="headerlink" title="Static Interceptor"></a>Static Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.sources.r1.interceptors  &#x3D; i1 i2 i3</p><p>a1.sources.r1.interceptors.i3.type &#x3D; static<br>a1.sources.r1.interceptors.i3.key &#x3D; name<br>a1.sources.r1.interceptors.i3.value &#x3D; zhangsan</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Search-and-Replace-Interceptor"><a href="#Search-and-Replace-Interceptor" class="headerlink" title="Search and Replace Interceptor"></a>Search and Replace Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; search_replace<br>a1.sources.r1.interceptors.i1.searchPattern&#x3D;[a-z]<br>a1.sources.r1.interceptors.i1.replaceString&#x3D;*&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h2 id="Regex-Filering-Interceptor-过滤的用正则"><a href="#Regex-Filering-Interceptor-过滤的用正则" class="headerlink" title="Regex Filering Interceptor(过滤的用正则)"></a>Regex Filering Interceptor(过滤的用正则)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_filter<br>a1.sources.r1.interceptors.i1.regex&#x3D;^jp.*<br>a1.sources.r1.interceptors.i1.excludeEvents&#x3D;true&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Regex-Extractor-Interceptor-通过正则对event进行捕获"><a href="#Regex-Extractor-Interceptor-通过正则对event进行捕获" class="headerlink" title="Regex Extractor Interceptor(通过正则对event进行捕获)"></a>Regex Extractor Interceptor(通过正则对event进行捕获)</h3><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_extractor<br>a1.sources.r1.interceptors.i1.regex&#x3D;(^[a-zA-Z]&#x3D;&#x3D;<em>&#x3D;&#x3D;)\s([0-9]&#x3D;&#x3D;</em>&#x3D;&#x3D;$)<br>a1.sources.r1.interceptors.i1.serializers&#x3D;s1 s2<br>a1.sources.r1.interceptors.i1.serializers.s1.name&#x3D;word<br>a1.sources.r1.interceptors.i1.serializers.s2.name&#x3D;num&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理&quot;&gt;&lt;a href=&quot;#就是拦截器，拦截器可以将flume收集到的event进行拦</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume taildir agent监听一个或者多个文件</title>
    <link href="https://bigdata-yx.github.io/posts/fe54.html"/>
    <id>https://bigdata-yx.github.io/posts/fe54.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="taildir-agent"><a href="#taildir-agent" class="headerlink" title="taildir agent"></a>taildir agent</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; taildir<br># 这个是用于断点续传，确保不被重复消费<br>a1.sources.r1.positionFile&#x3D;&#x2F;data&#x2F;flume&#x2F;position.json<br>a1.sources.r1.filegroups &#x3D; f1 a1.sources.r1.filegroups.f1 &#x3D; &#x2F;root&#x2F;taildir&#x2F;hkjcpdd.log&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>与之exev不同的是，他是监听的是文件内容，而exec是监听命令运行后的结果</p><p>监听多个文件的话那就多写几个</p><p>例如:</p><blockquote></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;taildir-agent&quot;&gt;&lt;a href=&quot;#taildir-agent&quot; class=&quot;headerlink&quot; title=&quot;taildir agent&quot;&gt;&lt;/a&gt;taildir agent&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;a1.sources &amp;#</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume</title>
    <link href="https://bigdata-yx.github.io/posts/fe52.html"/>
    <id>https://bigdata-yx.github.io/posts/fe52.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784c673d0e0a243d4f3d73f.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6784c673d0e0a243d4f3d73f.png&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>案例：将nginx的日志实时传输到hdfs和kafka上</title>
    <link href="https://bigdata-yx.github.io/posts/fe66.html"/>
    <id>https://bigdata-yx.github.io/posts/fe66.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>案例：将nginx的日志实时传输到hdfs和kafka上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type=taildir</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1=/usr/local/nginx/logs/access.log</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line">a1.channels.c1.capacity=10000</span><br><span class="line">a1.channels.c1.transactionCapacity=100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type=memory</span><br><span class="line">a1.channels.c2.capacity=10000</span><br><span class="line">a1.channels.c2.transactionCapacity=100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=/data/hkjcpdd/%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=true</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval=10</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k2.kafka.bootstrap.servers = master:9092</span><br><span class="line">a1.sinks.k2.kafka.topic = hkjcpdd</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;案例：将nginx的日志实时传输到hdfs和kafka上&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;s</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban</title>
    <link href="https://bigdata-yx.github.io/posts/969.html"/>
    <id>https://bigdata-yx.github.io/posts/969.html</id>
    <published>2025-01-13T07:24:22.000Z</published>
    <updated>2025-01-13T07:24:22.000Z</updated>
    
    
    
    
    <category term="demo" scheme="https://bigdata-yx.github.io/categories/demo/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="demo" scheme="https://bigdata-yx.github.io/tags/demo/"/>
    
  </entry>
  
  <entry>
    <title>AzkabanAzkaban搭建</title>
    <link href="https://bigdata-yx.github.io/posts/968.html"/>
    <id>https://bigdata-yx.github.io/posts/968.html</id>
    <published>2025-01-13T07:24:22.000Z</published>
    <updated>2025-01-13T07:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D-1024M并重启mysql服务"><a href="#1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D-1024M并重启mysql服务" class="headerlink" title="1.在搭建之前得有mysql环境,以及my.cnf文件中添加max_allowed_packet&#x3D;1024M并重启mysql服务"></a>1.在搭建之前得有mysql环境,以及my.cnf文件中添加max_allowed_packet&#x3D;1024M并重启mysql服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置mysql相关部分(db)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入Mysql然后创建azkaban库</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入库并<span class="built_in">source</span>进来azkaban-db/create-all-sql-3.84.4.sql</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改以下配置项(<span class="built_in">exec</span>)</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">executor.port=12321</span><br><span class="line">jetty.port=8061</span><br><span class="line">azkaban.webserver.url=http://master:8061</span><br><span class="line"></span><br><span class="line">mysql.host=master</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进行分发到其他机器，然后每台都要启动</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">最后进行激活</span></span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;master:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;slave1:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;slave2:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改一下配置项(web)</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">jetty.port=8061</span><br><span class="line"></span><br><span class="line">mysql.host=master</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置azkaban-users.xml</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加一行即可</span></span><br><span class="line">&lt;user password=&quot;123456&quot; roles=&quot;admin&quot; username=&quot;hkjcpdd&quot;/&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后启动即可</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D-1024M并重启mysql服务&quot;&gt;&lt;a href=&quot;#1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D</summary>
      
    
    
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/categories/Azkaban/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban的最简单案例</title>
    <link href="https://bigdata-yx.github.io/posts/967.html"/>
    <id>https://bigdata-yx.github.io/posts/967.html</id>
    <published>2025-01-13T07:24:22.000Z</published>
    <updated>2025-01-13T07:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># first.project</span><br><span class="line">azkaban-flow-version: <span class="number">2</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># first.flow</span><br><span class="line"><span class="function">nodes:</span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobA</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">hkjcpdd</span>&quot;</span></span><br></pre></td></tr></table></figure><p>然后将这两个打包成zip然后上传至webui界面提交即可</p>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight cmd&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;t</summary>
      
    
    
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/categories/Azkaban/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban的JavaProcess作业案例</title>
    <link href="https://bigdata-yx.github.io/posts/966.html"/>
    <id>https://bigdata-yx.github.io/posts/966.html</id>
    <published>2025-01-13T07:24:22.000Z</published>
    <updated>2025-01-13T07:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="JavaProcess作业类型案例"><a href="#JavaProcess作业类型案例" class="headerlink" title="JavaProcess作业类型案例"></a>JavaProcess作业类型案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JavaProcess类型可以运行一个自定义主类方法，<span class="built_in">type</span>类型为javaprocess,可用的配置为：</span></span><br><span class="line"></span><br><span class="line">​Xms: 最小堆</span><br><span class="line">​Xmx: 最大堆</span><br><span class="line">​classpath: 类路径</span><br><span class="line">​java.class: 要运行的Java对象，其中必须包含Main方法</span><br><span class="line">​main.args: main方法的参数</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">案例：</span></span><br><span class="line"></span><br><span class="line">1)新建一个azkaban的maven工程</span><br><span class="line">2）创建包名：com.hkjcpdd</span><br><span class="line">3)创建AzTest类</span><br><span class="line">4)打包jar包azkaban.jar</span><br><span class="line">5)新建testJava.flow</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">five.flow</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: test_java</span><br><span class="line">    type: javaprocess</span><br><span class="line">    config:</span><br><span class="line">      Xms: 96M</span><br><span class="line">      Xmx: 200M</span><br><span class="line">      java.class: com.hkjcpdd.TestJavaProcess</span><br><span class="line">      </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后将flow和jar和project打包在一起上传即可</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;JavaProcess作业类型案例&quot;&gt;&lt;a href=&quot;#JavaProcess作业类型案例&quot; class=&quot;headerlink&quot; title=&quot;JavaProcess作业类型案例&quot;&gt;&lt;/a&gt;JavaProcess作业类型案例&lt;/h2&gt;&lt;figure class</summary>
      
    
    
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/categories/Azkaban/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban失败重试</title>
    <link href="https://bigdata-yx.github.io/posts/958.html"/>
    <id>https://bigdata-yx.github.io/posts/958.html</id>
    <published>2025-01-13T07:24:22.000Z</published>
    <updated>2025-01-13T07:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">自动失败重试</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">  type: command</span><br><span class="line">  config:</span><br><span class="line">    command: sh /not exists.sh</span><br><span class="line">    retries: 3</span><br><span class="line">    retry.backoff: 10000</span><br><span class="line">    </span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">参数说明：</span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">retries: 重试次数</span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">retry.backoff: 重试的时间间隔</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">手动失败重试</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">理想的就是从成功的地方跳过，失败的地方重试</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">需求：JobA =&gt; JobB(依赖于A) =&gt; JobC =&gt; JobD =&gt; JobE =&gt; JobF。生产环境中，任何Job都有可能挂掉，可以根据需求执行想要的Job</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">直接从webui操作</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.<span class="built_in">history</span>-&gt;flow-&gt;Prepare execution-&gt;execute</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2.就是从头开始，但是把成功的<span class="built_in">disable</span>就可以了</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class</summary>
      
    
    
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/categories/Azkaban/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban作业依赖</title>
    <link href="https://bigdata-yx.github.io/posts/965.html"/>
    <id>https://bigdata-yx.github.io/posts/965.html</id>
    <published>2025-01-13T07:24:22.000Z</published>
    <updated>2025-01-13T07:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># second.flow</span><br><span class="line"><span class="function">nodes:</span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobA</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">aaa</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobB</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">bbb</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobC</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">dependsOn</span>:</span></span><br><span class="line"><span class="function">        - <span class="title">jobA</span></span></span><br><span class="line"><span class="function">        - <span class="title">jobB</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">ccc</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function"> # 添加<span class="title">dependsOn</span>就可以了，这个用处就是等<span class="title">AB</span>完成后再执行<span class="title">C</span></span></span><br><span class="line"><span class="function"> # 然后将<span class="title">second.fow</span>和<span class="title">first.project</span>打包<span class="title">zip</span>上传至<span class="title">webui</span>即可</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight cmd&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;</summary>
      
    
    
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/categories/Azkaban/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban邮件报警案例</title>
    <link href="https://bigdata-yx.github.io/posts/964.html"/>
    <id>https://bigdata-yx.github.io/posts/964.html</id>
    <published>2025-01-13T07:24:22.000Z</published>
    <updated>2025-01-13T07:24:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="邮件报警案例"><a href="#邮件报警案例" class="headerlink" title="邮件报警案例"></a>邮件报警案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">得先从邮箱那里开启STMP的服务，拿到授权码</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进入<span class="variable">$azkaban</span>-web/conf/azkaban.properties</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改22和23行</span></span><br><span class="line">mail.sender=邮箱地址</span><br><span class="line">mail.host=smtp.qq.com</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加上</span></span><br><span class="line">mail.user=邮箱地址</span><br><span class="line">mail.password=授权码</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新启动azkaban-web</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动任务的地方左侧有个Notification（通知）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后将邮箱地址放进那两个框，一个是失败发送给谁，一个是成功发送给谁，可以指定多个</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;邮件报警案例&quot;&gt;&lt;a href=&quot;#邮件报警案例&quot; class=&quot;headerlink&quot; title=&quot;邮件报警案例&quot;&gt;&lt;/a&gt;邮件报警案例&lt;/h2&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut</summary>
      
    
    
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/categories/Azkaban/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Azkaban" scheme="https://bigdata-yx.github.io/tags/Azkaban/"/>
    
  </entry>
  
</feed>
