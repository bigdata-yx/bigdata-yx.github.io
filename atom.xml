<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>曦</title>
  <icon>https://bigdata-yx.github.io/imgs/avatar.webp</icon>
  <subtitle>曦</subtitle>
  <link href="https://bigdata-yx.github.io/atom.xml" rel="self"/>
  
  <link href="https://bigdata-yx.github.io/"/>
  <updated>2025-01-13T08:12:05.000Z</updated>
  <id>https://bigdata-yx.github.io/</id>
  
  <author>
    <name>曦</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据可视化学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1083.html"/>
    <id>https://bigdata-yx.github.io/posts/1083.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据数据可视化学习路线"><a href="#大数据数据可视化学习路线" class="headerlink" title="大数据数据可视化学习路线"></a>大数据数据可视化学习路线</h1><h5 id="前提：具备一定的Python基础"><a href="#前提：具备一定的Python基础" class="headerlink" title="前提：具备一定的Python基础"></a>前提：具备一定的Python基础</h5><h2 id="1-python的基础学习，以及进阶学习"><a href="#1-python的基础学习，以及进阶学习" class="headerlink" title="1.python的基础学习，以及进阶学习"></a>1.python的基础学习，以及进阶学习</h2><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href>https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="2-numpy、pandas的入门学习（先学numpy）"><a href="#2-numpy、pandas的入门学习（先学numpy）" class="headerlink" title="2.numpy、pandas的入门学习（先学numpy）"></a>2.numpy、pandas的入门学习（先学numpy）</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="3-数据可视化matplotlib库的基础绘图"><a href="#3-数据可视化matplotlib库的基础绘图" class="headerlink" title="3.数据可视化matplotlib库的基础绘图"></a>3.数据可视化matplotlib库的基础绘图</h3><p>【千锋教育python数据可视化Matplotlib绘图教程，Matplotlib柱状图｜Matplotlib动态图｜Matplotlib散点图】<a href>https://www.bilibili.com/video/BV1nM411m7Cf?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看"><a href="#4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看" class="headerlink" title="4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)"></a>4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="5-数据可视化pyecharts可交互式基础绘图"><a href="#5-数据可视化pyecharts可交互式基础绘图" class="headerlink" title="5.数据可视化pyecharts可交互式基础绘图"></a>5.数据可视化pyecharts可交互式基础绘图</h3><p>【千锋教育PyEcharts数据可视化快速入门教程，大数据分析Python交互绘图实用利器】<a href>https://www.bilibili.com/video/BV1nM411F7GT?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="6-数据可视化tableau图标绘图"><a href="#6-数据可视化tableau图标绘图" class="headerlink" title="6.数据可视化tableau图标绘图"></a>6.数据可视化tableau图标绘图</h3><p>【【Tableau教程】Tableau零基础教程，带你解锁当下最受欢迎的数据可视化软件】<a href>https://www.bilibili.com/video/BV1E4411B7ef?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="7-数据交互式可视化进阶绘图前提基础html"><a href="#7-数据交互式可视化进阶绘图前提基础html" class="headerlink" title="7.数据交互式可视化进阶绘图前提基础html"></a>7.数据交互式可视化进阶绘图前提基础html</h3><p>【黑马程序员pink老师前端入门教程，零基础必看的h5(html5)+css3+移动端前端视频教程】<a href>https://www.bilibili.com/video/BV14J4114768?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="8-数据可视化echarts可交互式进阶绘图"><a href="#8-数据可视化echarts可交互式进阶绘图" class="headerlink" title="8.数据可视化echarts可交互式进阶绘图"></a>8.数据可视化echarts可交互式进阶绘图</h3><p>【电商平台数据可视化实时监控系统-Echarts-vue项目综合练习-pink老师推荐(持续更新)素材已经更新】<a href>https://www.bilibili.com/video/BV1bh41197p8?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="9-使用vue进行数据可视化前提基础Node-js"><a href="#9-使用vue进行数据可视化前提基础Node-js" class="headerlink" title="9.使用vue进行数据可视化前提基础Node.js"></a>9.使用vue进行数据可视化前提基础Node.js</h3><p>【黑马程序员Node.js全套入门教程，nodejs新教程含es6模块化+npm+express+webpack+promise等_Nodejs实战案例详解】<a href>https://www.bilibili.com/video/BV1a34y167AZ?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="10-使用vue进行数据可视化"><a href="#10-使用vue进行数据可视化" class="headerlink" title="10.使用vue进行数据可视化"></a>10.使用vue进行数据可视化</h3><p>【千锋Echarts+Vue3.0数据可视化项目构建_入门必备前端项目实战教程】<a href>https://www.bilibili.com/video/BV14u411D7qK?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据数据可视化学习路线&quot;&gt;&lt;a href=&quot;#大数据数据可视化学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据数据可视化学习路线&quot;&gt;&lt;/a&gt;大数据数据可视化学习路线&lt;/h1&gt;&lt;h5 id=&quot;前提：具备一定的Python基础&quot;&gt;&lt;a href</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>数据处理学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1082.html"/>
    <id>https://bigdata-yx.github.io/posts/1082.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><h1 id="大数据数据处理学习路线"><a href="#大数据数据处理学习路线" class="headerlink" title="大数据数据处理学习路线"></a>大数据数据处理学习路线</h1><p><strong>注意</strong>：下列无标注的全部要看</p><h2 id="第一阶段：基础部分"><a href="#第一阶段：基础部分" class="headerlink" title="第一阶段：基础部分"></a>第一阶段：基础部分</h2><ul><li>语言基础：python、java基础<ul><li>python：【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注释：第一阶段</li><li>java：【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>数据库基础：mysql<ul><li>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：基础篇</li></ul></li><li>linux：命令基础（）<ul><li>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></li></ul></li></ul><h2 id="第二部分：简单工具使用"><a href="#第二部分：简单工具使用" class="headerlink" title="第二部分：简单工具使用"></a>第二部分：简单工具使用</h2><ul><li><p>python第三方库：pandas、numpy、requests、bs4、jieba、snownlp</p><table><thead><tr><th>第三方库</th><th>链接</th></tr></thead><tbody><tr><td>pandas、numpy</td><td>【千锋教育python数据分析教程200集，Python数据分析师入门必备视频】<a href="https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E6%B3%A8%EF%BC%9Ap38-p117%EF%BC%89">https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58（注：p38-p117）</a></td></tr><tr><td>requests、bs4</td><td>【尚硅谷Python爬虫教程小白零基础速通（含python基础+爬虫案例）】<a href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a> (注：p52-最后)</td></tr><tr><td>jieba</td><td>【Python Jieba 中文分词工具】<a href="https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></td></tr><tr><td>snownlp</td><td>【Lecture 12 基于Snownlp的文本情感分析】<a href="https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E7%9C%8B%E5%AE%8C%E8%BF%99%E9%9B%86%E5%B0%B1%E8%A1%8C%E4%BA%86%EF%BC%89">https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58（看完这集就行了）</a></td></tr></tbody></table></li><li><p>excel：函数使用以及操作</p><ul><li>【2025必看！全网最新最细最实用Excel零基础入门到精通全套教程！专为零基础小白打造！内容富含Excel表格基础操作、实用函数讲解、项目实战等！】<a href="https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li><p>hadoop：hadoop的基本使用命令</p><ul><li><a href="https://blog.csdn.net/m0_43405302/article/details/122243263">hadoop的HDFS的shell命令大全（一篇文章就够了）_shell统计hdfs-CSDN博客</a></li></ul></li></ul><h2 id="第三部分：集群工具使用"><a href="#第三部分：集群工具使用" class="headerlink" title="第三部分：集群工具使用"></a>第三部分：集群工具使用</h2><ul><li><p>scala：scala语言基础</p><ul><li>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：看完前十章</li></ul></li><li><p>mapreduce：了解基本使用以及自定义方法</p><ul><li><p>【黑马程序员大数据Hadoop3.x全套教程，一套精通Hadoop的大数据入门教程】<a href="https://www.bilibili.com/video/BV11N411d7Zh?p=214&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV11N411d7Zh?p=214&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p171-214</p></li></ul></li><li><p>spark：掌握sparkcore sparksql</p><ul><li>【全网最全大数据Spark3.0教程 Spark3.0从入门到精通 黑马程序员大数据入门教程系列】<a href="https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：搭建部分不用看、只看sparkcore和sparksql</li></ul></li><li><p>hive：了解hive命令以及udf自定义函数</p><ul><li><p>【黑马程序员Hive全套教程，大数据Hive3.x数仓开发精讲到企业级实战应用】<a href="https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p0-p96</p></li></ul></li></ul><h2 id="第四部分：深入学习"><a href="#第四部分：深入学习" class="headerlink" title="第四部分：深入学习"></a>第四部分：深入学习</h2><ul><li>机器学习：了解机器学习原理以及sklearn使用<ul><li>【黑马程序员3天快速入门python机器学习】<a href="https://www.bilibili.com/video/BV1nt411r7tj?p=18&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1nt411r7tj?p=18&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>spark streaming：了解流式数据<ul><li>自己找视频或文档</li></ul></li><li>项目制作：尝试制作大数据项目</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;h1 id=&quot;大数据数据处理学习路线&quot;&gt;&lt;a href=&quot;#大数据数据处理学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据数据处理学习路线&quot;&gt;&lt;/a&gt;大数据数据处理学习路线&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：下列无标注</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>集群搭建学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1081.html"/>
    <id>https://bigdata-yx.github.io/posts/1081.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据集群搭建学习路线"><a href="#大数据集群搭建学习路线" class="headerlink" title="大数据集群搭建学习路线"></a>大数据集群搭建学习路线</h1><h3 id="前提：熟练使用Linux的命令及其操作"><a href="#前提：熟练使用Linux的命令及其操作" class="headerlink" title="前提：熟练使用Linux的命令及其操作"></a>前提：熟练使用Linux的命令及其操作</h3><h3 id="0-CentosLinux的基操"><a href="#0-CentosLinux的基操" class="headerlink" title="0.CentosLinux的基操"></a>0.CentosLinux的基操</h3><p>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="1-Mysql数据库的入门学习"><a href="#1-Mysql数据库的入门学习" class="headerlink" title="1.Mysql数据库的入门学习"></a>1.Mysql数据库的入门学习</h3><p>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="2-Hadoop集群的搭建及其使用和Hive的使用"><a href="#2-Hadoop集群的搭建及其使用和Hive的使用" class="headerlink" title="2.Hadoop集群的搭建及其使用和Hive的使用"></a>2.Hadoop集群的搭建及其使用和Hive的使用</h3><p>【黑马程序员大数据入门到实战教程，大数据开发必会的Hadoop、Hive，云平台实战项目全套一网打尽】<a href="https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="3-Java的基础学习（上部）"><a href="#3-Java的基础学习（上部）" class="headerlink" title="3.Java的基础学习（上部）"></a>3.Java的基础学习（上部）</h3><p>【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="4-scala语言的学习"><a href="#4-scala语言的学习" class="headerlink" title="4.scala语言的学习"></a>4.scala语言的学习</h3><p>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="5-Python基础的学习"><a href="#5-Python基础的学习" class="headerlink" title="5.Python基础的学习"></a>5.Python基础的学习</h3><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="6-zookeeper组件的学习"><a href="#6-zookeeper组件的学习" class="headerlink" title="6.zookeeper组件的学习"></a>6.zookeeper组件的学习</h3><p>【黑马程序员Zookeeper视频教程，快速入门zookeeper技术】<a href="https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="7-kafka组件的利用"><a href="#7-kafka组件的利用" class="headerlink" title="7.kafka组件的利用"></a>7.kafka组件的利用</h3><p>【尚硅谷Kafka教程，2024新版kafka视频，零基础入门到实战】<a href="https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="8-Flume的使用"><a href="#8-Flume的使用" class="headerlink" title="8.Flume的使用"></a>8.Flume的使用</h3><p>【【海牛大数据】Flume教程（学Flume看这个一套就够了，从基础到扩展到实战案例到组件配合全都有）】<a href="https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="9-Sqoop的运用"><a href="#9-Sqoop的运用" class="headerlink" title="9.Sqoop的运用"></a>9.Sqoop的运用</h3><p>【【海牛大数据】Sqoop教程（命令详解、各组件融合、实战案例）】<a href="https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="10-Spark的搭建"><a href="#10-Spark的搭建" class="headerlink" title="10.Spark的搭建"></a>10.Spark的搭建</h3><h4 id="Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）"><a href="#Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）" class="headerlink" title="Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）"></a>Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）</h4><p><a href="https://blog.csdn.net/JunLeon/article/details/123625680">(超详细) Spark环境搭建（Local模式、 StandAlone模式、Spark On Yarn模式）-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_44480968/article/details/119580363">Spark配置高可用（HA）_spark ha-CSDN博客</a></p><h3 id="11-Flink搭建及其使用-由于python版的使用还不是特别完整"><a href="#11-Flink搭建及其使用-由于python版的使用还不是特别完整" class="headerlink" title="11.Flink搭建及其使用(由于python版的使用还不是特别完整)"></a>11.Flink搭建及其使用(由于python版的使用还不是特别完整)</h3><h4 id="scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="scala版：【【尚硅谷】Flink1.13教程（Scala版）】https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>scala版：【【尚硅谷】Flink1.13教程（Scala版）】<a href="https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h4 id="java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】<a href="https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h3 id="11-Redis的搭建及其使用"><a href="#11-Redis的搭建及其使用" class="headerlink" title="11.Redis的搭建及其使用"></a>11.Redis的搭建及其使用</h3><p>【黑马程序员Redis入门到实战教程，深度透析redis底层原理+redis分布式锁+企业解决方案+黑马点评实战项目】<a href="https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="12-Hbase的搭建及其使用"><a href="#12-Hbase的搭建及其使用" class="headerlink" title="12.Hbase的搭建及其使用"></a>12.Hbase的搭建及其使用</h3><p>【【好程序员】大数据全新分布式存储HBase精品课程】<a href="https://www.bilibili.com/video/BV1RM411i7XM?p=15&vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1RM411i7XM?p=15&amp;vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="13-Azkaban的搭建及其使用"><a href="#13-Azkaban的搭建及其使用" class="headerlink" title="13.Azkaban的搭建及其使用"></a>13.Azkaban的搭建及其使用</h3><p>【尚硅谷大数据Azkaban 3.x教程（全新发布）】<a href="https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据集群搭建学习路线&quot;&gt;&lt;a href=&quot;#大数据集群搭建学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据集群搭建学习路线&quot;&gt;&lt;/a&gt;大数据集群搭建学习路线&lt;/h1&gt;&lt;h3 id=&quot;前提：熟练使用Linux的命令及其操作&quot;&gt;&lt;a href=</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop原理</title>
    <link href="https://bigdata-yx.github.io/posts/3985.html"/>
    <id>https://bigdata-yx.github.io/posts/3985.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop多map条件查询导入hdfs.md</title>
    <link href="https://bigdata-yx.github.io/posts/3992.html"/>
    <id>https://bigdata-yx.github.io/posts/3992.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多map条件查询导入hdfs"><a href="#多map条件查询导入hdfs" class="headerlink" title="多map条件查询导入hdfs"></a>多map条件查询导入hdfs</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect 数据库连接字符串 \</span><br><span class="line">--username 数据库用户名 \</span><br><span class="line">--password 数据库密码 \</span><br><span class="line">--target-dir hdfs位置 \</span><br><span class="line">--delete-target-dir \  <span class="comment"># 这个就是把目录删了，不然mapreduce会执行失败</span></span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \ <span class="comment"># 使用什么分隔符</span></span><br><span class="line">--num-mappers 3 \</span><br><span class="line">--split-by 切分数依据 \</span><br><span class="line">--query <span class="string">&#x27; SQL语句 and $CONDITIONS &#x27;</span></span><br></pre></td></tr></table></figure><h3 id="–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"><a href="#–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力" class="headerlink" title="–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"></a><strong>–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力</strong></h3><h3 id="CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理"><a href="#CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理" class="headerlink" title="$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理"></a><strong>$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理</strong></h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;多map条件查询导入hdfs&quot;&gt;&lt;a href=&quot;#多map条件查询导入hdfs&quot; class=&quot;headerlink&quot; title=&quot;多map条件查询导入hdfs&quot;&gt;&lt;/a&gt;多map条件查询导入hdfs&lt;/h2&gt;&lt;figure class=&quot;highlight </summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop实例命令</title>
    <link href="https://bigdata-yx.github.io/posts/3986.html"/>
    <id>https://bigdata-yx.github.io/posts/3986.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建hive的表根据mysql上的表进行创建-create-hive-table"><a href="#创建hive的表根据mysql上的表进行创建-create-hive-table" class="headerlink" title="创建hive的表根据mysql上的表进行创建(create-hive-table)"></a><strong>创建hive的表根据mysql上的表进行创建(create-hive-table)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table --connect jdbc:mysql://master:3306/sqoop_db --username root --password 123456 --table city --hive-table hkjcpdd.city</span><br></pre></td></tr></table></figure><h4 id="查看mysql上有什么表-list-tables"><a href="#查看mysql上有什么表-list-tables" class="headerlink" title="查看mysql上有什么表(list-tables)"></a><strong>查看mysql上有什么表(list-tables)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-tables --connect jdbc:mysql://master:3306/sys --username root --password 123456</span><br></pre></td></tr></table></figure><h3 id="查看mysql上有什么库-list-databases"><a href="#查看mysql上有什么库-list-databases" class="headerlink" title="查看mysql上有什么库(list-databases)"></a><strong>查看mysql上有什么库(list-databases)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://master:3306/ --username root --password 12345</span><br></pre></td></tr></table></figure><h3 id="使用sql进行操作-eval-query"><a href="#使用sql进行操作-eval-query" class="headerlink" title="使用sql进行操作(eval   query)"></a><strong>使用sql进行操作(eval   query)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">eval</span> --connect jdbc:mysql://master:3306/sqoop_db --username root --password  123456 --query <span class="string">&quot;select * from city limit 2;&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;创建hive的表根据mysql上的表进行创建-create-hive-table&quot;&gt;&lt;a href=&quot;#创建hive的表根据mysql上的表进行创建-create-hive-table&quot; class=&quot;headerlink&quot; title=&quot;创建hive的表根据my</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop导入到Hbase</title>
    <link href="https://bigdata-yx.github.io/posts/3988.html"/>
    <id>https://bigdata-yx.github.io/posts/3988.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-table hkjcpdd:city \</span><br><span class="line">--column-family cf \</span><br><span class="line">--hbase-row-key <span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --hbase-row-key:要求mysql表必须有主见，将主键作为rowkey,表示一行</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;l</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop导入其他格式文件</title>
    <link href="https://bigdata-yx.github.io/posts/3994.html"/>
    <id>https://bigdata-yx.github.io/posts/3994.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导入其他格式文件"><a href="#导入其他格式文件" class="headerlink" title="导入其他格式文件"></a>导入其他格式文件</h2><h3 id="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式"><a href="#导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式" class="headerlink" title="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)"></a><strong>导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /data/hkjcpdd \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-parquetfile \ <span class="comment">#文件格式</span></span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query <span class="string">&#x27;select * from city where id &lt; 10 and $CONDITIONS&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;导入其他格式文件&quot;&gt;&lt;a href=&quot;#导入其他格式文件&quot; class=&quot;headerlink&quot; title=&quot;导入其他格式文件&quot;&gt;&lt;/a&gt;导入其他格式文件&lt;/h2&gt;&lt;h3 id=&quot;导入不同格式，支持as-avrodatafile、as-parquetfile、a</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop应用案例</title>
    <link href="https://bigdata-yx.github.io/posts/3987.html"/>
    <id>https://bigdata-yx.github.io/posts/3987.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="创建一个执行文件然后给予权限然后执行"><a href="#创建一个执行文件然后给予权限然后执行" class="headerlink" title="创建一个执行文件然后给予权限然后执行"></a><strong>创建一个执行文件然后给予权限然后执行</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">batch_date=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--usernmae root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir hive表hdfs目录 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query select * from city</span><br><span class="line"></span><br><span class="line">result=$?</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [<span class="variable">$result</span> != 0];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;执行失败&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd</span><br><span class="line"><span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span>  <span class="string">&quot;执行成功&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;创建一个执行文件然后给予权限然后执行&quot;&gt;&lt;a href=&quot;#创建一个执行文件然后给予权限然后执行&quot; class=&quot;headerlink&quot; title=&quot;创建一个执行文件然后给予权限然后执行&quot;&gt;&lt;/a&gt;&lt;strong&gt;创建一个执行文件然后给予权限然后执行&lt;/stro</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop搭建</title>
    <link href="https://bigdata-yx.github.io/posts/3984.html"/>
    <id>https://bigdata-yx.github.io/posts/3984.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sqoop搭建"><a href="#Sqoop搭建" class="headerlink" title="Sqoop搭建"></a>Sqoop搭建</h2><h3 id="1-解压"><a href="#1-解压" class="headerlink" title="1.解压"></a>1.解压</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /root/software</span><br></pre></td></tr></table></figure><h3 id="更改名字"><a href="#更改名字" class="headerlink" title="更改名字"></a>更改名字</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/software</span><br><span class="line"><span class="built_in">mv</span> sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop</span><br></pre></td></tr></table></figure><h3 id="2-添加环境变量"><a href="#2-添加环境变量" class="headerlink" title="2.添加环境变量"></a>2.添加环境变量</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">export</span> SQOOP_HOME=/root/software/sqoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SQOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure><h3 id="3-配置-Sqoop-环境变量文件"><a href="#3-配置-Sqoop-环境变量文件" class="headerlink" title="3.配置 Sqoop 环境变量文件"></a>3.配置 Sqoop 环境变量文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切换到 Sqoop 配置文件目录</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$SQOOP_HOME</span>/conf</span><br><span class="line"><span class="comment"># 复制 Sqoop 环境变量模板文件</span></span><br><span class="line"><span class="built_in">cp</span> sqoop-env-template.sh sqoop-env.sh </span><br><span class="line"><span class="comment"># 编辑文件，指定相关路径</span></span><br><span class="line">vi sqoop-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最底下的配置加上，没有装的就不用去掉#号</span></span><br></pre></td></tr></table></figure><h3 id="4-MySQL-驱动"><a href="#4-MySQL-驱动" class="headerlink" title="4. MySQL 驱动"></a>4. MySQL 驱动</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拷贝 MySQL 驱动到 Sqoop 中的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> /opt/software/mysql-connector-java-5.1.37-bin.jar <span class="variable">$SQOOP_HOME</span>/lib</span><br></pre></td></tr></table></figure><h3 id="5-拷贝-Hive-文件"><a href="#5-拷贝-Hive-文件" class="headerlink" title="5. 拷贝 Hive 文件"></a>5. 拷贝 Hive 文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了后续方便操作 Hive，我们需要将 Hive 的驱动放入 Sqoop 的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> hive/lib/hive-common-3.1.2.jar sqoop/lib/</span><br></pre></td></tr></table></figure><h3 id="6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）"><a href="#6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）" class="headerlink" title="6.验证（输入 sqoop version，出现如下版本信息表示安装成功）"></a>6.验证（输入 <code>sqoop version</code>，出现如下版本信息表示安装成功）</h3><h3 id="7-展示Mysql中sys库下的所有表"><a href="#7-展示Mysql中sys库下的所有表" class="headerlink" title="7.展示Mysql中sys库下的所有表"></a>7.展示Mysql中sys库下的所有表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list<span class="operator">-</span>tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/sys \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123456</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Sqoop搭建&quot;&gt;&lt;a href=&quot;#Sqoop搭建&quot; class=&quot;headerlink&quot; title=&quot;Sqoop搭建&quot;&gt;&lt;/a&gt;Sqoop搭建&lt;/h2&gt;&lt;h3 id=&quot;1-解压&quot;&gt;&lt;a href=&quot;#1-解压&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop</title>
    <link href="https://bigdata-yx.github.io/posts/3989.html"/>
    <id>https://bigdata-yx.github.io/posts/3989.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据的导入导出"><a href="#数据的导入导出" class="headerlink" title="数据的导入导出"></a>数据的导入导出</h2><p>导入：import</p><p>导出：export</p><p><img src="https://pic1.imgdb.cn/item/6784f9d1d0e0a243d4f3f083.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;数据的导入导出&quot;&gt;&lt;a href=&quot;#数据的导入导出&quot; class=&quot;headerlink&quot; title=&quot;数据的导入导出&quot;&gt;&lt;/a&gt;数据的导入导出&lt;/h2&gt;&lt;p&gt;导入：import&lt;/p&gt;
&lt;p&gt;导出：export&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https:</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop导出</title>
    <link href="https://bigdata-yx.github.io/posts/3990.html"/>
    <id>https://bigdata-yx.github.io/posts/3990.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-从hdfs导出到mysql里"><a href="#1-从hdfs导出到mysql里" class="headerlink" title="1.从hdfs导出到mysql里"></a><strong>1.从hdfs导出到mysql里</strong></h3><ol><li><h4 id="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"><a href="#要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表" class="headerlink" title="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"></a>要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表</h4></li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir hdfs路径指定到文件 \  <span class="comment"># 要导出的文件</span></span><br><span class="line">--table emp \    <span class="comment"># 导出到哪张Mysql表</span></span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--input-fields-terminated-by <span class="string">&#x27;|&#x27;</span>  <span class="comment"># 分隔符</span></span><br></pre></td></tr></table></figure><h3 id="2-从hive导出到Mysql中"><a href="#2-从hive导出到Mysql中" class="headerlink" title="2.从hive导出到Mysql中"></a><strong>2.从hive导出到Mysql中</strong></h3><h4 id="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"><a href="#sqoop的export命令支持insert、update到关系型数据库，但是不支持merge" class="headerlink" title="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"></a><strong>sqoop的export命令支持insert、update到关系型数据库，但是不支持merge</strong></h4><h4 id="1-hive表导入Mysql数据库insert（直接写入）"><a href="#1-hive表导入Mysql数据库insert（直接写入）" class="headerlink" title="1.hive表导入Mysql数据库insert（直接写入）"></a><strong>1.hive表导入Mysql数据库insert（直接写入）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--table cityhkjcpdd </span><br><span class="line">--num-mappers 4 </span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="2-从hive表导入mysql数据库update（追加替换相同）"><a href="#2-从hive表导入mysql数据库update（追加替换相同）" class="headerlink" title="2.从hive表导入mysql数据库update（追加替换相同）"></a><strong>2.从hive表导入mysql数据库update（追加替换相同）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--update-key <span class="built_in">id</span> \    <span class="comment"># 根据id来进行去重</span></span><br><span class="line">--fields-terminated-by <span class="string">&#x27;\t&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-从hdfs导出到mysql里&quot;&gt;&lt;a href=&quot;#1-从hdfs导出到mysql里&quot; class=&quot;headerlink&quot; title=&quot;1.从hdfs导出到mysql里&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.从hdfs导出到mysql里&lt;/strong&gt;&lt;/h3&gt;</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>全量导入hive表和增量导入hive表</title>
    <link href="https://bigdata-yx.github.io/posts/3991.html"/>
    <id>https://bigdata-yx.github.io/posts/3991.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="全量导入："><a href="#全量导入：" class="headerlink" title="全量导入："></a><strong>全量导入：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-import \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table hive的库.hive的表</span><br></pre></td></tr></table></figure><h4 id="增量导入："><a href="#增量导入：" class="headerlink" title="增量导入："></a><strong>增量导入：</strong></h4><h5 id="1-append方式"><a href="#1-append方式" class="headerlink" title="1.append方式"></a><strong>1.append方式</strong></h5><h5 id="2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"><a href="#2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）" class="headerlink" title="2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"></a>2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）</h5><p><img src="https://pic1.imgdb.cn/item/6784fbd6d0e0a243d4f3f0e9.png"></p><h2 id="这个是第一种方式：incremental-append"><a href="#这个是第一种方式：incremental-append" class="headerlink" title="这个是第一种方式：incremental append"></a><strong>这个是第一种方式：incremental append</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line"></span><br><span class="line">--target-dir hdfs路径 \  这个路径是表的路径可以在hive中show create table city;就可以看到hdfs的位置了</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column <span class="built_in">id</span> \</span><br><span class="line">--last-value 1  这个就是看你的数字哪一行，如果是1那就从2开始增量数据</span><br></pre></td></tr></table></figure><p>参数解释：</p><p>1)incremental : append或lastmodified，使用lastmodified方式导入数据要指定增量数据是要 –append（追加）还是要 –merge-key（合并）</p><p>2)check-column&lt;字段&gt;: 作为增量导入判断的列名</p><p>3)last-value val : 指定某一个值，用于标记增量导入的位置，这个值的数据不会被导入列表中，只用于标记当前表中最后的值。</p><h3 id="第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）"><a href="#第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）" class="headerlink" title="第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）"></a><strong>第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--append</span><br><span class="line"></span><br><span class="line">--注意：last-value的设置把包括2020-10-10 13:00:00 时间的数据做增量导入</span><br></pre></td></tr></table></figure><h3 id="第三种导入方式–incremental-lastmodified-–append-（进行合并的）"><a href="#第三种导入方式–incremental-lastmodified-–append-（进行合并的）" class="headerlink" title="第三种导入方式–incremental lastmodified –append （进行合并的）"></a><strong>第三种导入方式–incremental lastmodified –append （进行合并的）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--merge-key <span class="built_in">id</span> <span class="comment"># 根据id来进行操作</span></span><br><span class="line"></span><br><span class="line">--incremental lastmodified --merge-key的作用：修改过得数据和新增的数据（前提是满足last-value的条件）都会导入尽力啊，并且重复的数据（不需要满足last-value的条件）都会进行合并</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;全量导入：&quot;&gt;&lt;a href=&quot;#全量导入：&quot; class=&quot;headerlink&quot; title=&quot;全量导入：&quot;&gt;&lt;/a&gt;&lt;strong&gt;全量导入：&lt;/strong&gt;&lt;/h3&gt;&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td </summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Spark RDD</title>
    <link href="https://bigdata-yx.github.io/posts/fa63.html"/>
    <id>https://bigdata-yx.github.io/posts/fa63.html</id>
    <published>2025-01-13T07:59:03.000Z</published>
    <updated>2025-01-13T07:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心"><a href="#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心" class="headerlink" title="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心"></a><strong>RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心</strong></h3><h4 id><a href="#" class="headerlink" title></a></h4><h4 id="Dataset-数据集"><a href="#Dataset-数据集" class="headerlink" title="Dataset:(数据集)"></a><strong>Dataset</strong><strong>:(数据集)</strong></h4><ul><li><strong>RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数；</strong></li><li><strong>RDD 也可以缓存起来, 相当于存储具体数据。</strong></li></ul><h4 id="Distributed-："><a href="#Distributed-：" class="headerlink" title="Distributed****："></a><strong>Distributed****：</strong></h4><h4 id="RDD-支持分区-可以运行在集群中。"><a href="#RDD-支持分区-可以运行在集群中。" class="headerlink" title="RDD 支持分区, 可以运行在集群中。"></a><strong>RDD 支持分区, 可以运行在集群中。</strong></h4><h4 id="Resilient-："><a href="#Resilient-：" class="headerlink" title="Resilient****："></a><strong>Resilient****：</strong></h4><ul><li><strong>RDD 支持高效的容错；</strong></li><li><strong>RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中。</strong></li></ul><h3 id="1-RDD的特点："><a href="#1-RDD的特点：" class="headerlink" title="1.RDD的特点："></a><strong>1.RDD的特点：</strong></h3><ul><li><p>弹性</p><ul><li>容错的弹性:数据丢失可以自动恢复;</li><li>存储的弹性:内存与磁盘的自动切换;</li><li>计算的弹性:计算出错重试机制;</li><li>分片的弹性:可根据需要重新分片。</li></ul></li><li><p>分布式:数据存储在集群不同节点上&#x2F;计算分布式。</p></li><li><p>数据集: RDD封装了计算逻辑，并不保存数据。</p></li><li><p>数据抽象: RDD是一个抽象类，需要子类具体实现。</p></li><li><p>不可变: RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑。</p></li><li><p>可分区、并行计算。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心&quot;&gt;&lt;a href=&quot;#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里</summary>
      
    
    
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/categories/Spark/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark on yarn</title>
    <link href="https://bigdata-yx.github.io/posts/fa64.html"/>
    <id>https://bigdata-yx.github.io/posts/fa64.html</id>
    <published>2025-01-13T07:59:03.000Z</published>
    <updated>2025-01-13T07:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-on-Yarn"><a href="#Spark-on-Yarn" class="headerlink" title="Spark on Yarn"></a>Spark on Yarn</h1><h3 id="SparkOnYarn本质"><a href="#SparkOnYarn本质" class="headerlink" title="SparkOnYarn本质"></a><strong>SparkOnYarn本质</strong></h3><p>master角色由yarn的Resourcemanager担任</p><p>worker角色由yarn的nodemanager担任</p><p>deiver角色运行在Yarn容器内或提交任务的客户端进程中</p><p>真正干活的Executor运行在yarn提供的容器内</p><h3 id="部署："><a href="#部署：" class="headerlink" title="部署："></a><strong>部署：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在spark-env.sh上只要添加这两个即可</span></span><br><span class="line">HADOOP_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line"><span class="comment"># 添加环境变量之后还要添加一个</span></span><br><span class="line"><span class="built_in">which</span> python</span><br><span class="line"><span class="built_in">export</span> SPARK_PYTHON=<span class="built_in">which</span> python</span><br><span class="line"><span class="comment"># 然后启动，bin/pyspark --master yarn</span></span><br></pre></td></tr></table></figure><h3 id="Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"><a href="#Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式" class="headerlink" title="Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"></a><strong>Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式</strong></h3><h3 id="这两种模式的区别就是Driver运行的位置"><a href="#这两种模式的区别就是Driver运行的位置" class="headerlink" title="这两种模式的区别就是Driver运行的位置"></a><strong>这两种模式的区别就是Driver运行的位置</strong></h3><h3 id="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"><a href="#集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内" class="headerlink" title="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"></a><strong>集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内</strong></h3><h3 id="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"><a href="#客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中" class="headerlink" title="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"></a><strong>客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端模式</span></span><br><span class="line">SPARK HOME=/export/server/spark<span class="variable">$&#123;SPARK HOME&#125;</span>/bin/spark-submit\--master yarn</span><br><span class="line">--deploy-mode client \（默认是客户端模式，不加也可以）</span><br><span class="line"><span class="comment"># 下面的参数可加可不加</span></span><br><span class="line">--driver-memory 512m \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line"><span class="variable">$&#123;SPARK HOME&#125;</span>/examples/src/main/python/pi.py 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群模式</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Spark-on-Yarn&quot;&gt;&lt;a href=&quot;#Spark-on-Yarn&quot; class=&quot;headerlink&quot; title=&quot;Spark on Yarn&quot;&gt;&lt;/a&gt;Spark on Yarn&lt;/h1&gt;&lt;h3 id=&quot;SparkOnYarn本质&quot;&gt;&lt;a hre</summary>
      
    
    
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/categories/Spark/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark搭建</title>
    <link href="https://bigdata-yx.github.io/posts/fa65.html"/>
    <id>https://bigdata-yx.github.io/posts/fa65.html</id>
    <published>2025-01-13T07:59:03.000Z</published>
    <updated>2025-01-13T07:59:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark安装部署"><a href="#spark安装部署" class="headerlink" title="spark安装部署"></a>spark安装部署</h1><h3 id="先安装anacondea3然后再解压spark"><a href="#先安装anacondea3然后再解压spark" class="headerlink" title="先安装anacondea3然后再解压spark"></a><strong>先安装anacondea3然后再解压spark</strong></h3><h3 id="然后追加以下内容至-x2F-root-x2F-condarc"><a href="#然后追加以下内容至-x2F-root-x2F-condarc" class="headerlink" title="然后追加以下内容至&#x2F;root&#x2F; .condarc"></a><strong>然后追加以下内容至&#x2F;root&#x2F; .condarc</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - defaults</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/fastai/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br></pre></td></tr></table></figure><h3 id="配置pyspark"><a href="#配置pyspark" class="headerlink" title="配置pyspark"></a><strong>配置pyspark</strong></h3><h3 id="conda-create-n-pyspark-python-x3D-版本号然后回车就下载了"><a href="#conda-create-n-pyspark-python-x3D-版本号然后回车就下载了" class="headerlink" title="conda create -n pyspark python&#x3D;版本号然后回车就下载了"></a><strong>conda create -n pyspark python&#x3D;版本号然后回车就下载了</strong></h3><h3 id="安装完之后conda-activate-pyspark切换虚拟环境"><a href="#安装完之后conda-activate-pyspark切换虚拟环境" class="headerlink" title="安装完之后conda activate pyspark切换虚拟环境"></a><strong>安装完之后conda activate pyspark切换虚拟环境</strong></h3><h1 id="Local环境部署"><a href="#Local环境部署" class="headerlink" title="Local环境部署"></a><strong>Local环境部署</strong></h1><h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a><strong>添加环境变量</strong></h3><h2 id="如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath"><a href="#如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath" class="headerlink" title="如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)"></a><strong>如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)</strong></h2><h3 id="然后就可以启动了pysparkspark-shell"><a href="#然后就可以启动了pysparkspark-shell" class="headerlink" title="然后就可以启动了pysparkspark shell"></a><strong>然后就可以启动了pysparkspark shell</strong></h3><h3 id="运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动"><a href="#运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动" class="headerlink" title="运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动"></a><strong>运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动</strong></h3><h3 id="Spark集群搭建"><a href="#Spark集群搭建" class="headerlink" title="Spark集群搭建"></a>Spark集群搭建</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br><span class="line">    <span class="comment"># 指定 Java Home</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/export/servers/jdk1.8.0_221</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定 Spark Master 地址</span></span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_HOST=node01</span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> works.template works</span><br><span class="line">    将Localhost删了换成master slave1 slave2</span><br><span class="line">    </span><br><span class="line">配置 HistoryServer</span><br><span class="line"><span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vi spark-defaults.conf (将这两个前面<span class="comment">#去掉)</span></span><br><span class="line">    spark.eventLog.enabled  <span class="literal">true</span></span><br><span class="line">    spark.eventLog.<span class="built_in">dir</span>      hdfs://node01:8020/spark_log</span><br><span class="line">vi spark-env.sh末尾添加</span><br><span class="line">    <span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:8020/spark_log&quot;</span></span><br><span class="line"> 创建hdfs目录</span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /spark_log</span><br><span class="line"></span><br><span class="line">修改log4j2.properties.template改名为log4j2.properties然后将19行的info改为WARN</span><br><span class="line"></span><br><span class="line">然后分发</span><br><span class="line">启动历史服务器：sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">最后启动</span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">连接，在webui的那个</span><br><span class="line">pyspark --master spark://master:7077</span><br></pre></td></tr></table></figure><h2 id="spark基于zookeeper实现HA"><a href="#spark基于zookeeper实现HA" class="headerlink" title="spark基于zookeeper实现HA"></a><strong>spark基于zookeeper实现HA</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前提确保zookeeper和hdfs均已启动</span><br><span class="line">vi spark-env.sh文件</span><br><span class="line">将<span class="built_in">export</span> SPARK_MASTER_HOST=master和他的那个端口注释了</span><br><span class="line">追加SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181,hadoop03:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span><br><span class="line">然后重新启动spark并且再启动另外一台或者多台机的start-master.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;spark安装部署&quot;&gt;&lt;a href=&quot;#spark安装部署&quot; class=&quot;headerlink&quot; title=&quot;spark安装部署&quot;&gt;&lt;/a&gt;spark安装部署&lt;/h1&gt;&lt;h3 id=&quot;先安装anacondea3然后再解压spark&quot;&gt;&lt;a href=&quot;#先安</summary>
      
    
    
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/categories/Spark/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Spark" scheme="https://bigdata-yx.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Flume Put事务和Take事务</title>
    <link href="https://bigdata-yx.github.io/posts/fe67.html"/>
    <id>https://bigdata-yx.github.io/posts/fe67.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">在source和chanel的传输中是批量过去的，channels传输到sinks也是批量的</span><br><span class="line"></span><br><span class="line">在source到channel中呢就有一个缓冲区</span><br><span class="line"></span><br><span class="line">1.doPut操作，浆皮数据写入到临时缓冲区 Putlist中</span><br><span class="line"></span><br><span class="line">2.doCommit操作：检查channel队列中是否有足够空间用来存放数据 有的话就会执行doCommit操作</span><br><span class="line"></span><br><span class="line">3.如果channel空间不够就会执行回滚数据的操作（doRollBack）</span><br><span class="line"></span><br><span class="line">在channel到sink中的事务叫take事务，也是批量过去的</span><br><span class="line"></span><br><span class="line">1.doTake操作，拉取一批channel的数据，然后进入缓冲区</span><br><span class="line"></span><br><span class="line">2.takeList操作，临时缓冲</span><br><span class="line"></span><br><span class="line">3.doCommit操作，将这一批数据发送出去，发送失败就会回滚</span><br><span class="line"></span><br><span class="line">4.doRollBack发送失败就进行回滚</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume source的type</title>
    <link href="https://bigdata-yx.github.io/posts/fe55.html"/>
    <id>https://bigdata-yx.github.io/posts/fe55.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="不支持断点续传"><a href="#不支持断点续传" class="headerlink" title="不支持断点续传"></a>不支持断点续传</h5><h1 id="spooling-directory-source"><a href="#spooling-directory-source" class="headerlink" title="spooling directory source"></a>spooling directory source</h1><h2 id="监听某一个目录，只要目录下有文件，文件中的数据就会收集"><a href="#监听某一个目录，只要目录下有文件，文件中的数据就会收集" class="headerlink" title="监听某一个目录，只要目录下有文件，文件中的数据就会收集"></a>监听某一个目录，只要目录下有文件，文件中的数据就会收集</h2><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; spooldir a1.sources.r1.spoolDir &#x3D; &#x2F;root&#x2F;spool</p><p>a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 10000 a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>注意Dir大写</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h5 id=&quot;不支持断点续传&quot;&gt;&lt;a href=&quot;#不支持断点续传&quot; class=&quot;headerlink&quot; title=&quot;不支持断点续传&quot;&gt;&lt;/a&gt;不支持断点续传&lt;/h5&gt;&lt;h1 id=&quot;spooling-directory-source&quot;&gt;&lt;a href=&quot;#spooling</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>案例：Flume avro sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe65.html"/>
    <id>https://bigdata-yx.github.io/posts/fe65.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="avro-sink"><a href="#avro-sink" class="headerlink" title="avro sink"></a>avro sink</h2><p>需要配置的：type &#x3D; avro</p><p>hostname &#x3D; 主机名</p><p>port  &#x3D; 端口</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; avro<br>a1.sources.r1.bind &#x3D; 192.168.1.122<br>a1.sources.r1.port &#x3D; 55555&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1 c2<br>a1.sinks &#x3D; k1 k2</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100<br>a1.channels.c2.type &#x3D; memory<br>a1.channels.c2.capacity &#x3D; 10000<br>a1.channels.c2.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; avro<br>a1.sinks.k1.hostname &#x3D; 192.168.1.123<br>a1.sinks.k1.port &#x3D; 55555<br>a1.sinks.k2.type &#x3D; avro<br>=&#x3D;a1.sinks.k2.hostname &#x3D; 192.168.1.124<br>a1.sinks.k2.port &#x3D; 55555</p><p>a1.sources.r1.channels &#x3D; c1 c2<br>a1.sinks.k1.channel &#x3D; c1<br>a1.sinks.k2.channel &#x3D; c2</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;avro-sink&quot;&gt;&lt;a href=&quot;#avro-sink&quot; class=&quot;headerlink&quot; title=&quot;avro sink&quot;&gt;&lt;/a&gt;avro sink&lt;/h2&gt;&lt;p&gt;需要配置的：type &amp;#x3D; avro&lt;/p&gt;
&lt;p&gt;hostname &amp;#x</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>案例：Flume exec sources监听命令</title>
    <link href="https://bigdata-yx.github.io/posts/fe64.html"/>
    <id>https://bigdata-yx.github.io/posts/fe64.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="exec-sources"><a href="#exec-sources" class="headerlink" title="exec sources"></a>exec sources</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; exec</p><p>a1.sources.r1.command &#x3D; tail -f &#x2F;root&#x2F;exec.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>那个命令就是需要监听的命令</p><p>他是不断的去监听你文件添加了什么</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;exec-sources&quot;&gt;&lt;a href=&quot;#exec-sources&quot; class=&quot;headerlink&quot; title=&quot;exec sources&quot;&gt;&lt;/a&gt;exec sources&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;a1.sources &amp;#x3D;</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>案例：Flume file channel</title>
    <link href="https://bigdata-yx.github.io/posts/fe63.html"/>
    <id>https://bigdata-yx.github.io/posts/fe63.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="file-channel"><a href="#file-channel" class="headerlink" title="file channel"></a>file channel</h2><p>需要的参数 type &#x3D; file</p><p>dataDirs &#x3D; &#x2F;roort</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.channels.c1.type &#x3D; file&#x3D;&#x3D;<br>a1.channels.c1.capaciry&#x3D;10000<br>a1.channels.c1.transactioncapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;file-channel&quot;&gt;&lt;a href=&quot;#file-channel&quot; class=&quot;headerlink&quot; title=&quot;file channel&quot;&gt;&lt;/a&gt;file channel&lt;/h2&gt;&lt;p&gt;需要的参数 type &amp;#x3D; file&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume file_roll sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe62.html"/>
    <id>https://bigdata-yx.github.io/posts/fe62.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="File-roll-sink"><a href="#File-roll-sink" class="headerlink" title="File_roll sink"></a>File_roll sink</h2><p>必须的：type &#x3D; file_roll</p><p>sink.directory 保存在那个目录&amp;#x20;</p><p>非必须：sink.rollInterval &#x3D; 30 就是每过30s就会生成一个新的文件用来存储数据</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; file_roll</p><p>a1.sinks.k1.sink.directory &#x3D; &#x2F;root&#x2F;file_roll&amp;#x20;</p><p>a1.sinks.k1.sink.rollInterval &#x3D; 10</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;File-roll-sink&quot;&gt;&lt;a href=&quot;#File-roll-sink&quot; class=&quot;headerlink&quot; title=&quot;File_roll sink&quot;&gt;&lt;/a&gt;File_roll sink&lt;/h2&gt;&lt;p&gt;必须的：type &amp;#x3D; file_r</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume hdfs sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe60.html"/>
    <id>https://bigdata-yx.github.io/posts/fe60.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hdfs-sink"><a href="#hdfs-sink" class="headerlink" title="hdfs sink"></a>hdfs sink</h2><p>hdfs sink是将flume收集到的数据写入到hdfs中，方便数据可靠的保存</p><p>sink输出到hdfs中，默认每10个event生成一个hdfs文件，hdfs文件目录会根据hdfs.path的配置自动创建</p><p>配置参数：</p><p>hdfs.pathhdfs目录路径</p><p>hdsf.filePrefix文件前缀，默认值是FlumeData</p><p>hdfs.fileSuffix文件后缀</p><p>hdfs.rollnterval就是滚动，设置多长时间创建一个新文件进行存储，默认是30s</p><p>hdfs.rollSize文件大小超过一定值后，然后再创建一个新文件进行存储，默认是1024</p><p>hdfs.rollCount写入了多少个事件然后再创建一个新文件进行存储，默认是10个，设置为0的话表示不基于事件个数</p><p>hdfs.file.Type文件格式，有三种格式可选择：SequenceFile（默认，二进制），DataStream(不压缩,以文本的形式【方便观察】)，CompressedStream（可压缩）</p><p>hdfs.batchSize批数次，HDFS Sink每次从Channel中拿的事件个数。默认值100（就是每100个事件就从sink中传到hdfs上一次）## 一般不设置</p><p>hdfs.maxOpenFiles允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭 ## 一般不设置</p><p>hdfs.callTimeouthdfs允许操作的事件，比如hdfs文件的open, write, flush, close操作，单位是ms，默认值10000</p><p>hdfs.codeC压缩编解码器。以下之一：gzip, bzip2, lzo, lzop, snappy</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; hdfs <br>a1.sinks.k1.hdfs.path &#x3D; &#x2F;data&#x2F;hkjcpdd&#x2F;%Y-%m-%d <br>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true <br>a1.sinks.k1.hdfs.rollInterval&#x3D; 10 <br>a1.sinks.k1.hdfs.fileType &#x3D; DataStream</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true这个是使用本地的事件戳</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;hdfs-sink&quot;&gt;&lt;a href=&quot;#hdfs-sink&quot; class=&quot;headerlink&quot; title=&quot;hdfs sink&quot;&gt;&lt;/a&gt;hdfs sink&lt;/h2&gt;&lt;p&gt;hdfs sink是将flume收集到的数据写入到hdfs中，方便数据可靠的保存&lt;/</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume http source 监听Http</title>
    <link href="https://bigdata-yx.github.io/posts/fe59.html"/>
    <id>https://bigdata-yx.github.io/posts/fe59.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="http-source-监听Http"><a href="#http-source-监听Http" class="headerlink" title="http source 监听Http"></a>http source 监听Http</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; http<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试办法curl -X POST -d ‘[{“headers”:{“key”:”Flume”},”body”:”TestEvent1”}]’ <a href="http://master:4141/">http://master:4141/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;http-source-监听Http&quot;&gt;&lt;a href=&quot;#http-source-监听Http&quot; class=&quot;headerlink&quot; title=&quot;http source 监听Http&quot;&gt;&lt;/a&gt;http source 监听Http&lt;/h2&gt;&lt;blockquo</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume kafka channel</title>
    <link href="https://bigdata-yx.github.io/posts/fe58.html"/>
    <id>https://bigdata-yx.github.io/posts/fe58.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kafka-channel"><a href="#kafka-channel" class="headerlink" title="kafka channel"></a>kafka channel</h2><p>需要指定的</p><blockquote><p>type &#x3D; org.apache.flume.channel.kafka.KafkaChannel</p><p>kafka.bootstrap.server &#x3D; hostname:port</p><p>kafka.topic &#x3D; hkjcpdd</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; org.apache.flume.channel.kafka.KafkaChannel <br>a1.channels.c1.kafka.bootstrap.servers &#x3D; master:9092</p><p>a1.channels.c1.kafka.topic&#x3D;hkjmjj</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;kafka-channel&quot;&gt;&lt;a href=&quot;#kafka-channel&quot; class=&quot;headerlink&quot; title=&quot;kafka channel&quot;&gt;&lt;/a&gt;kafka channel&lt;/h2&gt;&lt;p&gt;需要指定的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume kafka sink</title>
    <link href="https://bigdata-yx.github.io/posts/fe57.html"/>
    <id>https://bigdata-yx.github.io/posts/fe57.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kafka-sink"><a href="#kafka-sink" class="headerlink" title="kafka sink"></a>kafka sink</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; org.apache.flume.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.server &#x3D; master<br>a1.sinks.k1.kafka.topic &#x3D; hkjcpdd&#x3D;&#x3D;</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>flume监听nginx的access.log文件给kafka消费</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; taildir<br>a1.sources.r1.filegroups &#x3D; f1<br>a1.sources.r1.filegroups.f1 &#x3D; &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;access.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.servers &#x3D; master:9092<br>a1.sinks.k1.kafka.topic&#x3D;hkjcpdd</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;kafka-sink&quot;&gt;&lt;a href=&quot;#kafka-sink&quot; class=&quot;headerlink&quot; title=&quot;kafka sink&quot;&gt;&lt;/a&gt;kafka sink&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;a1.sources &amp;#x3D; r1&lt;br&gt;a</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume Sink Processors sink处理器</title>
    <link href="https://bigdata-yx.github.io/posts/fe66.html"/>
    <id>https://bigdata-yx.github.io/posts/fe66.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sink-Processors-sink处理器"><a href="#Sink-Processors-sink处理器" class="headerlink" title="Sink Processors sink处理器"></a>Sink Processors sink处理器</h2><h3 id="failover-sink-Processor故障转移处理器"><a href="#failover-sink-Processor故障转移处理器" class="headerlink" title="failover sink Processor故障转移处理器"></a><em><strong>failover sink Processor故障转移处</strong>理器</em></h3><h4 id="amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"><a href="#amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力" class="headerlink" title="&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"></a>&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力</h4><p>&amp;#x9;2)需要指定: processor.type  &#x3D; failover</p><p>&amp;#x9;processor.priority.&lt;sinkName&gt; &#x3D; 数字（就是优先级，数字越大优先级越高）</p><p>&amp;#x9;processor.maxpenalty &#x3D; 10000（就是允许你宕机以后给你重连的时间）</p><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>负载平衡处理器提供了在多个sink负载平衡流量的能力。支持两种模式：round<em>robin and random。round</em>_robin可以将数据负载均衡到多个sink上，random支持随机分发到不同的sink上</p><p>第一种就是随机给你发送（random随机）</p><p>第二种就是负载均衡（robin负载均衡）</p><blockquote><p>a1.sources&#x3D;r1 <br>a1.sinks&#x3D;k1 k2 <br>a1.channels&#x3D;c1 <br><br>a1.sources.r1.type&#x3D;netcat <br>a1.sources.r1.bind&#x3D;worker-1 <br>a1.sources.r1.port&#x3D;44444 <br><br>a1.channels.c1.type&#x3D;memory <br>a1.channels.c1.capacity&#x3D;100000 <br>a1.channels.c1.transactionCapacity&#x3D;100 <br><br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-1<br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-2 <br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinkgroups &#x3D; g1 <br>a1.sinkgroups.g1.sinks &#x3D; k1 k2 <br>a1.sinkgroups.g1.processor.type &#x3D; failover(故障转移)   or    load_<em>balance(随机)<br>a1.sinkgroups.g1.processor.selector &#x3D; random or    round</em>_<em>robin# 默认是故障转移的不写就是故障转移，写了就是随机random是随机的意思，而round</em>_robin是轮询的意思<br><br>a1.sources.r1.channels&#x3D;c1 <br>a1.sinks.k1.channel&#x3D;c1 <br>a1.sinks.k2.channel&#x3D;c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Sink-Processors-sink处理器&quot;&gt;&lt;a href=&quot;#Sink-Processors-sink处理器&quot; class=&quot;headerlink&quot; title=&quot;Sink Processors sink处理器&quot;&gt;&lt;/a&gt;Sink Processors s</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume source的type</title>
    <link href="https://bigdata-yx.github.io/posts/fe56.html"/>
    <id>https://bigdata-yx.github.io/posts/fe56.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。</p><p>spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文件，并在新文件处显示从新文件中解析出来。 与exec source不同，spooling directory source是可靠的， 即使flume重新启动或被kill，也不会丢失数据，同时作为这种可靠的代价，指定目录中的被手机的文件必须是不可变的、唯一命名的。flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常；</p><p>taildir source 监控指定的一些文件，并在检测新的一行数据残生的时候几乎实时的读取他们，如果新的一行数据还没写完，taildir source 等到这行写完后读取</p><p>kafka source 就是一个apache kafka消费者， 他从kafka的topic中读取消息，如果运行了多个Kafka source 则可以把他们配置到同一个消费者组，以便每个source都读取一组唯一的topic分区</p><p>syslog sources 针对系统日志</p><p>http sources 发送get协议请求的</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。&lt;/p&gt;
&lt;p&gt;spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume</title>
    <link href="https://bigdata-yx.github.io/posts/fe69.html"/>
    <id>https://bigdata-yx.github.io/posts/fe69.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume的netcat 监听端口</title>
    <link href="https://bigdata-yx.github.io/posts/fe61.html"/>
    <id>https://bigdata-yx.github.io/posts/fe61.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="netcat-agent"><a href="#netcat-agent" class="headerlink" title="netcat agent"></a>netcat agent</h1><p>配置文件如下</p><blockquote><p>a1.sources &#x3D; r1&amp;#x20;</p><p>a1.channels &#x3D; c1&amp;#x20;</p><p>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat&amp;#x20;</p><p>=&#x3D;a1.sources.r1.bind &#x3D; localhost &#x3D;&#x3D;</p><p>=&#x3D;a1.sources.r1.port &#x3D; 44444&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory&amp;#x20;</p><p>a1.channels.c1.capacity &#x3D; 10000&amp;#x20;</p><p>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试方法：telnet localhost 44444</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;netcat-agent&quot;&gt;&lt;a href=&quot;#netcat-agent&quot; class=&quot;headerlink&quot; title=&quot;netcat agent&quot;&gt;&lt;/a&gt;netcat agent&lt;/h1&gt;&lt;p&gt;配置文件如下&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a1.</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume Channel Selectors通道选择器</title>
    <link href="https://bigdata-yx.github.io/posts/fe69.html"/>
    <id>https://bigdata-yx.github.io/posts/fe69.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Channel-Selectors通道选择器"><a href="#Channel-Selectors通道选择器" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><h3 id="Channel-Selectors通道选择器-1"><a href="#Channel-Selectors通道选择器-1" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><p>多路复用通道选择器，source是通过event header来决定传输到哪一个channel。source是通过event header来决定传输到哪一个channel</p><p><img src="https://pic1.imgdb.cn/item/6784c37dd0e0a243d4f3d664.png"></p><p><strong>replicating type是复制选择器</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Channel-Selectors通道选择器&quot;&gt;&lt;a href=&quot;#Channel-Selectors通道选择器&quot; class=&quot;headerlink&quot; title=&quot;Channel Selectors通道选择器&quot;&gt;&lt;/a&gt;&lt;strong&gt;Channel Sele</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume Interceptor</title>
    <link href="https://bigdata-yx.github.io/posts/fe68.html"/>
    <id>https://bigdata-yx.github.io/posts/fe68.html</id>
    <published>2025-01-13T07:37:07.000Z</published>
    <updated>2025-01-13T07:37:07.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"><a href="#就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理" class="headerlink" title="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"></a>就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理</h4><p>timestamp Interceptor给event的头信息中添加时间戳</p><p>Static Interceptor 给event的头信息中添加自定义键值</p><p>Host Interceptor给event的头信息中添加主机名或者ip信息</p><p>Search and Replace Interceptor拦截信息进行匹配和替换</p><p>Regex File</p><h2 id="timestamp-interceptor-添加时间戳"><a href="#timestamp-interceptor-添加时间戳" class="headerlink" title="timestamp interceptor(添加时间戳)"></a>timestamp interceptor(添加时间戳)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; timestamp&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="host-interceptor（添加主机信息）"><a href="#host-interceptor（添加主机信息）" class="headerlink" title="host interceptor（添加主机信息）"></a>host interceptor（添加主机信息）</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1 &#x3D;&#x3D;<br>=&#x3D;a1.sources.r1.interceptors.i1.type &#x3D; host&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Static-Interceptor"><a href="#Static-Interceptor" class="headerlink" title="Static Interceptor"></a>Static Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.sources.r1.interceptors  &#x3D; i1 i2 i3</p><p>a1.sources.r1.interceptors.i3.type &#x3D; static<br>a1.sources.r1.interceptors.i3.key &#x3D; name<br>a1.sources.r1.interceptors.i3.value &#x3D; zhangsan</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Search-and-Replace-Interceptor"><a href="#Search-and-Replace-Interceptor" class="headerlink" title="Search and Replace Interceptor"></a>Search and Replace Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; search_replace<br>a1.sources.r1.interceptors.i1.searchPattern&#x3D;[a-z]<br>a1.sources.r1.interceptors.i1.replaceString&#x3D;*&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h2 id="Regex-Filering-Interceptor-过滤的用正则"><a href="#Regex-Filering-Interceptor-过滤的用正则" class="headerlink" title="Regex Filering Interceptor(过滤的用正则)"></a>Regex Filering Interceptor(过滤的用正则)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_filter<br>a1.sources.r1.interceptors.i1.regex&#x3D;^jp.*<br>a1.sources.r1.interceptors.i1.excludeEvents&#x3D;true&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Regex-Extractor-Interceptor-通过正则对event进行捕获"><a href="#Regex-Extractor-Interceptor-通过正则对event进行捕获" class="headerlink" title="Regex Extractor Interceptor(通过正则对event进行捕获)"></a>Regex Extractor Interceptor(通过正则对event进行捕获)</h3><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_extractor<br>a1.sources.r1.interceptors.i1.regex&#x3D;(^[a-zA-Z]&#x3D;&#x3D;<em>&#x3D;&#x3D;)\s([0-9]&#x3D;&#x3D;</em>&#x3D;&#x3D;$)<br>a1.sources.r1.interceptors.i1.serializers&#x3D;s1 s2<br>a1.sources.r1.interceptors.i1.serializers.s1.name&#x3D;word<br>a1.sources.r1.interceptors.i1.serializers.s2.name&#x3D;num&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理&quot;&gt;&lt;a href=&quot;#就是拦截器，拦截器可以将flume收集到的event进行拦</summary>
      
    
    
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/categories/Flume/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flume" scheme="https://bigdata-yx.github.io/tags/Flume/"/>
    
  </entry>
  
</feed>
