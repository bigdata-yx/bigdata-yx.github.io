<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>曦</title>
  <icon>https://bigdata-yx.github.io/imgs/avatar.webp</icon>
  <subtitle>曦</subtitle>
  <link href="https://bigdata-yx.github.io/atom.xml" rel="self"/>
  
  <link href="https://bigdata-yx.github.io/"/>
  <updated>2025-05-23T07:17:58.000Z</updated>
  <id>https://bigdata-yx.github.io/</id>
  
  <author>
    <name>曦</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>git学习</title>
    <link href="https://bigdata-yx.github.io/posts/git001.html"/>
    <id>https://bigdata-yx.github.io/posts/git001.html</id>
    <published>2025-05-23T07:17:58.000Z</published>
    <updated>2025-05-23T07:17:58.000Z</updated>
    
    
    
    
    <category term="demo" scheme="https://bigdata-yx.github.io/categories/demo/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="demo" scheme="https://bigdata-yx.github.io/tags/demo/"/>
    
  </entry>
  
  <entry>
    <title>mysql搭建使用</title>
    <link href="https://bigdata-yx.github.io/posts/mysql1.html"/>
    <id>https://bigdata-yx.github.io/posts/mysql1.html</id>
    <published>2025-04-17T01:18:18.000Z</published>
    <updated>2025-04-17T01:18:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Mysql的搭建和使用"><a href="#Mysql的搭建和使用" class="headerlink" title="Mysql的搭建和使用"></a>Mysql的搭建和使用</h2><h3 id="大数据项目主要分为三个模块"><a href="#大数据项目主要分为三个模块" class="headerlink" title="大数据项目主要分为三个模块"></a>大数据项目主要分为三个模块</h3><h4 id="模块一：大数据集群的搭建及其部署优化和数据库运维"><a href="#模块一：大数据集群的搭建及其部署优化和数据库运维" class="headerlink" title="模块一：大数据集群的搭建及其部署优化和数据库运维"></a>模块一：大数据集群的搭建及其部署优化和数据库运维</h4><h4 id="模块二：数据获取以及数据清洗"><a href="#模块二：数据获取以及数据清洗" class="headerlink" title="模块二：数据获取以及数据清洗"></a>模块二：数据获取以及数据清洗</h4><h4 id="模块三：数据可视化"><a href="#模块三：数据可视化" class="headerlink" title="模块三：数据可视化"></a>模块三：数据可视化</h4><h3 id="而Mysql则是模块一中的数据库方面的内容"><a href="#而Mysql则是模块一中的数据库方面的内容" class="headerlink" title="而Mysql则是模块一中的数据库方面的内容"></a>而Mysql则是模块一中的数据库方面的内容</h3><h3 id="废话不多说直接开始："><a href="#废话不多说直接开始：" class="headerlink" title="废话不多说直接开始："></a>废话不多说直接开始：</h3><h3 id="首先是安装Mysql，而安装Mysql则根据不同的操作系统进行安装（这里主要围绕Linux，操作系统是Centos7）"><a href="#首先是安装Mysql，而安装Mysql则根据不同的操作系统进行安装（这里主要围绕Linux，操作系统是Centos7）" class="headerlink" title="首先是安装Mysql，而安装Mysql则根据不同的操作系统进行安装（这里主要围绕Linux，操作系统是Centos7）"></a>首先是安装Mysql，而安装Mysql则根据不同的操作系统进行安装（这里主要围绕Linux，操作系统是Centos7）</h3><h3 id="Windows和Mac的自行找教程-因为比赛使用的是Centos7所以我使用的就是Centos7"><a href="#Windows和Mac的自行找教程-因为比赛使用的是Centos7所以我使用的就是Centos7" class="headerlink" title="[Windows和Mac的自行找教程] 因为比赛使用的是Centos7所以我使用的就是Centos7"></a>[Windows和Mac的自行找教程] 因为比赛使用的是Centos7所以我使用的就是Centos7</h3><h3 id="怎么安装Centos7我录制了一个视频自己看，不喜欢虚拟机的也可以找没用的机装系统"><a href="#怎么安装Centos7我录制了一个视频自己看，不喜欢虚拟机的也可以找没用的机装系统" class="headerlink" title="怎么安装Centos7我录制了一个视频自己看，不喜欢虚拟机的也可以找没用的机装系统"></a>怎么安装Centos7我录制了一个视频自己看，不喜欢虚拟机的也可以找没用的机装系统</h3><h4 id="主分享链接：https-www-123865-com-s-JaZrjv-rifl-提取码-nLDd"><a href="#主分享链接：https-www-123865-com-s-JaZrjv-rifl-提取码-nLDd" class="headerlink" title="主分享链接：https://www.123865.com/s/JaZrjv-rifl   提取码:nLDd"></a>主分享链接：<a href="https://www.123865.com/s/JaZrjv-rifl">https://www.123865.com/s/JaZrjv-rifl</a>   提取码:nLDd</h4><h4 id="备分享链接：https-www-123684-com-s-JaZrjv-rifl-提取码-nLDd"><a href="#备分享链接：https-www-123684-com-s-JaZrjv-rifl-提取码-nLDd" class="headerlink" title="备分享链接：https://www.123684.com/s/JaZrjv-rifl   提取码:nLDd"></a>备分享链接：<a href="https://www.123684.com/s/JaZrjv-rifl">https://www.123684.com/s/JaZrjv-rifl</a>   提取码:nLDd</h4><h3 id="1-安装Mysql"><a href="#1-安装Mysql" class="headerlink" title="1.安装Mysql"></a>1.安装Mysql</h3><h4 id="要安装Mysql得先进行安装Mysql所需要的依赖（也可以使用默认官方的Yum源下载，就是很慢）"><a href="#要安装Mysql得先进行安装Mysql所需要的依赖（也可以使用默认官方的Yum源下载，就是很慢）" class="headerlink" title="要安装Mysql得先进行安装Mysql所需要的依赖（也可以使用默认官方的Yum源下载，就是很慢）"></a>要安装Mysql得先进行安装Mysql所需要的依赖（也可以使用默认官方的Yum源下载，就是很慢）</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先将镜像挂载</span></span><br><span class="line">[root@localhost ~]<span class="comment"># mount /dev/cdrom /mnt</span></span><br><span class="line">mount: /dev/sr0 is write-protected, mounting read-only</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看/mnt下有东西就是挂载成功了</span></span><br><span class="line">[root@localhost mnt]<span class="comment"># ls /mnt</span></span><br><span class="line">CentOS_BuildTag  EFI  EULA  GPL  images  isolinux  LiveOS  Packages  repodata  RPM-GPG-KEY-CentOS-7  RPM-GPG-KEY-CentOS-Testing-7  TRANS.TBL</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后去弄一个Yum</span></span><br><span class="line">[root@localhost mnt]<span class="comment"># cd /etc/yum.repos.d/</span></span><br><span class="line">[root@localhost yum.repos.d]<span class="comment"># ls</span></span><br><span class="line">CentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repo</span><br><span class="line">CentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo</span><br><span class="line">[root@localhost yum.repos.d]<span class="comment"># rm -rf *</span></span><br><span class="line">[root@localhost yum.repos.d]<span class="comment"># vi yum.repo</span></span><br><span class="line"><span class="comment"># 然后摁一下i键就可以进行编写了，编写完之后摁esc然后:wq就可以保存并退出了，应该都有Linux基础吧</span></span><br><span class="line"><span class="comment"># 最后更新元数据缓存就可以了</span></span><br><span class="line">[root@localhost yum.repos.d]<span class="comment"># yum makecache</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后进行安装依赖包</span></span><br><span class="line">[root@localhost yum.repos.d]<span class="comment"># yum install net-tools perl lrzsz -y</span></span><br></pre></td></tr></table></figure><h4 id="然后正式开始安装Mysql，我这里使用rpm包安装"><a href="#然后正式开始安装Mysql，我这里使用rpm包安装" class="headerlink" title="然后正式开始安装Mysql，我这里使用rpm包安装"></a>然后正式开始安装Mysql，我这里使用rpm包安装</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先将Mysql的压缩包放进Centos7中来</span></span><br><span class="line"><span class="comment"># 选定目录然后直接拖进来或者使用rz命令都可以，我这里使用rz来教学一波.如果出现乱码不要慌张，断开链接重新连接然后重新导入进来就可以了</span></span><br><span class="line"><span class="comment"># 导入完后该目录下就会出现你的包</span></span><br><span class="line">[root@localhost opt]<span class="comment"># ls</span></span><br><span class="line">mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后进行解压，解压完后/root目录下就会有很多rpm包</span></span><br><span class="line">[root@localhost opt]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /root/</span></span><br><span class="line">mysql-community-client-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-common-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-devel-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-devel-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-server-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-test-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@localhost opt]<span class="comment"># ls /root/</span></span><br><span class="line">anaconda-ks.cfg                                 mysql-community-embedded-5.7.44-1.el7.x86_64.rpm         mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-client-5.7.44-1.el7.x86_64.rpm  mysql-community-embedded-compat-5.7.44-1.el7.x86_64.rpm  mysql-community-server-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-common-5.7.44-1.el7.x86_64.rpm  mysql-community-embedded-devel-5.7.44-1.el7.x86_64.rpm   mysql-community-test-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-devel-5.7.44-1.el7.x86_64.rpm   mysql-community-libs-5.7.44-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对rpm包进行安装，并且初始化以及启动Mysql</span></span><br><span class="line"><span class="comment"># 首先我们得先清理掉旧版的mariadb-libs</span></span><br><span class="line">[root@localhost ~]<span class="comment"># rpm -e --nodeps mariadb-libs</span></span><br><span class="line"><span class="comment"># 然后再进行安装</span></span><br><span class="line">[root@localhost ~]<span class="comment"># cd /root/</span></span><br><span class="line">[root@localhost ~]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm</span></span><br><span class="line">[root@localhost ~]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm</span></span><br><span class="line">[root@localhost ~]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm</span></span><br><span class="line">[root@localhost ~]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm</span></span><br><span class="line">[root@localhost ~]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后进行初始化</span></span><br><span class="line">[root@localhost ~]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br><span class="line"><span class="comment"># 最后启动Mysql</span></span><br><span class="line">[root@localhost ~]<span class="comment"># systemctl start mysqld</span></span><br></pre></td></tr></table></figure><h3 id="2-使用Mysql"><a href="#2-使用Mysql" class="headerlink" title="2.使用Mysql"></a>2.使用Mysql</h3><h4 id="既然已经安装完Mysql了，我们该如何去使用呢"><a href="#既然已经安装完Mysql了，我们该如何去使用呢" class="headerlink" title="既然已经安装完Mysql了，我们该如何去使用呢"></a>既然已经安装完Mysql了，我们该如何去使用呢</h4><h4 id="首先我们先登录进Mysql"><a href="#首先我们先登录进Mysql" class="headerlink" title="首先我们先登录进Mysql"></a>首先我们先登录进Mysql</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># mysql -uroot -p</span></span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection <span class="built_in">id</span> is 3</span><br><span class="line">Server version: 5.7.44 MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2023, Oracle and/or its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> or <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> <span class="built_in">help</span>. Type <span class="string">&#x27;\c&#x27;</span> to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"><span class="comment"># 登录进去后就是这样的，这条命令中的-u后面是用户名，-p后面是密码，一般不直接在命令中写密码，因为没有设置了空密码，所以提示输入密码的时候直接回车就可以了</span></span><br></pre></td></tr></table></figure><h3 id="3-创建数据库"><a href="#3-创建数据库" class="headerlink" title="3.创建数据库"></a>3.创建数据库</h3><h4 id="既然已经能够正常运行数据库了，这个时候我们就得需要对数据库进行增删改查的操作了"><a href="#既然已经能够正常运行数据库了，这个时候我们就得需要对数据库进行增删改查的操作了" class="headerlink" title="既然已经能够正常运行数据库了，这个时候我们就得需要对数据库进行增删改查的操作了"></a>既然已经能够正常运行数据库了，这个时候我们就得需要对数据库进行增删改查的操作了</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个数据库(使用<span class="keyword">create</span> database <span class="operator">+</span> 数据库名称)</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database bigdata;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line"># 然后是使用数据库(使用use <span class="operator">+</span> 数据库名称)</span><br><span class="line">mysql<span class="operator">&gt;</span> use bigdata;</span><br><span class="line">Database changed</span><br><span class="line"></span><br><span class="line"># 要是不知道你有什么数据库，就可以查看所有数据库(使用<span class="keyword">show</span> databases)</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> bigdata            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> sys                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line"># 要是你不想要这个数据库了，你可以进行删除，删库跑路(<span class="keyword">drop</span> <span class="operator">+</span> 数据库名称)</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">drop</span> database bigdata;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h3 id="4-创建表"><a href="#4-创建表" class="headerlink" title="4.创建表"></a>4.创建表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># 有了数据库，你的库里面肯定得有表才能进行存放数据嘛</span><br><span class="line"># 创建表( <span class="keyword">create</span> <span class="keyword">table</span> <span class="operator">+</span> 表名称(字段 类型); )</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> hkjcpdd(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> id <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> name <span class="type">varchar</span>(<span class="number">255</span>));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br><span class="line"></span><br><span class="line"># 查看这个库中的所有表</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_bigdata <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> hkjcpdd           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line"># 查看某个表的详细信息</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> hkjcpdd;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field <span class="operator">|</span> Type         <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> id    <span class="operator">|</span> <span class="type">int</span>(<span class="number">11</span>)      <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">255</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line"># 查看表中的所有信息(<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> 表名，<span class="operator">*</span>代表所有的意思)因为这个表没数据所以没东西</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hkjcpdd;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line"># 删除表(<span class="keyword">drop</span> <span class="keyword">table</span> 表名)</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> hkjcpdd;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h3 id="5-增加数据，这里使用一个常用的"><a href="#5-增加数据，这里使用一个常用的" class="headerlink" title="5.增加数据，这里使用一个常用的"></a>5.增加数据，这里使用一个常用的</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># 增加数据( <span class="keyword">insert</span> <span class="keyword">into</span> [表名] <span class="keyword">value</span> <span class="keyword">values</span> )</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> hkjcpdd <span class="keyword">value</span> (<span class="number">1</span>, <span class="string">&#x27;hkj&#x27;</span>), (<span class="number">2</span>, <span class="string">&#x27;dcx&#x27;</span>), (<span class="number">3</span>, <span class="string">&#x27;cbc&#x27;</span>), (<span class="number">4</span>, <span class="string">&#x27;o</span></span><br><span class="line"><span class="string">ojh&#x27;</span>);</span><br><span class="line">Query OK, <span class="number">4</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line">Records: <span class="number">4</span>  Duplicates: <span class="number">0</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line"># 然后查看数据是否添加进去了</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hkjcpdd;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="operator">|</span> id   <span class="operator">|</span> name <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">1</span> <span class="operator">|</span> hkj  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">2</span> <span class="operator">|</span> dcx  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">3</span> <span class="operator">|</span> cbc  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">4</span> <span class="operator">|</span> ojh  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line"># 删除数据(<span class="keyword">delete</span> <span class="keyword">from</span> 表名 <span class="keyword">where</span> 条件)</span><br><span class="line"># 比如我想删除名字为cbc的数据</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">delete</span> <span class="keyword">from</span> hkjcpdd <span class="keyword">where</span> name<span class="operator">=</span><span class="string">&#x27;cbc&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hkjcpdd;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="operator">|</span> id   <span class="operator">|</span> name <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">1</span> <span class="operator">|</span> hkj  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">2</span> <span class="operator">|</span> dcx  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">4</span> <span class="operator">|</span> ojh  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line"># 修改数据(<span class="keyword">update</span> [表] <span class="keyword">set</span> [修改内容<span class="number">1</span>, 修改内容<span class="number">2</span>, ....] <span class="keyword">where</span>  [条件];)</span><br><span class="line"># 比如我想修改名字为hkj的修改为hkjcpdd</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> hkjcpdd <span class="keyword">set</span> name<span class="operator">=</span><span class="string">&#x27;hkjcpdd&#x27;</span> <span class="keyword">where</span> name<span class="operator">=</span><span class="string">&#x27;hkj&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hkjcpdd;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+---------+</span></span><br><span class="line"><span class="operator">|</span> id   <span class="operator">|</span> name    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+---------+</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">1</span> <span class="operator">|</span> hkjcpdd <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">2</span> <span class="operator">|</span> dcx     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">4</span> <span class="operator">|</span> ojh     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+---------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="6-查找查看数据"><a href="#6-查找查看数据" class="headerlink" title="6.查找查看数据"></a>6.查找查看数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 查看查找数据就是根据不同的条件找出需要的数据</span><br><span class="line"># 命令：<span class="keyword">select</span> 显示的内容 <span class="keyword">from</span> 表名 <span class="keyword">where</span> 条件;</span><br><span class="line"># 比如我想查看id为<span class="number">2</span>的是哪个人</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hkjcpdd <span class="keyword">where</span> id<span class="operator">=</span><span class="number">2</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="operator">|</span> id   <span class="operator">|</span> name <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">2</span> <span class="operator">|</span> dcx  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line"># 比如我想知道hkjcpdd是什么id</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> id <span class="keyword">from</span> hkjcpdd <span class="keyword">where</span> name<span class="operator">=</span><span class="string">&#x27;hkjcpdd&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> id   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span>    <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Mysql的搭建和使用&quot;&gt;&lt;a href=&quot;#Mysql的搭建和使用&quot; class=&quot;headerlink&quot; title=&quot;Mysql的搭建和使用&quot;&gt;&lt;/a&gt;Mysql的搭建和使用&lt;/h2&gt;&lt;h3 id=&quot;大数据项目主要分为三个模块&quot;&gt;&lt;a href=&quot;#大数据项</summary>
      
    
    
    
    <category term="mysql搭建使用" scheme="https://bigdata-yx.github.io/categories/mysql%E6%90%AD%E5%BB%BA%E4%BD%BF%E7%94%A8/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="mysql" scheme="https://bigdata-yx.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>mysql运维</title>
    <link href="https://bigdata-yx.github.io/posts/a403.html"/>
    <id>https://bigdata-yx.github.io/posts/a403.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>（1）查看当前 MySQL 服务器状态和版本信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> status 或者 \s</span><br></pre></td></tr></table></figure><p>（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE house_market <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8mb4;</span><br></pre></td></tr></table></figure><p>（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">all</span> privileges <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure><p>（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# mkdir mysqlBackup</span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# vi mysqlBackup.sh</span><br><span class="line">#<span class="operator">!</span><span class="operator">/</span>bin<span class="operator">/</span>bash</span><br><span class="line"></span><br><span class="line">DB_NAME<span class="operator">=</span>&quot;house_market&quot;</span><br><span class="line">BACKUP_DIR<span class="operator">=</span>&quot;/root/mysqlBackup&quot;</span><br><span class="line"><span class="type">DATE</span><span class="operator">=</span>$(<span class="type">date</span> <span class="operator">+</span>&quot;%Y%m%d%H%M%S&quot;)</span><br><span class="line">BACKUP_FILE<span class="operator">=</span>&quot;$BACKUP_DIR/$DB_NAME-$DATE.sql&quot;</span><br><span class="line"></span><br><span class="line">mysqldump <span class="operator">-</span>u root <span class="operator">-</span>p<span class="string">&#x27;123456&#x27;</span> $DB_NAME <span class="operator">&gt;</span> $BACKUP_FILE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# chmod <span class="operator">-</span>x mysqlBackup.sh</span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# crontab <span class="operator">-</span>e</span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">/</span>root<span class="operator">/</span>mysqlBackup.sh</span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# crontab <span class="operator">-</span>l</span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">/</span>root<span class="operator">/</span>mysqlBackup.sh</span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# sh mysqlBackup.sh </span><br><span class="line">mysqldump: [Warning] <span class="keyword">Using</span> a password <span class="keyword">on</span> the command line interface can be insecure.</span><br><span class="line">[root<span class="variable">@master</span> mysqlBackup]# ls</span><br><span class="line">house_market<span class="number">-20250217194213.</span><span class="keyword">sql</span></span><br></pre></td></tr></table></figure><p>（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> max_connections <span class="operator">=</span> <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> query_cache_size<span class="operator">=</span><span class="number">64</span><span class="operator">*</span><span class="number">1024</span><span class="operator">*</span><span class="number">1024</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（7）查看 house_market 数据库中所有表的存储引擎类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLE</span> STATUS <span class="keyword">FROM</span> house_market;</span><br></pre></td></tr></table></figure><p>（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> BACKUP_ADMIN,RELOAD <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables <span class="keyword">from</span> mysql;</span><br></pre></td></tr></table></figure><p>（5）创建数据库 education 并设置正确的字符集</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database education <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）授予 eduadmin 用户对学习数据库的查询权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> education.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;eduadmin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（4）创建新的用户 bike_admin</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（5）创建数据库 bike_data，并设置正确的字符集</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database bike_data <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br></pre></td></tr></table></figure><p>（6）授予新用户查询数据和插入数据的权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span> <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（1）配置 MySQL 服务器的最大连接数为 1000</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> max_connections <span class="operator">=</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure><p>（2）创建数据库 tourism 并设置默认字符集为 utf8mb4</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database tourism <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br></pre></td></tr></table></figure><p>（3）创建一个名为 tourism_backup 的定时备份脚本，每天凌晨 2 点自动备份数据库 tourism</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#<span class="operator">!</span><span class="operator">/</span>bin<span class="operator">/</span>bash</span><br><span class="line"></span><br><span class="line">db_name<span class="operator">=</span>&quot;tourism&quot;</span><br><span class="line">backup_dir<span class="operator">=</span>&quot;/opt/tourism_backup&quot;</span><br><span class="line"><span class="type">date</span><span class="operator">=</span>$(<span class="type">date</span> <span class="operator">+</span>&quot;%Y%m%d%H%M%S&quot;)</span><br><span class="line">backup_file<span class="operator">=</span>&quot;$backup_dir/$db_name-$date.sql&quot;</span><br><span class="line"></span><br><span class="line">mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p<span class="string">&#x27;123456&#x27;</span> $db_name <span class="operator">&gt;</span> $backup_file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# chmod <span class="operator">-</span>x tourism_backup.sh </span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# crontab <span class="operator">-</span>e</span><br><span class="line"><span class="keyword">no</span> crontab <span class="keyword">for</span> root <span class="operator">-</span> <span class="keyword">using</span> an <span class="keyword">empty</span> <span class="keyword">one</span></span><br><span class="line">crontab: installing <span class="keyword">new</span> crontab</span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# crontab <span class="operator">-</span>l</span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>tourism_backup.sh</span><br></pre></td></tr></table></figure><p>（4）优化 MySQL 查询性能，开启慢查询日志，设置超过 3 秒的查询被记录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> slow_query_log<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> long_query_time<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><p>（5）配置 MySQL 的 binlog 日志，设置过期时间为 7 天</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> expire_logs_days <span class="operator">=</span> <span class="number">7</span>;</span><br></pre></td></tr></table></figure><p>（2）启用 MySQL 慢查询日志功能，设置慢查询阈值为 2 秒</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> slow_query_log <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> long_query_time <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>（3）创建名为 hadoop 的数据库用户，密码设置为 Hadoop@2025</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;Hadoop@2025&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（4）为 hadoop 用户授予 bigdata 数据库的 SELECT、INSERT 和 UPDATE 权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span>, <span class="keyword">update</span> <span class="keyword">on</span> bigdata.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure><p>（5）创建名为 monitor 的监控专用用户，仅授予 PROCESS 和 SHOW DATABASES 权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> process, <span class="keyword">show</span> databases <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure><p>（6）使用 mysqldump 工具备份 bigdata 数据库，将备份文件保存为 bigdata_backup.sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p bigdata <span class="operator">&gt;</span> bigdata_backup.sql</span><br><span class="line">Enter password: </span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# ls</span><br><span class="line">bigdata_backup.sql</span><br></pre></td></tr></table></figure><p>（7）配置数据库字符集为 utf8mb4，并将默认排序规则设置为 utf8mb4_unicode_ci</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> character_set_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> collation_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4_unicode_ci&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（8）创建一个定时备份脚本，实现每天凌晨 2 点自动备份 bigdata 数据库到 &#x2F;opt&#x2F;backup 目录，并只保留最近 7 天的备份文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#<span class="operator">!</span><span class="operator">/</span>bin<span class="operator">/</span>bash</span><br><span class="line">db_name<span class="operator">=</span>&quot;bigdata&quot;</span><br><span class="line">backup_dir<span class="operator">=</span>&quot;/opt/backup&quot;</span><br><span class="line"><span class="type">date</span><span class="operator">=</span>$(<span class="type">date</span> <span class="operator">+</span>&quot;%Y%m%d%H%M%S&quot;)</span><br><span class="line">backup_file<span class="operator">=</span>&quot;$backup_dir/$db_name-$date.sql&quot;</span><br><span class="line"></span><br><span class="line">mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p<span class="string">&#x27;123456&#x27;</span> $db_name <span class="operator">&gt;</span> $backup_file</span><br><span class="line"></span><br><span class="line">find &quot;$backup_dir&quot; <span class="operator">-</span>type f <span class="operator">-</span>name &quot;$&#123;db_name&#125;-*.sql&quot; <span class="operator">-</span>mtime <span class="operator">+</span><span class="number">7</span> <span class="operator">-</span><span class="keyword">exec</span> rm &#123;&#125; \;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;（1）查看当前 MySQL 服务器状态和版本信息&lt;/p&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题1</title>
    <link href="https://bigdata-yx.github.io/posts/a298.html"/>
    <id>https://bigdata-yx.github.io/posts/a298.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>1</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>在大数据技术快速发展的今天，房地产市场正经历着数字化转型。传统的房地产交易和分析模式主要依赖经纪人的个人经验和直觉，这种方式不仅效率低下，而且难以准确把握市场动态。随着大数据技术的应用，房地产行业正在向数据驱动的决策模式转变，这使得市场分析更加精准，服务更加个性化。</p><p>房地产大数据分析平台通过采集和处理海量的交易数据、用户行为数据和市场环境数据，可以全方位地描绘市场格局。这些数据包括但不限于房源基本信息、交易历史、区域配套设施、用户浏览轨迹、市场成交周期等。通过对这些数据的深度分析，可以准确预测房价走势、评估投资价值、识别市场机会，从而为购房者、房产经纪和开发商提供数据支持。为完成二手房销售数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的SSH免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><h4 id="3．子任务三：Zookeeper-集群安装配置"><a href="#3．子任务三：Zookeeper-集群安装配置" class="headerlink" title="3．子任务三：Zookeeper 集群安装配置"></a>3．子任务三：Zookeeper 集群安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）在 master 节点将 &#x2F;opt&#x2F;software 目录下的 apache-zookeeper-3.8.3-bin.tar.gz 包解压到 &#x2F;opt&#x2F;module 路径下， 将解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><p>（2）把解压后的 apache-zookeeper-3.8.3-bin 文件夹更名为 zookeeper-3.8.3，将命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）设置 zookeeper 环境变量，将新增的环境变量内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）创建 zookeeper 配置文件 zoo.cfg 并配置 master、slave1、slave2 三个节点的集群配置，其中 dataDir 参数设置为 &#x2F;opt&#x2F;module&#x2F;zookeeper-3.8.3&#x2F;data ，提交 zoo.cfg 配置内容至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 master 节点上创建文件 myid 用于标识服务器序号，并将文件内容设置为1；</p><p>（6）在 master 节点上将配置的 zookeeper 环境变量文件及 zookeeper 解压包拷贝到 slave1、slave2 节点，提交命令至【提交结果.docx】中对应的任务序号下；</p><p>（7）在 slave1 节点上修改 myid 文件内容修改为 2，在 slave2 节点上修改 myid 文件内容修改为 3，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）在 master 节点、slave1 节点、slave2 节点分别启动 zookeeper，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下； </p><p>（9）在 master 节点、slave1 节点、slave2 节点分别查看 zookeeper 的状态，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（10）在 master 节点查看 Java 进程，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><h4 id="4．子任务四：Kafka-安装配置"><a href="#4．子任务四：Kafka-安装配置" class="headerlink" title="4．子任务四：Kafka 安装配置"></a>4．子任务四：Kafka 安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）从 master 中的 &#x2F;opt&#x2F;software 目录下将文件 kafka_2.12-3.6.1.tgz 解压到 &#x2F;opt&#x2F;module 目录下，把解压后的 kafka_2.12-3.6.1 文件夹更名为 kafka，将 Kafka 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><p>（2）配置好 zookeeper，其中 zookeeper 使用集群模式，分别将 master、slave1、slave2 作为其节点（若 zookpeer 已安装配置好，则无需再次配置）；</p><p>（3）配置 Kafka 环境变量，并使环境变量生效，将新增的环境变量内容截图粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4） 使用 kafka-server-start.sh --version 查看 Kafka 的版本内容， 并将命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><p>（5）修改 server.properties 配置文件，并分发 Kafka 文件到 slave1、slave2 中，并在每个节点启动 Kafka，将启动命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><p>（6）创建 Topic，其中 Topic 名称为 installtopic，分区数为 2，副本数为 2，将创建命令和创建成果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2. 子任务二：MySQL 运维"></a>2. 子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><h4 id="（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3. 子任务三：数据表的创建及维护"></a>3. 子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 house_market 数据库中创建房源信息表（house_info）。房源信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>主键</td></tr><tr><td><strong>community</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>layout</strong></td><td>varchar(50)</td><td>户型</td><td>不能为空</td></tr><tr><td><strong>area</strong></td><td>decimal(10,2)</td><td>建筑面积</td><td>不能为空</td></tr><tr><td><strong>price</strong></td><td>decimal(12,2)</td><td>挂牌价格</td><td>不能为空</td></tr><tr><td><strong>floor_info</strong></td><td>varchar(50)</td><td>楼层信息</td><td></td></tr><tr><td><strong>orientation</strong></td><td>varchar(50)</td><td>朝向</td><td></td></tr><tr><td><strong>status</strong></td><td>enum</td><td>房源状态</td><td>默认”在售”</td></tr></tbody></table><p>（2）根据以下数据字段在 house_market 数据库中创建小区信息表（community_info）。小区信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>community_id</strong></td><td>int</td><td>小区编号</td><td>主键</td></tr><tr><td><strong>community_name</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>build_year</strong></td><td>year</td><td>建成年份</td><td></td></tr><tr><td><strong>property_fee</strong></td><td>decimal(10,2)</td><td>物业费用</td><td></td></tr><tr><td><strong>subway_distance</strong></td><td>int</td><td>地铁距离</td><td>单位：米</td></tr></tbody></table><p>（3）根据以下数据字段在 house_market 数据库中创建交易记录表（transaction_records）。交易记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>record_id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>外键</td></tr><tr><td><strong>transaction_date</strong></td><td>date</td><td>成交日期</td><td>不能为空</td></tr><tr><td><strong>transaction_price</strong></td><td>decimal(12,2)</td><td>成交价格</td><td>不能为空</td></tr><tr><td><strong>price_change</strong></td><td>decimal(12,2)</td><td>价格变动</td><td></td></tr><tr><td><strong>transaction_type</strong></td><td>varchar(50)</td><td>交易类型</td><td>默认”二手房”</td></tr></tbody></table><p>将这三个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（4）将提供的数据文件 house_market_data.sql 导入数据库 house_market中并编写如下 SQL 查询语句：</p><ul><li><p>统计每个区域的在售房源数量和平均价格</p></li><li><p>查询成交价格高于该区域平均成交价格的房源信息</p></li><li><p>查询距离地铁站1000米以内的小区及其房源数量</p></li><li><p>统计每种户型的平均单价（按面积计算）并按降序排列</p></li><li><p>查询最近一个月内成交的房源信息及其所在小区详情</p></li></ul><p>将这五个 SQL 查询语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题1解析</title>
    <link href="https://bigdata-yx.github.io/posts/a398.html"/>
    <id>https://bigdata-yx.github.io/posts/a398.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>1</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>在大数据技术快速发展的今天，房地产市场正经历着数字化转型。传统的房地产交易和分析模式主要依赖经纪人的个人经验和直觉，这种方式不仅效率低下，而且难以准确把握市场动态。随着大数据技术的应用，房地产行业正在向数据驱动的决策模式转变，这使得市场分析更加精准，服务更加个性化。</p><p>房地产大数据分析平台通过采集和处理海量的交易数据、用户行为数据和市场环境数据，可以全方位地描绘市场格局。这些数据包括但不限于房源基本信息、交易历史、区域配套设施、用户浏览轨迹、市场成交周期等。通过对这些数据的深度分析，可以准确预测房价走势、评估投资价值、识别市场机会，从而为购房者、房产经纪和开发商提供数据支持。为完成二手房销售数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p><img src="https://pic1.imgdb.cn/item/67b2e605d0e0a243d400288a.png"></p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dyn...</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dyna...</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDr...</span><br><span class="line">Feb 17 02:46:40 master systemd[1]: Stopping firewalld - dynamic firewall.....</span><br><span class="line">Feb 17 02:46:40 master systemd[1]: Stopped firewalld - dynamic firewall ...n.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的SSH免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Mon Feb 17 02:49:05 2025 from master</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Mon Feb 17 02:49:10 2025 from master</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo scp -r hadoop/ slave1:/opt/module</span><br><span class="line">sudo scp -r hadoop/ slave2:/opt/module</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br><span class="line">[hadoop@slave2 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-17 03:30:54,631 INFO namenode.FSImage: Allocated new BlockPoolId: BP-515666401-192.168.1.91-1739781054627</span><br><span class="line">2025-02-17 03:30:54,637 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-17 03:30:54,653 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-17 03:30:54,714 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-17 03:30:54,718 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-17 03:30:54,733 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-17 03:30:54,733 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-17 03:30:54,735 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-17 03:30:54,735 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">8672 SecondaryNameNode</span><br><span class="line">9478 WebAppProxyServer</span><br><span class="line">9622 Jps</span><br><span class="line">8491 DataNode</span><br><span class="line">8907 ResourceManager</span><br><span class="line">9228 NodeManager</span><br><span class="line">9565 JobHistoryServer</span><br></pre></td></tr></table></figure><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ jps</span><br><span class="line">2565 DataNode</span><br><span class="line">2664 NodeManager</span><br><span class="line">2780 Jps</span><br></pre></td></tr></table></figure><h4 id="3．子任务三：Zookeeper-集群安装配置"><a href="#3．子任务三：Zookeeper-集群安装配置" class="headerlink" title="3．子任务三：Zookeeper 集群安装配置"></a>3．子任务三：Zookeeper 集群安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）在 master 节点将 &#x2F;opt&#x2F;software 目录下的 apache-zookeeper-3.8.3-bin.tar.gz 包解压到 &#x2F;opt&#x2F;module 路径下， 将解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）把解压后的 apache-zookeeper-3.8.3-bin 文件夹更名为 zookeeper-3.8.3，将命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7</span></span><br></pre></td></tr></table></figure><p>（3）设置 zookeeper 环境变量，将新增的环境变量内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）创建 zookeeper 配置文件 zoo.cfg 并配置 master、slave1、slave2 三个节点的集群配置，其中 dataDir 参数设置为 &#x2F;opt&#x2F;module&#x2F;zookeeper-3.8.3&#x2F;data ，提交 zoo.cfg 配置内容至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper-3.5.7/data</span><br><span class="line"></span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=slave1:2888:3888</span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure><p>（5）在 master 节点上创建文件 myid 用于标识服务器序号，并将文件内容设置为1；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master data]<span class="comment"># cat myid </span></span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>（6）在 master 节点上将配置的 zookeeper 环境变量文件及 zookeeper 解压包拷贝到 slave1、slave2 节点，提交命令至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># scp -r zookeeper-3.5.7/ slave1:`pwd` &amp;&amp; scp -r zookeeper-3.5.7/ slave2:`pwd` &amp;&amp; scp /etc/profile slave1:/etc &amp;&amp; scp /etc/profile slave2:/etc</span></span><br></pre></td></tr></table></figure><p>（7）在 slave1 节点上修改 myid 文件内容修改为 2，在 slave2 节点上修改 myid 文件内容修改为 3，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slave1 data]<span class="comment"># echo &quot;2&quot; &gt; myid</span></span><br><span class="line">[root@slave2 data]<span class="comment"># echo &quot;3&quot; &gt; myid</span></span><br></pre></td></tr></table></figure><p>（8）在 master 节点、slave1 节点、slave2 节点分别启动 zookeeper，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">[root@slave1 data]<span class="comment"># zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">[root@slave2 data]<span class="comment"># zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><p>（9）在 master 节点、slave1 节点、slave2 节点分别查看 zookeeper 的状态，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@master logs]<span class="comment"># zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line">[root@slave1 data]<span class="comment"># zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br><span class="line">[root@slave2 data]<span class="comment"># zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><p>（10）在 master 节点查看 Java 进程，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master logs]<span class="comment"># jps</span></span><br><span class="line">8672 SecondaryNameNode</span><br><span class="line">10309 QuorumPeerMain</span><br><span class="line">9478 WebAppProxyServer</span><br><span class="line">10393 Jps</span><br><span class="line">8491 DataNode</span><br><span class="line">8907 ResourceManager</span><br><span class="line">9228 NodeManager</span><br><span class="line">9565 JobHistoryServer</span><br></pre></td></tr></table></figure><h4 id="4．子任务四：Kafka-安装配置"><a href="#4．子任务四：Kafka-安装配置" class="headerlink" title="4．子任务四：Kafka 安装配置"></a>4．子任务四：Kafka 安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）从 master 中的 &#x2F;opt&#x2F;software 目录下将文件 kafka_2.12-3.6.1.tgz 解压到 &#x2F;opt&#x2F;module 目录下，把解压后的 kafka_2.12-3.6.1 文件夹更名为 kafka，将 Kafka 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf kafka_2.12-3.6.1.tgz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）配置好 zookeeper，其中 zookeeper 使用集群模式，分别将 master、slave1、slave2 作为其节点（若 zookpeer 已安装配置好，则无需再次配置）；</p><p>（3）配置 Kafka 环境变量，并使环境变量生效，将新增的环境变量内容截图粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KAFKA_HOME=/opt/module/kafka</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4） 使用 kafka-server-start.sh --version 查看 Kafka 的版本内容， 并将命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># kafka-server-start.sh --version</span></span><br><span class="line">[2025-02-17 03:54:04,061] INFO Registered kafka:<span class="built_in">type</span>=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)</span><br><span class="line">3.6.1</span><br></pre></td></tr></table></figure><p>（5）修改 server.properties 配置文件，并分发 Kafka 文件到 slave1、slave2 中，并在每个节点启动 Kafka，将启动命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master kafka]<span class="comment"># bin/kafka-server-start.sh config/server.properties</span></span><br><span class="line"></span><br><span class="line">[root@slave1 kafka]<span class="comment"># bin/kafka-server-start.sh config/server.properties </span></span><br><span class="line"></span><br><span class="line">[root@slave2 kafka]<span class="comment"># bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure><p>（6）创建 Topic，其中 Topic 名称为 installtopic，分区数为 2，副本数为 2，将创建命令和创建成果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master kafka]<span class="comment"># kafka-topics.sh --create --bootstrap-server master:9092 --partitions 2 --replication-factor 2 --topic installtopic</span></span><br><span class="line">Created topic installtopic.</span><br></pre></td></tr></table></figure><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpmwarning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">   </span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2. 子任务二：MySQL 运维"></a>2. 子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><h4 id="（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> version();</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> version() <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5.7</span><span class="number">.44</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> status;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name                                 <span class="operator">|</span> <span class="keyword">Value</span>                                                                                                                                                                                                                                                                                                                                                                                                                                                               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Aborted_clients                               <span class="operator">|</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h4 id="（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE house_market <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h4 id="（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">all</span> privileges <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h4 id="（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># mkdir mysqlBackup</span></span><br><span class="line">[root@master ~]<span class="comment"># vi mysqlBackup.sh</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">DB_NAME=<span class="string">&quot;house_market&quot;</span></span><br><span class="line">BACKUP_DIR=<span class="string">&quot;/root/mysqlBackup&quot;</span></span><br><span class="line">DATE=$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M%S&quot;</span>)</span><br><span class="line">BACKUP_FILE=<span class="string">&quot;<span class="variable">$BACKUP_DIR</span>/<span class="variable">$DB_NAME</span>-<span class="variable">$DATE</span>.sql&quot;</span></span><br><span class="line"></span><br><span class="line">mysqldump -u root -p<span class="string">&#x27;123456&#x27;</span> <span class="variable">$DB_NAME</span> &gt; <span class="variable">$BACKUP_FILE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># chmod -x mysqlBackup.sh</span></span><br><span class="line">[root@master ~]<span class="comment"># crontab -e</span></span><br><span class="line">0 2 * * * /root/mysqlBackup.sh</span><br><span class="line">[root@master ~]<span class="comment"># crontab -l</span></span><br><span class="line">0 2 * * * /root/mysqlBackup.sh</span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># sh mysqlBackup.sh </span></span><br><span class="line">mysqldump: [Warning] Using a password on the <span class="built_in">command</span> line interface can be insecure.</span><br><span class="line">[root@master mysqlBackup]<span class="comment"># ls</span></span><br><span class="line">house_market-20250217194213.sql</span><br></pre></td></tr></table></figure><h4 id="（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line">+-----------------+-------+</span><br><span class="line">| Variable_name   | Value |</span><br><span class="line">+-----------------+-------+</span><br><span class="line">| max_connections | 151   |</span><br><span class="line">+-----------------+-------+</span><br><span class="line">1 row <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; show variables like <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br><span class="line">+------------------+---------+</span><br><span class="line">| Variable_name    | Value   |</span><br><span class="line">+------------------+---------+</span><br><span class="line">| query_cache_size | 1048576 |</span><br><span class="line">+------------------+---------+</span><br><span class="line">1 row <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure><h4 id="（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> mysqlBackup]# vi <span class="operator">/</span>etc<span class="operator">/</span>my.cnf</span><br><span class="line">max_connections <span class="operator">=</span> <span class="number">1000</span></span><br><span class="line">query_cache_size <span class="operator">=</span> <span class="number">64</span>M</span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> mysqlBackup]# systemctl restart mysqld</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name   <span class="operator">|</span> <span class="keyword">Value</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-------+</span></span><br><span class="line"><span class="operator">|</span> max_connections <span class="operator">|</span> <span class="number">1000</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name    <span class="operator">|</span> <span class="keyword">Value</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+</span></span><br><span class="line"><span class="operator">|</span> query_cache_size <span class="operator">|</span> <span class="number">67108864</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h4 id="（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLE</span> STATUS <span class="keyword">FROM</span> house_market;</span><br></pre></td></tr></table></figure><h4 id="（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> BACKUP_ADMIN,RELOAD <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3. 子任务三：数据表的创建及维护"></a>3. 子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 house_market 数据库中创建房源信息表（house_info）。房源信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>主键</td></tr><tr><td><strong>community</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>layout</strong></td><td>varchar(50)</td><td>户型</td><td>不能为空</td></tr><tr><td><strong>area</strong></td><td>decimal(10,2)</td><td>建筑面积</td><td>不能为空</td></tr><tr><td><strong>price</strong></td><td>decimal(12,2)</td><td>挂牌价格</td><td>不能为空</td></tr><tr><td><strong>floor_info</strong></td><td>varchar(50)</td><td>楼层信息</td><td></td></tr><tr><td><strong>orientation</strong></td><td>varchar(50)</td><td>朝向</td><td></td></tr><tr><td><strong>status</strong></td><td>enum</td><td>房源状态</td><td>默认”在售”</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> house_info( house_id <span class="type">int</span> <span class="keyword">primary</span> key, community <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">not</span> <span class="keyword">null</span>, district <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, layout <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, area <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">not</span> <span class="keyword">null</span>, price <span class="type">decimal</span>(<span class="number">12</span>,<span class="number">2</span>) <span class="keyword">not</span> <span class="keyword">null</span>, floor_info <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, orientation <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, status enum(<span class="string">&#x27;在售&#x27;</span>) <span class="keyword">default</span> <span class="string">&#x27;在售&#x27;</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 house_market 数据库中创建小区信息表（community_info）。小区信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>community_id</strong></td><td>int</td><td>小区编号</td><td>主键</td></tr><tr><td><strong>community_name</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>build_year</strong></td><td>year</td><td>建成年份</td><td></td></tr><tr><td><strong>property_fee</strong></td><td>decimal(10,2)</td><td>物业费用</td><td></td></tr><tr><td><strong>subway_distance</strong></td><td>int</td><td>地铁距离</td><td>单位：米</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> community_info( community_id <span class="type">int</span> <span class="keyword">primary</span> key, community_naame <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">not</span> <span class="keyword">null</span>, district <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, build_year <span class="keyword">year</span>, prooperty_fee <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">2</span>), subway_distance <span class="type">int</span> comment &quot;单位:米&quot;);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><p>（3）根据以下数据字段在 house_market 数据库中创建交易记录表（transaction_records）。交易记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>record_id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>外键</td></tr><tr><td><strong>transaction_date</strong></td><td>date</td><td>成交日期</td><td>不能为空</td></tr><tr><td><strong>transaction_price</strong></td><td>decimal(12,2)</td><td>成交价格</td><td>不能为空</td></tr><tr><td><strong>price_change</strong></td><td>decimal(12,2)</td><td>价格变动</td><td></td></tr><tr><td><strong>transaction_type</strong></td><td>varchar(50)</td><td>交易类型</td><td>默认”二手房”</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> transaction_records( record_id <span class="type">int</span> <span class="keyword">primary</span> key, house_id <span class="type">int</span>, transaction_date <span class="type">date</span> <span class="keyword">not</span> <span class="keyword">null</span>, transaction_price <span class="type">decimal</span>(<span class="number">12</span>,<span class="number">2</span>) <span class="keyword">not</span> <span class="keyword">null</span>, price_change <span class="type">decimal</span>(<span class="number">12</span>,<span class="number">2</span>), transaction_type <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">default</span> <span class="string">&#x27;二手房&#x27;</span>,<span class="keyword">foreign</span> key (house_id) <span class="keyword">references</span> house_info(house_id));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这三个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（4）将提供的数据文件 house_market_data.sql 导入数据库 house_market中并编写如下 SQL 查询语句：</p><ul><li><p>统计每个区域的在售房源数量和平均价格</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> district, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> house_count, <span class="built_in">avg</span>(price) <span class="keyword">as</span> avg_price <span class="keyword">from</span> house_info <span class="keyword">where</span> status<span class="operator">=</span><span class="string">&#x27;在售&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> district;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> district  <span class="operator">|</span> house_count <span class="operator">|</span> avg_price      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> 丰台区    <span class="operator">|</span>           <span class="number">1</span> <span class="operator">|</span> <span class="number">5000000.000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 朝阳区    <span class="operator">|</span>           <span class="number">2</span> <span class="operator">|</span> <span class="number">8000000.000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 海淀区    <span class="operator">|</span>           <span class="number">2</span> <span class="operator">|</span> <span class="number">9000000.000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------+----------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询成交价格高于该区域平均成交价格的房源信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> t.record_id, h.community, h.district, h.layout, h.area, t.transaction_price</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">FROM</span> transaction_records t</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> house_info h <span class="keyword">ON</span> t.house_id <span class="operator">=</span> h.house_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">WHERE</span> t.transaction_price <span class="operator">&gt;</span> (</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">SELECT</span> <span class="built_in">AVG</span>(tr.transaction_price)</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">FROM</span> transaction_records tr</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">WHERE</span> tr.house_id <span class="operator">=</span> h.house_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> );</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询距离地铁站1000米以内的小区及其房源数量</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> community_name, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> house_count <span class="keyword">from</span> community_info <span class="keyword">where</span> subway_distance <span class="operator">&lt;</span> <span class="number">1000</span> <span class="keyword">group</span> <span class="keyword">by</span> community_name;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>统计每种户型的平均单价（按面积计算）并按降序排列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> layout, <span class="built_in">avg</span>(price <span class="operator">/</span> area) <span class="keyword">as</span> avg_price <span class="keyword">from</span> house_info <span class="keyword">group</span> <span class="keyword">by</span> layout <span class="keyword">order</span> <span class="keyword">by</span> avg_price <span class="keyword">desc</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+------------------+</span></span><br><span class="line"><span class="operator">|</span> layout       <span class="operator">|</span> avg_price        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+------------------+</span></span><br><span class="line"><span class="operator">|</span> 四室两厅     <span class="operator">|</span> <span class="number">80000.0000000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 三室两厅     <span class="operator">|</span> <span class="number">70539.4190870000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 三室一厅     <span class="operator">|</span> <span class="number">68181.8181820000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 两室一厅     <span class="operator">|</span> <span class="number">66666.6666670000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 两室两厅     <span class="operator">|</span> <span class="number">50000.0000000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询最近一个月内成交的房源信息及其所在小区详情</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> h.house_id, h.community, h.district, h.layout, h.area, h.price, h.floor_info, h.orientation  <span class="keyword">from</span> house_info h <span class="keyword">join</span> transaction_records t <span class="keyword">on</span> h.house_id<span class="operator">=</span>t.house_id <span class="keyword">where</span> t.transaction_date <span class="operator">&gt;=</span> date_sub(curdate(), <span class="type">interval</span> <span class="number">1</span> <span class="keyword">month</span>);</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这五个 SQL 查询语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题2</title>
    <link href="https://bigdata-yx.github.io/posts/a299.html"/>
    <id>https://bigdata-yx.github.io/posts/a299.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p>样</p><p>题</p><p>2</p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，教育行业正在经历深刻的变革。在传统教育模式中，教学过程往往依赖教师的经验判断，缺乏对学习者行为的深入理解和精准分析。而在线教育平台的兴起，为教育领域带来了全新的可能。通过收集和分析学习者在平台上的行为数据，如课程选择、学习进度、作业完成情况、互动参与度等，可以更准确地把握学习者的需求和学习特点。平台能够根据用户的学习轨迹、知识掌握程度、学习时长等数据，建立个性化的学习档案，为每位学习者提供更有针对性的课程推荐和学习建议。</p><p>因数据驱动的大数据时代已经到来，在线教育平台需要通过数据分析来提供更优质的教育服务。为完成在线教育平台的大数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p>__（一）任务一：大数据平台搭建 __</p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>HDFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><p>（2）在 master 节点中使用 rpm -ivh 安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）创建新用户 eduadmin 并设置密码，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建数据库 education 并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）授予 eduadmin 用户对学习数据库的查询权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 education 数据库中创建课程表（course）。课程表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>课程编号</td><td>主键</td></tr><tr><td>course_name</td><td>varchar</td><td>课程名称</td><td></td></tr><tr><td>category</td><td>varchar</td><td>课程类别</td><td></td></tr><tr><td>level</td><td>varchar</td><td>难度等级</td><td></td></tr><tr><td>duration</td><td>int</td><td>课程时长(分钟)</td><td></td></tr><tr><td>price</td><td>decimal</td><td>课程价格</td><td></td></tr><tr><td>instructor</td><td>varchar</td><td>讲师姓名</td><td></td></tr><tr><td>avg_rating</td><td>float</td><td>平均评分</td><td></td></tr><tr><td>enrollment_count</td><td>int</td><td>报名人数</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 education 数据库中创建学习记录表（learning_record）。学习记录表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td>user_id</td><td>int</td><td>用户ID</td><td></td></tr><tr><td>course_id</td><td>int</td><td>课程ID</td><td></td></tr><tr><td>watch_duration</td><td>int</td><td>观看时长(分钟)</td><td></td></tr><tr><td>completion_rate</td><td>float</td><td>完成率</td><td></td></tr><tr><td>last_watch_time</td><td>datetime</td><td>最后观看时间</td><td></td></tr><tr><td>quiz_score</td><td>float</td><td>测验得分</td><td></td></tr><tr><td>study_duration</td><td>int</td><td>学习时长(分钟)</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）编写下列 SQL 查询语句：</p><ul><li><p>查询每门课程的平均完成率</p></li><li><p>统计每个课程类别的总报名人数</p></li><li><p>查找观看时长超过课程时长的学习记录</p></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;样&lt;/p&gt;
&lt;p&gt;题&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、背景描述&lt;/strong&gt;&lt;/</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题2解析</title>
    <link href="https://bigdata-yx.github.io/posts/a399.html"/>
    <id>https://bigdata-yx.github.io/posts/a399.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p>样</p><p>题</p><p>2</p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，教育行业正在经历深刻的变革。在传统教育模式中，教学过程往往依赖教师的经验判断，缺乏对学习者行为的深入理解和精准分析。而在线教育平台的兴起，为教育领域带来了全新的可能。通过收集和分析学习者在平台上的行为数据，如课程选择、学习进度、作业完成情况、互动参与度等，可以更准确地把握学习者的需求和学习特点。平台能够根据用户的学习轨迹、知识掌握程度、学习时长等数据，建立个性化的学习档案，为每位学习者提供更有针对性的课程推荐和学习建议。</p><p>因数据驱动的大数据时代已经到来，在线教育平台需要通过数据分析来提供更优质的教育服务。为完成在线教育平台的大数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p>__（一）任务一：大数据平台搭建 __</p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic <span class="keyword">fi</span>....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting ...</span><br><span class="line">Feb 17 22:09:22 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 17 22:09:23 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Mon Feb 17 22:11:09 2025 from master</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Mon Feb 17 22:03:12 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>HDFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave1:`<span class="built_in">pwd</span>`</span><br><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave2:`<span class="built_in">pwd</span>`</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop /opt/module/hadoop/</span><br><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop /opt/module/hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-17 22:31:36,568 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">2025-02-17 22:31:36,568 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 533.1 KB</span><br><span class="line">2025-02-17 22:31:36,568 INFO util.GSet: capacity      = 2^16 = 65536 entries</span><br><span class="line">2025-02-17 22:31:36,585 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1945592902-192.168.1.91-1739849496581</span><br><span class="line">2025-02-17 22:31:36,592 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-17 22:31:36,608 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-17 22:31:36,668 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-17 22:31:36,672 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-17 22:31:36,687 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-17 22:31:36,687 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-17 22:31:36,690 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-17 22:31:36,690 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">11424 Jps</span><br><span class="line">10273 DataNode</span><br><span class="line">11025 NodeManager</span><br><span class="line">11362 JobHistoryServer</span><br><span class="line">10468 SecondaryNameNode</span><br><span class="line">10024 NameNode</span><br><span class="line">11276 WebAppProxyServer</span><br><span class="line">10703 ResourceManager</span><br></pre></td></tr></table></figure><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 hadoop]$ jps</span><br><span class="line">4064 Jps</span><br><span class="line">3838 DataNode</span><br><span class="line">3951 NodeManager</span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br><span class="line">[root@master module]<span class="comment"># systemctl start mysqld</span></span><br></pre></td></tr></table></figure><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind-address=0.0.0.0</span><br></pre></td></tr></table></figure><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables <span class="keyword">from</span> mysql;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_mysql           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> columns_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> db                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> engine_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> event                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> func                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> general_log               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> gtid_executed             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_category             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_keyword              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_relation             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_topic                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_index_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_table_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ndb_binlog_index          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proc                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> procs_priv                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proxies_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> server_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> servers                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_master_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_relay_log_info      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_worker_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slow_log                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tables_priv               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_leap_second     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_name            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition_type <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="number">31</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（4）创建新用户 eduadmin 并设置密码，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;eduadmin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（5）创建数据库 education 并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database education <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）授予 eduadmin 用户对学习数据库的查询权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> education.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;eduadmin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 education 数据库中创建课程表（course）。课程表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>课程编号</td><td>主键</td></tr><tr><td>course_name</td><td>varchar</td><td>课程名称</td><td></td></tr><tr><td>category</td><td>varchar</td><td>课程类别</td><td></td></tr><tr><td>level</td><td>varchar</td><td>难度等级</td><td></td></tr><tr><td>duration</td><td>int</td><td>课程时长(分钟)</td><td></td></tr><tr><td>price</td><td>decimal</td><td>课程价格</td><td></td></tr><tr><td>instructor</td><td>varchar</td><td>讲师姓名</td><td></td></tr><tr><td>avg_rating</td><td>float</td><td>平均评分</td><td></td></tr><tr><td>enrollment_count</td><td>int</td><td>报名人数</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> course( id <span class="type">int</span> <span class="keyword">primary</span> key, course_name <span class="type">varchar</span>(<span class="number">255</span>), category vaarchar(<span class="number">255</span>), level <span class="type">varchar</span>(<span class="number">255</span>), duration <span class="type">int</span>, price <span class="type">decimal</span>, instructor <span class="type">varchar</span>(<span class="number">255</span>), avg_rating <span class="type">float</span>, enrollment_count <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 education 数据库中创建学习记录表（learning_record）。学习记录表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td>user_id</td><td>int</td><td>用户ID</td><td></td></tr><tr><td>course_id</td><td>int</td><td>课程ID</td><td></td></tr><tr><td>watch_duration</td><td>int</td><td>观看时长(分钟)</td><td></td></tr><tr><td>completion_rate</td><td>float</td><td>完成率</td><td></td></tr><tr><td>last_watch_time</td><td>datetime</td><td>最后观看时间</td><td></td></tr><tr><td>quiz_score</td><td>float</td><td>测验得分</td><td></td></tr><tr><td>study_duration</td><td>int</td><td>学习时长(分钟)</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> learning_record( id <span class="type">int</span> <span class="keyword">primary</span> key, user_id <span class="type">int</span>, course_id <span class="type">int</span>, watch_duration <span class="type">int</span>, completion_rate <span class="type">float</span>, last_watch_time datetime, quiz_score floaat, study_duration <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）编写下列 SQL 查询语句：</p><ul><li><p>查询每门课程的平均完成率</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> course_id, <span class="built_in">avg</span>(completion_rate) <span class="keyword">from</span> learning_record <span class="keyword">group</span> <span class="keyword">by</span> course_id;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.03</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>统计每个课程类别的总报名人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> category, <span class="built_in">sum</span>(course_name) <span class="keyword">from</span> course <span class="keyword">group</span> <span class="keyword">by</span> category;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查找观看时长超过课程时长的学习记录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> l.id, l.user_id, l.course_id, l.watch_duration, l.completion_rate, l.ltast_watch_time, l.quiz_score, l.study_duration <span class="keyword">from</span> learning_record l <span class="keyword">join</span> course con l.course_id<span class="operator">=</span>c.id <span class="keyword">where</span> l.watch_duration <span class="operator">&gt;</span> c.duration;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;样&lt;/p&gt;
&lt;p&gt;题&lt;/p&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、背景描述&lt;/strong&gt;&lt;/</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题3</title>
    <link href="https://bigdata-yx.github.io/posts/a300.html"/>
    <id>https://bigdata-yx.github.io/posts/a300.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>3</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们的出行方式发生了显著变化。共享单车作为一种新型的城市短途出行解决方案，不仅满足了人们”最后一公里”的出行需求，也为城市交通带来了新的活力。在传统的运营模式中，由于缺乏数据支持，共享单车的投放和调度主要依靠运营人员的经验判断，导致供需失衡、车辆分布不均等问题。而在大数据时代，通过对骑行数据的分析，可以更精准地预测用户需求，优化车辆调度，提升运营效率。</p><p>共享单车平台可以收集包括用户骑行轨迹、使用时段、车辆状态等多维度数据。通过对这些数据的分析，可以识别热门区域和时段，预测车辆使用需求，优化调度策略，同时也能够对车辆维护进行预警，提升用户体验。这些数据还可以为城市交通规划提供重要参考，助力智慧城市建设。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供高效的共享单车服务。为完成共享单车使用数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令 java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）创建新的用户 bike_admin，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建数据库 bike_data，并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）授予新用户查询数据和插入数据的权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 bike_data 数据库中创建骑行记录表（ride_records）。骑行记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td></td></tr><tr><td><strong>user_id</strong></td><td>varchar</td><td>用户ID</td><td></td></tr><tr><td><strong>start_time</strong></td><td>datetime</td><td>开始时间</td><td></td></tr><tr><td><strong>end_time</strong></td><td>datetime</td><td>结束时间</td><td></td></tr><tr><td><strong>start_location</strong></td><td>varchar</td><td>起始位置</td><td></td></tr><tr><td><strong>end_location</strong></td><td>varchar</td><td>结束位置</td><td></td></tr><tr><td><strong>ride_distance</strong></td><td>double</td><td>骑行距离(km)</td><td></td></tr><tr><td><strong>ride_duration</strong></td><td>int</td><td>骑行时长(分钟)</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 bike_data 数据库中创建单车状态表（bike_status）。单车状态表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td>主键</td></tr><tr><td><strong>status</strong></td><td>varchar</td><td>车辆状态</td><td></td></tr><tr><td><strong>battery_level</strong></td><td>int</td><td>电量百分比</td><td></td></tr><tr><td><strong>last_maintain_time</strong></td><td>datetime</td><td>最后维护时间</td><td></td></tr><tr><td><strong>current_location</strong></td><td>varchar</td><td>当前位置</td><td></td></tr><tr><td><strong>total_mileage</strong></td><td>double</td><td>总里程(km)</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已提供的 SQL 文件将这两份数据导入 bike_data 数据库中，并对其中的数据进行如下操作：</p><ul><li><p>在 ride_records 表中查询骑行时长超过60分钟的记录数量；</p></li><li><p>在 bike_status 表中统计各个状态的单车数量；</p></li></ul><p>将这两个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题3解析</title>
    <link href="https://bigdata-yx.github.io/posts/a400.html"/>
    <id>https://bigdata-yx.github.io/posts/a400.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>3</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们的出行方式发生了显著变化。共享单车作为一种新型的城市短途出行解决方案，不仅满足了人们”最后一公里”的出行需求，也为城市交通带来了新的活力。在传统的运营模式中，由于缺乏数据支持，共享单车的投放和调度主要依靠运营人员的经验判断，导致供需失衡、车辆分布不均等问题。而在大数据时代，通过对骑行数据的分析，可以更精准地预测用户需求，优化车辆调度，提升运营效率。</p><p>共享单车平台可以收集包括用户骑行轨迹、使用时段、车辆状态等多维度数据。通过对这些数据的分析，可以识别热门区域和时段，预测车辆使用需求，优化调度策略，同时也能够对车辆维护进行预警，提升用户体验。这些数据还可以为城市交通规划提供重要参考，助力智慧城市建设。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供高效的共享单车服务。为完成共享单车使用数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令 java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic ....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting...</span><br><span class="line">Feb 18 06:28:12 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 18 06:28:13 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Tue Feb 18 06:23:09 2025 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Tue Feb 18 06:23:12 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind-address=0.0.0.0</span><br></pre></td></tr></table></figure><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables <span class="keyword">from</span> mysql;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_mysql           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> columns_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> db                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> engine_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> event                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> func                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> general_log               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> gtid_executed             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_category             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_keyword              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_relation             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_topic                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_index_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_table_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ndb_binlog_index          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proc                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> procs_priv                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proxies_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> server_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> servers                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_master_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_relay_log_info      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_worker_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slow_log                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tables_priv               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_leap_second     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_name            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition_type <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="number">31</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（4）创建新的用户 bike_admin，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（5）创建数据库 bike_data，并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database bike_data <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）授予新用户查询数据和插入数据的权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span> <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 bike_data 数据库中创建骑行记录表（ride_records）。骑行记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td></td></tr><tr><td><strong>user_id</strong></td><td>varchar</td><td>用户ID</td><td></td></tr><tr><td><strong>start_time</strong></td><td>datetime</td><td>开始时间</td><td></td></tr><tr><td><strong>end_time</strong></td><td>datetime</td><td>结束时间</td><td></td></tr><tr><td><strong>start_location</strong></td><td>varchar</td><td>起始位置</td><td></td></tr><tr><td><strong>end_location</strong></td><td>varchar</td><td>结束位置</td><td></td></tr><tr><td><strong>ride_distance</strong></td><td>double</td><td>骑行距离(km)</td><td></td></tr><tr><td><strong>ride_duration</strong></td><td>int</td><td>骑行时长(分钟)</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> ride_records( id <span class="type">int</span> <span class="keyword">primary</span> key, bike_id <span class="type">varchar</span>(<span class="number">255</span>), user_id <span class="type">varchar</span>(<span class="number">255</span>), start_time datetime, end_time datetime, start_location <span class="type">varchar</span>(<span class="number">255</span>), end_location <span class="type">varchar</span>(<span class="number">255</span>), ride_distance <span class="keyword">double</span>, ride_duration <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 bike_data 数据库中创建单车状态表（bike_status）。单车状态表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td>主键</td></tr><tr><td><strong>status</strong></td><td>varchar</td><td>车辆状态</td><td></td></tr><tr><td><strong>battery_level</strong></td><td>int</td><td>电量百分比</td><td></td></tr><tr><td><strong>last_maintain_time</strong></td><td>datetime</td><td>最后维护时间</td><td></td></tr><tr><td><strong>current_location</strong></td><td>varchar</td><td>当前位置</td><td></td></tr><tr><td><strong>total_mileage</strong></td><td>double</td><td>总里程(km)</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> bike_status(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> bike_id <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> status <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> battery_level <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> last_maintain_time datetime,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> current_location <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> total_mileage <span class="keyword">double</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已提供的 SQL 文件将这两份数据导入 bike_data 数据库中，并对其中的数据进行如下操作：</p><ul><li><p>在 ride_records 表中查询骑行时长超过60分钟的记录数量；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> ride_records.<span class="operator">*</span> <span class="keyword">from</span> ride_records <span class="keyword">where</span> ride_duration<span class="operator">&gt;</span><span class="number">60</span>;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>在 bike_status 表中统计各个状态的单车数量；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> bike_status <span class="keyword">group</span> <span class="keyword">by</span> status;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这两个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题4</title>
    <link href="https://bigdata-yx.github.io/posts/a301.html"/>
    <id>https://bigdata-yx.github.io/posts/a301.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>4</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。旅游行业作为现代服务业的重要组成部分，其数据分析对于提升游客体验、优化资源配置具有重要意义。通过收集和分析游客的行为数据，包括景点偏好、消费习惯、住宿选择等，可以为旅游服务提供者制定更精准的运营策略提供依据。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务。为完成旅游景区客流量预测与分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>DHFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置 MySQL 服务器的最大连接数为 1000，并将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）创建数据库 tourism 并设置默认字符集为 utf8mb4，并将设置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）创建一个名为 tourism_backup 的定时备份脚本，每天凌晨 2 点自动备份数据库 tourism，将脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）优化 MySQL 查询性能，开启慢查询日志，设置超过 3 秒的查询被记录，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）配置 MySQL 的 binlog 日志，设置过期时间为 7 天，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 tourism 数据库中创建景区信息表（scenic_spot）。景区信息表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>主键</td></tr><tr><td>spot_name</td><td>varchar</td><td>景区名称</td><td></td></tr><tr><td>city</td><td>varchar</td><td>所在城市</td><td></td></tr><tr><td>level</td><td>varchar</td><td>景区等级</td><td></td></tr><tr><td>type</td><td>varchar</td><td>景区类型</td><td></td></tr><tr><td>ticket_price</td><td>decimal</td><td>门票价格</td><td></td></tr><tr><td>opening_hours</td><td>varchar</td><td>开放时间</td><td></td></tr><tr><td>max_capacity</td><td>int</td><td>最大承载量</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 tourism 数据库中创建游客流量表（visitor_flow）。游客流量表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>record_id</td><td>int</td><td>记录ID</td><td>主键</td></tr><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>外键</td></tr><tr><td>visit_date</td><td>date</td><td>参观日期</td><td></td></tr><tr><td>visitor_count</td><td>int</td><td>游客数量</td><td></td></tr><tr><td>peak_hour</td><td>time</td><td>高峰时段</td><td></td></tr><tr><td>weather</td><td>varchar</td><td>天气状况</td><td></td></tr><tr><td>is_holiday</td><td>boolean</td><td>是否节假日</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）将提供的数据文件导入数据库 tourism 中并编写以下数据库操作语句：</p><ul><li><p>查询2024年1月每个景区的总游客人数。查询结果需要显示景区名称、所在城市和游客总数，并按游客总数降序排列。</p></li><li><p>统计每个景区在节假日和非节假日的平均游客数量。查询结果需要显示景区名称、景区等级、节假日平均游客数和非节假日平均游客数，按景区编号排序。</p></li><li><p>找出所有景区中单日游客量超过该景区最大承载量的情况。查询结果需要显示景区名称、参观日期、实际游客数量、最大承载量和超出人数，按超出人数降序排列。</p></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题4解析</title>
    <link href="https://bigdata-yx.github.io/posts/a401.html"/>
    <id>https://bigdata-yx.github.io/posts/a401.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>4</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。旅游行业作为现代服务业的重要组成部分，其数据分析对于提升游客体验、优化资源配置具有重要意义。通过收集和分析游客的行为数据，包括景点偏好、消费习惯、住宿选择等，可以为旅游服务提供者制定更精准的运营策略提供依据。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务。为完成旅游景区客流量预测与分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic <span class="keyword">fi</span>....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting ...</span><br><span class="line">Feb 18 21:04:08 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 18 21:04:09 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Tue Feb 18 21:00:07 2025 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Tue Feb 18 21:00:01 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>DHFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave1:`<span class="built_in">pwd</span>`</span><br><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave2:`<span class="built_in">pwd</span>`</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br><span class="line">[hadoop@slave2 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-18 21:22:43,566 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">2025-02-18 21:22:43,566 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 533.1 KB</span><br><span class="line">2025-02-18 21:22:43,566 INFO util.GSet: capacity      = 2^16 = 65536 entries</span><br><span class="line">2025-02-18 21:22:43,585 INFO namenode.FSImage: Allocated new BlockPoolId: BP-464131597-192.168.1.91-1739931763581</span><br><span class="line">2025-02-18 21:22:43,593 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-18 21:22:43,609 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-18 21:22:43,672 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-18 21:22:43,676 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-18 21:22:43,691 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-18 21:22:43,691 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-18 21:22:43,694 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-18 21:22:43,694 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">4528 NameNode</span><br><span class="line">5202 ResourceManager</span><br><span class="line">5748 WebAppProxyServer</span><br><span class="line">5349 NodeManager</span><br><span class="line">4774 DataNode</span><br><span class="line">5896 Jps</span><br><span class="line">4969 SecondaryNameNode</span><br><span class="line">5834 JobHistoryServer</span><br></pre></td></tr></table></figure><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ jps</span><br><span class="line">2935 Jps</span><br><span class="line">2730 DataNode</span><br><span class="line">2842 NodeManager</span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置 MySQL 服务器的最大连接数为 1000，并将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> max_connections <span class="operator">=</span> <span class="number">1000</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（2）创建数据库 tourism 并设置默认字符集为 utf8mb4，并将设置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database tourism <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（3）创建一个名为 tourism_backup 的定时备份脚本，每天凌晨 2 点自动备份数据库 tourism，将脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">db_name=<span class="string">&quot;tourism&quot;</span></span><br><span class="line">backup_dir=<span class="string">&quot;/opt/tourism_backup&quot;</span></span><br><span class="line"><span class="built_in">date</span>=$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M%S&quot;</span>)</span><br><span class="line">backup_file=<span class="string">&quot;<span class="variable">$backup_dir</span>/<span class="variable">$db_name</span>-<span class="variable">$date</span>.sql&quot;</span></span><br><span class="line"></span><br><span class="line">mysqldump -uroot -p<span class="string">&#x27;123456&#x27;</span> <span class="variable">$db_name</span> &gt; <span class="variable">$backup_file</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@master module]<span class="comment"># chmod -x tourism_backup.sh </span></span><br><span class="line">[root@master module]<span class="comment"># crontab -e</span></span><br><span class="line">no crontab <span class="keyword">for</span> root - using an empty one</span><br><span class="line">crontab: installing new crontab</span><br><span class="line">[root@master module]<span class="comment"># crontab -l</span></span><br><span class="line">0 2 * * * /opt/module/tourism_backup.sh</span><br></pre></td></tr></table></figure><p>（4）优化 MySQL 查询性能，开启慢查询日志，设置超过 3 秒的查询被记录，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> slow_query_log<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> long_query_time<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（5）配置 MySQL 的 binlog 日志，设置过期时间为 7 天，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> expire_logs_days <span class="operator">=</span> <span class="number">7</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 tourism 数据库中创建景区信息表（scenic_spot）。景区信息表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>主键</td></tr><tr><td>spot_name</td><td>varchar</td><td>景区名称</td><td></td></tr><tr><td>city</td><td>varchar</td><td>所在城市</td><td></td></tr><tr><td>level</td><td>varchar</td><td>景区等级</td><td></td></tr><tr><td>type</td><td>varchar</td><td>景区类型</td><td></td></tr><tr><td>ticket_price</td><td>decimal</td><td>门票价格</td><td></td></tr><tr><td>opening_hours</td><td>varchar</td><td>开放时间</td><td></td></tr><tr><td>max_capacity</td><td>int</td><td>最大承载量</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> </span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> scenic_spot(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> spot_id <span class="type">int</span> <span class="keyword">primary</span> key,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> spot_name <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> city <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> level <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> type <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ticket_price <span class="type">decimal</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> opening_hours <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> max_capacity <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 tourism 数据库中创建游客流量表（visitor_flow）。游客流量表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>record_id</td><td>int</td><td>记录ID</td><td>主键</td></tr><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>外键</td></tr><tr><td>visit_date</td><td>date</td><td>参观日期</td><td></td></tr><tr><td>visitor_count</td><td>int</td><td>游客数量</td><td></td></tr><tr><td>peak_hour</td><td>time</td><td>高峰时段</td><td></td></tr><tr><td>weather</td><td>varchar</td><td>天气状况</td><td></td></tr><tr><td>is_holiday</td><td>boolean</td><td>是否节假日</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> visitor_flow(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> record_id <span class="type">int</span> <span class="keyword">primary</span> key,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> spot_id <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> visit_date <span class="type">date</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> visitor_count <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> peak_hour <span class="type">time</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> weather <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> is_holiday <span class="type">boolean</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">foreign</span> key (spot_id) <span class="keyword">references</span> scenic_spot(spot_id));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）将提供的数据文件导入数据库 tourism 中并编写以下数据库操作语句：</p><ul><li><p>查询2024年1月每个景区的总游客人数。查询结果需要显示景区名称、所在城市和游客总数，并按游客总数降序排列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> spot_name, city, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> scenic_spot s <span class="keyword">join</span> visitor_flow v <span class="keyword">on</span> s.spot_id<span class="operator">=</span>v.spot_id <span class="keyword">where</span> visit_date<span class="operator">&gt;=</span>&quot;2025:01:01&quot; <span class="keyword">and</span> visit_date<span class="operator">&lt;=</span>&quot;2025:01:31&quot; <span class="keyword">group</span></span><br><span class="line"><span class="keyword">by</span> spot_name, city;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>统计每个景区在节假日和非节假日的平均游客数量。查询结果需要显示景区名称、景区等级、节假日平均游客数和非节假日平均游客数，按景区编号排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_name <span class="keyword">AS</span> 景区名称,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.level <span class="keyword">AS</span> 景区等级,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="built_in">AVG</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> v.is_holiday <span class="operator">=</span> <span class="number">1</span> <span class="keyword">THEN</span> v.visitor_count <span class="keyword">ELSE</span> <span class="keyword">NULL</span> <span class="keyword">END</span>) <span class="keyword">AS</span> 节 假日平均游客数,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="built_in">AVG</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> v.is_holiday <span class="operator">=</span> <span class="number">0</span> <span class="keyword">THEN</span> v.visitor_count <span class="keyword">ELSE</span> <span class="keyword">NULL</span> <span class="keyword">END</span>) <span class="keyword">AS</span> 非 节假日平均游客数</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">FROM</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     scenic_spot s</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     visitor_flow v </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">ON</span> s.spot_id <span class="operator">=</span> v.spot_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_name, </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.level, </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_id;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>找出所有景区中单日游客量超过该景区最大承载量的情况。查询结果需要显示景区名称、参观日期、实际游客数量、最大承载量和超出人数，按超出人数降序排列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> spot_name, visit_date, visitor_count, max_capacity, (max_capacity<span class="operator">-</span>visitor_count) <span class="keyword">as</span> 超出人数 <span class="keyword">from</span> scenic_spot s <span class="keyword">join</span> visitor_flow v <span class="keyword">on</span> s.spot_id<span class="operator">=</span>v.spot_id <span class="keyword">where</span> v.visitor_count <span class="operator">&gt;</span> s.max_capacity <span class="keyword">order</span> <span class="keyword">by</span> 超出人数 <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题5</title>
    <link href="https://bigdata-yx.github.io/posts/a302.html"/>
    <id>https://bigdata-yx.github.io/posts/a302.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>5</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。平台可以根据用户的浏览，点击，评论等行为信息数据进行收集和整理。通过大量用户的行为可以对某一个产品进行比较准确客观的评分和评价，或者进行相应的用户画像，将产品推荐给喜欢该产品的用户进行相应的消费。 </p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务，为完成互联网酒店的大数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2.子任务二：MySQL 运维"></a>2.子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）修改 MySQL 配置文件启用远程连接，将修改后的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）启用 MySQL 慢查询日志功能，设置慢查询阈值为 2 秒，将完整的配置命令和验证命令结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）创建名为 hadoop 的数据库用户，密码设置为 Hadoop@2025，将创建用户的完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）为 hadoop 用户授予 bigdata 数据库的 SELECT、INSERT 和 UPDATE 权限，将授权命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建名为 monitor 的监控专用用户，仅授予 PROCESS 和 SHOW DATABASES 权限，将完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）使用 mysqldump 工具备份 bigdata 数据库，将备份文件保存为 bigdata_backup.sql，将备份命令及执行过程复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置数据库字符集为 utf8mb4，并将默认排序规则设置为 utf8mb4_unicode_ci，将配置命令和验证结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）创建一个定时备份脚本，实现每天凌晨 2 点自动备份 bigdata 数据库到 &#x2F;opt&#x2F;backup 目录，并只保留最近 7 天的备份文件，将完整脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3.子任务三：数据表的创建及维护"></a>3.子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 bigdata 数据库中创建酒店表 （hotel）。酒店表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>酒店编号</td><td></td></tr><tr><td><strong>hotel_name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>city</strong></td><td>varchar</td><td>城市</td><td></td></tr><tr><td><strong>province</strong></td><td>varchar</td><td>省份</td><td></td></tr><tr><td><strong>level</strong></td><td>varchar</td><td>星级</td><td></td></tr><tr><td><strong>room_num</strong></td><td>int</td><td>房间数</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>commnet_num</strong></td><td>varchar</td><td>评论数</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 bigdata 数据库中创建评论表 （comment）。评论表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>评论编号</td><td></td></tr><tr><td><strong>name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>commentator</strong></td><td>varchar</td><td>评论人</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>comment_time</strong></td><td>datetime</td><td>评论时间</td><td></td></tr><tr><td><strong>content</strong></td><td>varchar</td><td>评论内容</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已给到的 sql 文件将这两份数据导入 bigdata 数据库中，并对其中的数据进行如下操作：</p><ul><li><p>统计各省份的平均酒店评分，并将评分低于该省份平均分的酒店评分上调0.5分（评分上限为5分）；</p></li><li><p>查找出每个城市评论数最多的酒店及其详细信息；</p></li><li><p>找出所有发表过3条及以上差评（评分小于等于2分）的评论人，并列出这些评论人的评论时间、评论内容及对应的酒店名称，按评论时间降序排序。</p></li></ul><p>将这3个 SQL 语句分别复制粘贴至 【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>省赛样题5解析</title>
    <link href="https://bigdata-yx.github.io/posts/a402.html"/>
    <id>https://bigdata-yx.github.io/posts/a402.html</id>
    <published>2025-02-17T06:59:37.000Z</published>
    <updated>2025-02-17T06:59:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>5</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。平台可以根据用户的浏览，点击，评论等行为信息数据进行收集和整理。通过大量用户的行为可以对某一个产品进行比较准确客观的评分和评价，或者进行相应的用户画像，将产品推荐给喜欢该产品的用户进行相应的消费。 </p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务，为完成互联网酒店的大数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl disable firewalld</span></span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic <span class="keyword">fi</span>....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting ...</span><br><span class="line">Feb 18 02:09:52 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 18 02:09:53 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last failed login: Tue Feb 18 02:11:10 EST 2025 from slave2 on ssh:notty</span><br><span class="line">There was 1 failed login attempt since the last successful login.</span><br><span class="line">Last login: Tue Feb 18 02:02:49 2025 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Tue Feb 18 02:02:52 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave1:`<span class="built_in">pwd</span>`</span><br><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave2:`<span class="built_in">pwd</span>`</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br><span class="line">[hadoop@slave2 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-18 02:31:58,444 INFO util.GSet: capacity      = 2^16 = 65536 entries</span><br><span class="line">2025-02-18 02:31:58,461 INFO namenode.FSImage: Allocated new BlockPoolId: BP-610088070-192.168.1.91-1739863918457</span><br><span class="line">2025-02-18 02:31:58,468 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-18 02:31:58,484 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-18 02:31:58,544 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-18 02:31:58,549 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-18 02:31:58,564 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-18 02:31:58,564 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-18 02:31:58,566 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-18 02:31:58,566 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">4372 NameNode</span><br><span class="line">5045 ResourceManager</span><br><span class="line">5253 NodeManager</span><br><span class="line">5591 WebAppProxyServer</span><br><span class="line">4618 DataNode</span><br><span class="line">4812 SecondaryNameNode</span><br><span class="line">5724 Jps</span><br><span class="line">5662 JobHistoryServer</span><br></pre></td></tr></table></figure><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ jps</span><br><span class="line">2838 Jps</span><br><span class="line">2634 DataNode</span><br><span class="line">2746 NodeManager</span><br></pre></td></tr></table></figure><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2.子任务二：MySQL 运维"></a>2.子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）修改 MySQL 配置文件启用远程连接，将修改后的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind-address=0.0.0.0</span><br></pre></td></tr></table></figure><p>（2）启用 MySQL 慢查询日志功能，设置慢查询阈值为 2 秒，将完整的配置命令和验证命令结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global slow_query_log = 1;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="built_in">set</span> global long_query_time = 2;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure><p>（3）创建名为 hadoop 的数据库用户，密码设置为 Hadoop@2025，将创建用户的完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;Hadoop@2025&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（4）为 hadoop 用户授予 bigdata 数据库的 SELECT、INSERT 和 UPDATE 权限，将授权命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span>, <span class="keyword">update</span> <span class="keyword">on</span> bigdata.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（5）创建名为 monitor 的监控专用用户，仅授予 PROCESS 和 SHOW DATABASES 权限，将完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> process, <span class="keyword">show</span> databases <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）使用 mysqldump 工具备份 bigdata 数据库，将备份文件保存为 bigdata_backup.sql，将备份命令及执行过程复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p bigdata <span class="operator">&gt;</span> bigdata_backup.sql</span><br><span class="line">Enter password: </span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# ls</span><br><span class="line">bigdata_backup.sql</span><br></pre></td></tr></table></figure><p>（7）配置数据库字符集为 utf8mb4，并将默认排序规则设置为 utf8mb4_unicode_ci，将配置命令和验证结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> character_set_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> collation_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4_unicode_ci&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（8）创建一个定时备份脚本，实现每天凌晨 2 点自动备份 bigdata 数据库到 &#x2F;opt&#x2F;backup 目录，并只保留最近 7 天的备份文件，将完整脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">db_name=<span class="string">&quot;bigdata&quot;</span></span><br><span class="line">backup_dir=<span class="string">&quot;/opt/backup&quot;</span></span><br><span class="line"><span class="built_in">date</span>=$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M%S&quot;</span>)</span><br><span class="line">backup_file=<span class="string">&quot;<span class="variable">$backup_dir</span>/<span class="variable">$db_name</span>-<span class="variable">$date</span>.sql&quot;</span></span><br><span class="line"></span><br><span class="line">mysqldump -uroot -p<span class="string">&#x27;123456&#x27;</span> <span class="variable">$db_name</span> &gt; <span class="variable">$backup_file</span></span><br><span class="line"></span><br><span class="line">find <span class="string">&quot;<span class="variable">$backup_dir</span>&quot;</span> -<span class="built_in">type</span> f -name <span class="string">&quot;<span class="variable">$&#123;db_name&#125;</span>-*.sql&quot;</span> -mtime +7 -<span class="built_in">exec</span> <span class="built_in">rm</span> &#123;&#125; \;</span><br></pre></td></tr></table></figure><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3.子任务三：数据表的创建及维护"></a>3.子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 bigdata 数据库中创建酒店表 （hotel）。酒店表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>酒店编号</td><td></td></tr><tr><td><strong>hotel_name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>city</strong></td><td>varchar</td><td>城市</td><td></td></tr><tr><td><strong>province</strong></td><td>varchar</td><td>省份</td><td></td></tr><tr><td><strong>level</strong></td><td>varchar</td><td>星级</td><td></td></tr><tr><td><strong>room_num</strong></td><td>int</td><td>房间数</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>commnet_num</strong></td><td>varchar</td><td>评论数</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> hotel(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> id <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> hotel_name <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> city <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> province <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> level <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> room_num <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> score <span class="keyword">double</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> commnet_num <span class="type">varchar</span>(<span class="number">255</span>));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 bigdata 数据库中创建评论表 （comment）。评论表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>评论编号</td><td></td></tr><tr><td><strong>name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>commentator</strong></td><td>varchar</td><td>评论人</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>comment_time</strong></td><td>datetime</td><td>评论时间</td><td></td></tr><tr><td><strong>content</strong></td><td>varchar</td><td>评论内容</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> comment( id <span class="type">int</span>, name <span class="type">varchar</span>(<span class="number">255</span>), commentator <span class="type">varchar</span>(<span class="number">255</span>), score <span class="keyword">double</span>, comment_time datetime, content <span class="type">varchar</span>(<span class="number">255</span>));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已给到的 sql 文件将这两份数据导入 bigdata 数据库中，并对其中的数据进行如下操作：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> source <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>hotel_all_data.sql</span><br><span class="line">mysql<span class="operator">&gt;</span> source <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>comment_all_data.sql</span><br></pre></td></tr></table></figure><ul><li><p>统计各省份的平均酒店评分，并将评分低于该省份平均分的酒店评分上调0.5分（评分上限为5分）；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> hotels h</span><br><span class="line"><span class="keyword">JOIN</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> province, <span class="built_in">AVG</span>(score) <span class="keyword">AS</span> avg_score</span><br><span class="line">    <span class="keyword">FROM</span> hotels</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> province</span><br><span class="line">) <span class="keyword">AS</span> prov_avg <span class="keyword">ON</span> h.province <span class="operator">=</span> prov_avg.province</span><br><span class="line"><span class="keyword">SET</span> h.score <span class="operator">=</span> LEAST(h.score <span class="operator">+</span> <span class="number">0.5</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">WHERE</span> h.score <span class="operator">&lt;</span> prov_avg.avg_score;</span><br></pre></td></tr></table></figure></li><li><p>查找出每个城市评论数最多的酒店及其详细信息；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> h.<span class="operator">*</span> <span class="keyword">from</span> ( <span class="keyword">select</span> city, <span class="built_in">max</span>(commnet_num) <span class="keyword">as</span> max_commnet_num <span class="keyword">from</span> hotel <span class="keyword">group</span> <span class="keyword">by</span> city) <span class="keyword">as</span> city_max  <span class="keyword">join</span> hotel h <span class="keyword">on</span> h.city <span class="operator">=</span> city_max.city <span class="keyword">and</span> h.commnet_num <span class="operator">=</span> city_max.max_commnet_num <span class="keyword">order</span> <span class="keyword">by</span> h.city;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>找出所有发表过3条及以上差评（评分小于等于2分）的评论人，并列出这些评论人的评论时间、评论内容及对应的酒店名称，按评论时间降序排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> c.commentator, c.comment_time, c.content, c.name <span class="keyword">from</span> comment c <span class="keyword">join</span> (seleect commeor <span class="keyword">having</span> count_min <span class="operator">&gt;=</span><span class="number">3</span>) <span class="keyword">as</span> select_count_min <span class="keyword">on</span> c.commentator<span class="operator">=</span>select_count_min.commentattor <span class="keyword">where</span> c.score <span class="operator">&lt;=</span> <span class="number">2</span> <span class="keyword">order</span> <span class="keyword">by</span> comment_time <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这3个 SQL 语句分别复制粘贴至 【提交结果.docx】中对应的任务序号下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;2024-2025学年广东省职业院校技能大赛&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中职组大数据应用与服务赛项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/categories/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="省赛样题" scheme="https://bigdata-yx.github.io/tags/%E7%9C%81%E8%B5%9B%E6%A0%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop Ha</title>
    <link href="https://bigdata-yx.github.io/posts/yx1.html"/>
    <id>https://bigdata-yx.github.io/posts/yx1.html</id>
    <published>2025-01-15T08:47:36.000Z</published>
    <updated>2025-01-15T08:47:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="配置Hadoop-Ha"><a href="#配置Hadoop-Ha" class="headerlink" title="配置Hadoop Ha"></a>配置Hadoop Ha</h1><blockquote><p>[!NOTE]</p><p>在hadoop集群的基础上</p></blockquote><h3 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hadoop-env.sh</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/root/software/jdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/root/software/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_LOG_DIR=<span class="variable">$HADOOP_HOME</span>/logs</span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_uSER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_PROXYUSER_USER=root</span><br><span class="line"><span class="comment"># 加上了以下两个</span></span><br><span class="line"><span class="built_in">export</span> HDFS_JOURNALNODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_ZKFC_USER=root</span><br></pre></td></tr></table></figure><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  # 这里就不能指定 主机名:端口了</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>131072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> # 加上了zookeeper的配置</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/nn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/dn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir.perm<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>700<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>268435456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"># 以上是原来的配置，但是不用hosts了，因为默认是全部</span><br><span class="line"># 以下则是高可用的配置</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://master:8485;slave1:8485;slave2:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"># 这个得添加，配置hdfs-site.xml隔离机制方法(栅栏方法）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">          sshfence</span><br><span class="line">          shell(/bin/true)</span><br><span class="line">      <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="mapred-site-xml-保持原有配置即可"><a href="#mapred-site-xml-保持原有配置即可" class="headerlink" title="mapred-site.xml(保持原有配置即可)"></a>mapred-site.xml(保持原有配置即可)</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19899<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$HADOOP_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># 不能有resourcemanager指定hostname了因为是高可用</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/software/hadoop/etc/hadoop:/root/software/hadoop/share/hadoop/common/lib/*:/root/software/hadoop/share/hadoop/common/*:/root/software/hadoop/share/hadoop/hdfs:/root/software/hadoop/share/hadoop/hdfs/lib/*:/root/software/hadoop/share/hadoop/hdfs/*:/root/software/hadoop/share/hadoop/mapreduce/*:/root/software/hadoop/share/hadoop/yarn:/root/software/hadoop/share/hadoop/yarn/lib/*:/root/software/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.zk.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-首先启动zookeeper且保证健康状态"><a href="#1-首先启动zookeeper且保证健康状态" class="headerlink" title="1.首先启动zookeeper且保证健康状态"></a>1.首先启动zookeeper且保证健康状态</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh status</span><br></pre></td></tr></table></figure><h3 id="2-启动JournalNode（三台都要）"><a href="#2-启动JournalNode（三台都要）" class="headerlink" title="2. 启动JournalNode（三台都要）"></a>2. 启动JournalNode（三台都要）</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs --daemon start journalnode</span><br></pre></td></tr></table></figure><h3 id="3-格式化HDFS"><a href="#3-格式化HDFS" class="headerlink" title="3.格式化HDFS"></a>3.格式化HDFS</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><h3 id="4-FSImage文件同步"><a href="#4-FSImage文件同步" class="headerlink" title="4.FSImage文件同步"></a>4.FSImage文件同步</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]<span class="comment"># scp -r /data/nn/ slave1:/data</span></span><br></pre></td></tr></table></figure><h4 id="5-格式化ZKFC"><a href="#5-格式化ZKFC" class="headerlink" title="5.格式化ZKFC"></a>5.格式化ZKFC</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure><h3 id="6-启动"><a href="#6-启动" class="headerlink" title="6.启动"></a>6.启动</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><h3 id="可能看不到datanode之类的那就手动启动"><a href="#可能看不到datanode之类的那就手动启动" class="headerlink" title="可能看不到datanode之类的那就手动启动"></a>可能看不到datanode之类的那就手动启动</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@slave1 ~]<span class="comment"># hadoop-daemon.sh start datanode</span></span><br></pre></td></tr></table></figure><h3 id="然后就可以去查看两个节点的hdfs-webui了"><a href="#然后就可以去查看两个节点的hdfs-webui了" class="headerlink" title="然后就可以去查看两个节点的hdfs webui了"></a>然后就可以去查看两个节点的hdfs webui了</h3><p><img src="https://pic1.imgdb.cn/item/67886febd0e0a243d4f4ae34.png"></p><p><img src="https://pic1.imgdb.cn/item/67886ff3d0e0a243d4f4ae37.png"></p><h3 id="会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs-ha成功了"><a href="#会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs-ha成功了" class="headerlink" title="会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs ha成功了"></a>会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs ha成功了</h3><h4 id="查看NameNode的状态"><a href="#查看NameNode的状态" class="headerlink" title="查看NameNode的状态"></a>查看NameNode的状态</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs haadmin -getServiceState nn1</span><br><span class="line">hdfs haadmin -getServiceState nn2</span><br></pre></td></tr></table></figure><h4 id="查看ResourceManager的状态"><a href="#查看ResourceManager的状态" class="headerlink" title="查看ResourceManager的状态"></a>查看ResourceManager的状态</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn rmadmin -getServiceState rm1</span><br><span class="line">yarn rmadmin -getServiceState rm2</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;配置Hadoop-Ha&quot;&gt;&lt;a href=&quot;#配置Hadoop-Ha&quot; class=&quot;headerlink&quot; title=&quot;配置Hadoop Ha&quot;&gt;&lt;/a&gt;配置Hadoop Ha&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;[!NOTE]&lt;/p&gt;
&lt;p&gt;在hadoo</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://bigdata-yx.github.io/categories/Hadoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Hadoop" scheme="https://bigdata-yx.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Redis</title>
    <link href="https://bigdata-yx.github.io/posts/nb11.html"/>
    <id>https://bigdata-yx.github.io/posts/nb11.html</id>
    <published>2025-01-15T06:52:32.000Z</published>
    <updated>2025-01-15T06:52:32.000Z</updated>
    
    
    
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/categories/Redis/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis常见客户端</title>
    <link href="https://bigdata-yx.github.io/posts/nb14.html"/>
    <id>https://bigdata-yx.github.io/posts/nb14.html</id>
    <published>2025-01-15T06:52:32.000Z</published>
    <updated>2025-01-15T06:52:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Redis客户端"><a href="#Redis客户端" class="headerlink" title="Redis客户端"></a>Redis客户端</h2><ul><li>命令行客户端</li><li>图形化桌面客户端</li><li>编程客户端</li></ul><h3 id="1-1Redis命令行客户端"><a href="#1-1Redis命令行客户端" class="headerlink" title="1.1Redis命令行客户端"></a>1.1Redis命令行客户端</h3><p>Redis安装完成后就自带了命令行客户端：redis-cli，使用方式如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli [options] [commonds] <span class="comment"># 一般就是redis-cli -h master -a 123456即可</span></span><br></pre></td></tr></table></figure><p>其中常见的options有：</p><ul><li><code>-h 127.0.0.1</code>：指定要连接的redis节点的IP地址，默认是127.0.0.1</li><li><code>-p 6379</code>：指定要连接的redis节点的端口，默认是6379</li><li><code>-a 123321</code>：指定redis的访问密码</li></ul><p>其中的commonds就是Redis的操作命令，例如：</p><ul><li><code>ping</code>：与redis服务端做心跳测试，服务端正常会返回<code>pong</code></li></ul><h3 id="1-2图形化桌面客户端"><a href="#1-2图形化桌面客户端" class="headerlink" title="1.2图形化桌面客户端"></a>1.2图形化桌面客户端</h3><p><a href="https://github.com/lework/RedisDesktopManager-Windows/releases">https://github.com/lework/RedisDesktopManager-Windows/releases</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Redis客户端&quot;&gt;&lt;a href=&quot;#Redis客户端&quot; class=&quot;headerlink&quot; title=&quot;Redis客户端&quot;&gt;&lt;/a&gt;Redis客户端&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;命令行客户端&lt;/li&gt;
&lt;li&gt;图形化桌面客户端&lt;/li&gt;
&lt;li&gt;编程客户端&lt;</summary>
      
    
    
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/categories/Redis/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis常见命令</title>
    <link href="https://bigdata-yx.github.io/posts/nb13.html"/>
    <id>https://bigdata-yx.github.io/posts/nb13.html</id>
    <published>2025-01-15T06:52:32.000Z</published>
    <updated>2025-01-15T06:52:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Redis的常见命令"><a href="#Redis的常见命令" class="headerlink" title="Redis的常见命令"></a>Redis的常见命令</h2><h3 id="1-1Redis数据结构介绍，数据类型"><a href="#1-1Redis数据结构介绍，数据类型" class="headerlink" title="1.1Redis数据结构介绍，数据类型"></a>1.1Redis数据结构介绍，数据类型</h3><p><img src="https://pic1.imgdb.cn/item/67773d23d0e0a243d4ee11a6.png"></p><h3 id="1-2Redis通用命令"><a href="#1-2Redis通用命令" class="headerlink" title="1.2Redis通用命令"></a>1.2Redis通用命令</h3><ul><li>keys pattern(查看符合模版的所有key)例如：keys *keys ?keys *n?*是多个字符 ?是单个字符</li><li>del key [key …]    (删除一个或多个指定的key)    例如：del k1 k2 k3</li><li>exists key [key …] (判断一个k是否存在)              例如：exists name</li><li>expire key seconds (给一个key设置有效期)        例如: expire name 1</li><li>ttl key                       (查看一个key的生育有效期) 例如：ttl name</li></ul><h3 id="1-3String类型"><a href="#1-3String类型" class="headerlink" title="1.3String类型"></a>1.3String类型</h3><p><img src="https://pic1.imgdb.cn/item/677743f4d0e0a243d4ee2616.png"></p><h3 id="1-4key层级结构"><a href="#1-4key层级结构" class="headerlink" title="1.4key层级结构"></a>1.4key层级结构</h3><p><img src="https://pic1.imgdb.cn/item/67774629d0e0a243d4ee2ac2.png"></p><h3 id="1-5Hash类型"><a href="#1-5Hash类型" class="headerlink" title="1.5Hash类型"></a>1.5Hash类型</h3><p><img src="https://pic1.imgdb.cn/item/677746c1d0e0a243d4ee2c52.png"></p><p><img src="https://pic1.imgdb.cn/item/677746d2d0e0a243d4ee2c83.png"></p><h3 id="1-6List类型"><a href="#1-6List类型" class="headerlink" title="1.6List类型"></a>1.6List类型</h3><p><img src="https://pic1.imgdb.cn/item/67774f9ad0e0a243d4ee350d.png"></p><p><img src="https://pic1.imgdb.cn/item/67774fadd0e0a243d4ee3510.png"></p><h3 id="1-7Set类型"><a href="#1-7Set类型" class="headerlink" title="1.7Set类型"></a>1.7Set类型</h3><p><img src="https://pic1.imgdb.cn/item/67775270d0e0a243d4ee3572.png"></p><p><img src="https://pic1.imgdb.cn/item/6777527dd0e0a243d4ee3573.png"></p><h3 id="1-8SortedSet类型"><a href="#1-8SortedSet类型" class="headerlink" title="1.8SortedSet类型"></a>1.8SortedSet类型</h3><p><img src="https://pic1.imgdb.cn/item/67775933d0e0a243d4ee4227.png"></p><p><img src="https://pic1.imgdb.cn/item/6777594ad0e0a243d4ee422b.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Redis的常见命令&quot;&gt;&lt;a href=&quot;#Redis的常见命令&quot; class=&quot;headerlink&quot; title=&quot;Redis的常见命令&quot;&gt;&lt;/a&gt;Redis的常见命令&lt;/h2&gt;&lt;h3 id=&quot;1-1Redis数据结构介绍，数据类型&quot;&gt;&lt;a href=&quot;#1-</summary>
      
    
    
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/categories/Redis/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis搭建</title>
    <link href="https://bigdata-yx.github.io/posts/nb12.html"/>
    <id>https://bigdata-yx.github.io/posts/nb12.html</id>
    <published>2025-01-15T06:52:32.000Z</published>
    <updated>2025-01-15T06:52:32.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Redis搭建"><a href="#Redis搭建" class="headerlink" title="Redis搭建"></a>Redis搭建</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压 tar -xvzf redis-6.2.6.tar.gz</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入解压后的目录 make &amp;&amp; make install</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">默认的安装路径是在 `/usr/local/bin`目录下</span></span><br></pre></td></tr></table></figure><h3 id="1-1默认启动"><a href="#1-1默认启动" class="headerlink" title="1.1默认启动"></a>1.1默认启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br></pre></td></tr></table></figure><h3 id="1-2指定配置启动"><a href="#1-2指定配置启动" class="headerlink" title="1.2指定配置启动"></a>1.2指定配置启动</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conf文件在解压后的目录下，然后进行配置</span></span><br><span class="line"><span class="comment"># 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0</span></span><br><span class="line"><span class="attr">bind</span> <span class="string">0.0.0.0</span></span><br><span class="line"><span class="comment"># 守护进程，修改为yes后即可后台运行</span></span><br><span class="line"><span class="attr">daemonize</span> <span class="string">yes </span></span><br><span class="line"><span class="comment"># 密码，设置后访问Redis必须输入密码</span></span><br><span class="line"><span class="attr">requirepass</span> <span class="string">123321</span></span><br></pre></td></tr></table></figure><h4 id="Redis的其它常见配置："><a href="#Redis的其它常见配置：" class="headerlink" title="Redis的其它常见配置："></a>Redis的其它常见配置：</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 监听的端口</span></span><br><span class="line"><span class="attr">port</span> <span class="string">6379</span></span><br><span class="line"><span class="comment"># 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录</span></span><br><span class="line"><span class="attr">dir</span> <span class="string">.</span></span><br><span class="line"><span class="comment"># 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15</span></span><br><span class="line"><span class="attr">databases</span> <span class="string">1</span></span><br><span class="line"><span class="comment"># 设置redis能够使用的最大内存</span></span><br><span class="line"><span class="attr">maxmemory</span> <span class="string">512mb</span></span><br><span class="line"><span class="comment"># 日志文件，默认为空，不记录日志，可以指定日志文件名</span></span><br><span class="line"><span class="attr">logfile</span> <span class="string">&quot;redis.log&quot;</span></span><br></pre></td></tr></table></figure><h3 id="启动Redis"><a href="#启动Redis" class="headerlink" title="启动Redis:"></a>启动Redis:</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入redis安装目录 </span></span><br><span class="line"><span class="built_in">cd</span> /usr/local/src/redis-6.2.6</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">redis-server redis.conf</span><br></pre></td></tr></table></figure><h3 id="停止服务："><a href="#停止服务：" class="headerlink" title="停止服务："></a>停止服务：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务，</span></span><br><span class="line"><span class="comment"># 因为之前配置了密码，因此需要通过 -u 来指定密码</span></span><br><span class="line">redis-cli -u 123321 shutdown</span><br></pre></td></tr></table></figure><h3 id="1-3开机自启："><a href="#1-3开机自启：" class="headerlink" title="1.3开机自启："></a>1.3开机自启：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先创建一个系统服务文件</span></span><br><span class="line">vi /etc/systemd/system/redis.service</span><br></pre></td></tr></table></figure><h3 id="内容如下："><a href="#内容如下：" class="headerlink" title="内容如下："></a>内容如下：</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">[Unit]</span></span><br><span class="line"><span class="attr">Description</span>=<span class="string">redis-server</span></span><br><span class="line"><span class="attr">After</span>=<span class="string">network.target</span></span><br><span class="line"></span><br><span class="line"><span class="attr">[Service]</span></span><br><span class="line"><span class="attr">Type</span>=<span class="string">forking</span></span><br><span class="line"><span class="attr">ExecStart</span>=<span class="string">/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf</span></span><br><span class="line"><span class="attr">PrivateTmp</span>=<span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">[Install]</span></span><br><span class="line"><span class="attr">WantedBy</span>=<span class="string">multi-user.target</span></span><br></pre></td></tr></table></figure><h3 id="然后重载系统服务："><a href="#然后重载系统服务：" class="headerlink" title="然后重载系统服务："></a>然后重载系统服务：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure><h3 id="现在，我们可以用下面这组命令来操作redis了："><a href="#现在，我们可以用下面这组命令来操作redis了：" class="headerlink" title="现在，我们可以用下面这组命令来操作redis了："></a>现在，我们可以用下面这组命令来操作redis了：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动</span></span><br><span class="line">systemctl start redis</span><br><span class="line"><span class="comment"># 停止</span></span><br><span class="line">systemctl stop redis</span><br><span class="line"><span class="comment"># 重启</span></span><br><span class="line">systemctl restart redis</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">systemctl status redis</span><br></pre></td></tr></table></figure><h3 id="执行下面的命令，可以让redis开机自启："><a href="#执行下面的命令，可以让redis开机自启：" class="headerlink" title="执行下面的命令，可以让redis开机自启："></a>执行下面的命令，可以让redis开机自启：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> redis</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Redis搭建&quot;&gt;&lt;a href=&quot;#Redis搭建&quot; class=&quot;headerlink&quot; title=&quot;Redis搭建&quot;&gt;&lt;/a&gt;Redis搭建&lt;/h2&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=</summary>
      
    
    
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/categories/Redis/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Redis" scheme="https://bigdata-yx.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Flink</title>
    <link href="https://bigdata-yx.github.io/posts/pd11.html"/>
    <id>https://bigdata-yx.github.io/posts/pd11.html</id>
    <published>2025-01-14T07:38:45.000Z</published>
    <updated>2025-01-14T07:38:45.000Z</updated>
    
    
    
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/categories/Flink/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink快速上手</title>
    <link href="https://bigdata-yx.github.io/posts/pd13.html"/>
    <id>https://bigdata-yx.github.io/posts/pd13.html</id>
    <published>2025-01-14T07:38:45.000Z</published>
    <updated>2025-01-14T07:38:45.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink快速上手"><a href="#Flink快速上手" class="headerlink" title="Flink快速上手"></a>Flink快速上手</h1><blockquote><p>[!NOTE]</p><p>前提准备好相关maven环境和依赖</p></blockquote><h2 id="1-WordCount代码编写"><a href="#1-WordCount代码编写" class="headerlink" title="1.WordCount代码编写"></a>1.WordCount代码编写</h2><h5 id="需求：统计一段文字中，每个单词出现的频次。"><a href="#需求：统计一段文字中，每个单词出现的频次。" class="headerlink" title="需求：统计一段文字中，每个单词出现的频次。"></a>需求：统计一段文字中，每个单词出现的频次。</h5><h5 id="环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。"><a href="#环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。" class="headerlink" title="环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。"></a>环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。</h5><h3 id="1-1-批处理"><a href="#1-1-批处理" class="headerlink" title="1.1  批处理"></a>1.1  批处理</h3><h5 id="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"><a href="#批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。" class="headerlink" title="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"></a>批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。</h5><h4 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1)数据准备"></a>1)数据准备</h4><ul><li>（1）在工程根目录下新建一个input文件夹，并在下面创建文本文件words.txt</li><li>（2）在words.txt中输入一些文字，例如：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello flink</span><br><span class="line">hello world</span><br><span class="line">hello java</span><br></pre></td></tr></table></figure><h4 id="2-代码编写"><a href="#2-代码编写" class="headerlink" title="2)代码编写"></a>2)代码编写</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建一个执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineData = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据集进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineData.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneGroup = wordAndOne.groupBy(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合统计</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）输出"><a href="#（2）输出" class="headerlink" title="（2）输出"></a>（2）输出</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(flink,1)</span><br><span class="line">(world,1)</span><br><span class="line">(hello,3)</span><br><span class="line">(java,1)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>需要注意的是，这种代码的实现方式，是基于DataSet API的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上Flink本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的API来实现。所以从Flink 1.12开始，官方推荐的做法是直接使用DataStream API，在提交任务时通过将执行模式设为BATCH来进行批处理：</p><p>$ bin&#x2F;flink run -Dexecution.runtime-mode&#x3D;BATCH BatchWordCount.jar</p><p>这样，DataSet API就没什么用了，在实际应用中我们只要维护一套DataStream API就可以。这里只是为了方便大家理解，我们依然用DataSet API做了批处理的实现。</p></blockquote><h3 id="1-2流处理"><a href="#1-2流处理" class="headerlink" title="1.2流处理"></a>1.2流处理</h3><blockquote><p>[!CAUTION]</p><p>对于Flink而言，流才是整个处理逻辑的底层核心，所以流批统一之后的DataStream API更加强大，可以直接处理批处理和流处理的所有场景。</p></blockquote><h5 id="我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"><a href="#我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致" class="headerlink" title="我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"></a>我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BoundedStreamWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineDataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndGroup = wordAndOne.keyBy(_._1)</span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合操作</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行任务</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3&gt; (java,1)</span><br><span class="line">5&gt; (hello,1)</span><br><span class="line">5&gt; (hello,2)</span><br><span class="line">5&gt; (hello,3)</span><br><span class="line">13&gt; (flink,1)</span><br><span class="line">9&gt; (world,1)</span><br></pre></td></tr></table></figure><h3 id="主要观察与批处理程序BatchWordCount的不同："><a href="#主要观察与批处理程序BatchWordCount的不同：" class="headerlink" title="主要观察与批处理程序BatchWordCount的不同："></a>主要观察与批处理程序BatchWordCount的不同：</h3><ul><li>创建执行环境的不同，流处理程序使用的是StreamExecutionEnvironment</li><li>转换处理之后，得到的数据对象类型不同</li><li>分组操作调用的是keyBy方法，可以传入一个匿名函数作为键选择器（KeySelector），指定当前分组的key是什么</li><li>代码末尾需要调用env的execute方法，开始执行任务</li></ul><h4 id="2）读取socket文本流"><a href="#2）读取socket文本流" class="headerlink" title="2）读取socket文本流"></a>2）读取socket文本流</h4><blockquote><p>[!NOTE]</p><p>在实际的生产环境中，真正的数据流其实是无界的，有开始却没有结束，这就要求我们需要持续地处理捕获的数据。为了模拟这种场景，可以监听socket端口，然后向该端口不断的发送数据。</p></blockquote><h4 id="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："><a href="#（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：" class="headerlink" title="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："></a>（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.<span class="type">ParameterTool</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCOunt</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> parameterTool = <span class="type">ParameterTool</span>.fromArgs(args)</span><br><span class="line">    <span class="keyword">val</span> hostname = parameterTool.get(<span class="string">&quot;host&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> port = parameterTool.getInt(<span class="string">&quot;port&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineDataStream = env.socketTextStream(hostname, port)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineAndOneGroup = wordAndOne.keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sum = lineAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"><a href="#（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试" class="headerlink" title="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"></a>（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ nc -lk 7777</span><br></pre></td></tr></table></figure><blockquote><p>[!IMPORTANT]</p><p>注意：要先启动端口，后启动StreamWordCount程序，否则会报超时连接异常。</p></blockquote><h4 id="（3）启动StreamWordCount程序"><a href="#（3）启动StreamWordCount程序" class="headerlink" title="（3）启动StreamWordCount程序"></a>（3）启动StreamWordCount程序</h4><blockquote><p>[!WARNING]</p><p>我们会发现程序启动之后没有任何输出、也不会退出。这是正常的，因为Flink的流处理是事件驱动的，当前程序会一直处于监听状态，只有接收到数据才会执行任务、输出统计结果。</p></blockquote><h5 id="（4）从hadoop102发送数据"><a href="#（4）从hadoop102发送数据" class="headerlink" title="（4）从hadoop102发送数据"></a>（4）从hadoop102发送数据</h5><h5 id="①在hadoop102主机中，输入“hello-flink”，输出如下内容"><a href="#①在hadoop102主机中，输入“hello-flink”，输出如下内容" class="headerlink" title="①在hadoop102主机中，输入“hello flink”，输出如下内容"></a>①在hadoop102主机中，输入“hello flink”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13&gt; (flink,1)</span><br><span class="line">5&gt; (hello,1)</span><br></pre></td></tr></table></figure><h5 id="②再输入“hello-world”，输出如下内容"><a href="#②再输入“hello-world”，输出如下内容" class="headerlink" title="②再输入“hello world”，输出如下内容"></a>②再输入“hello world”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2&gt; (world,1)</span><br><span class="line">5&gt; (hello,2)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p><p>因为对于flatMap里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Flink快速上手&quot;&gt;&lt;a href=&quot;#Flink快速上手&quot; class=&quot;headerlink&quot; title=&quot;Flink快速上手&quot;&gt;&lt;/a&gt;Flink快速上手&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;[!NOTE]&lt;/p&gt;
&lt;p&gt;前提准备好相关maven环境</summary>
      
    
    
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/categories/Flink/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink中的时间和窗口、水位线、</title>
    <link href="https://bigdata-yx.github.io/posts/pd15.html"/>
    <id>https://bigdata-yx.github.io/posts/pd15.html</id>
    <published>2025-01-14T07:38:45.000Z</published>
    <updated>2025-01-14T07:38:45.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。"><a href="#在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。" class="headerlink" title="在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。"></a>在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。</h5><h5 id="所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。"><a href="#所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。" class="headerlink" title="所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。"></a>所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。</h5><h2 id="1-1-窗口（Window）"><a href="#1-1-窗口（Window）" class="headerlink" title="1.1 窗口（Window）"></a><strong>1.1</strong> 窗口（Window）</h2><h3 id="1-1-窗口的概念"><a href="#1-1-窗口的概念" class="headerlink" title="1.1 窗口的概念"></a><strong>1.1</strong> <strong>窗口的概念</strong></h3><h5 id="Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。"><a href="#Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。" class="headerlink" title="Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。"></a>Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。</h5><p><img src="https://pic1.imgdb.cn/item/67871bf5d0e0a243d4f45ac6.png"></p><p><strong>注意：</strong>Flink中窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。另外，这里我们认为到达窗口结束时间时，窗口就触发计算并关闭，事实上“触发计算”和“窗口关闭”两个行为也可以分开，这部分内容我们会在后面详述。</p><h3 id="6-1-2-窗口的分类"><a href="#6-1-2-窗口的分类" class="headerlink" title="6.1.2 窗口的分类"></a><strong>6.1.2</strong> <strong>窗口的分类</strong></h3><p>我们在上一节举的例子，其实是最为简单的一种时间窗口。在Flink中，窗口的应用非常灵活，我们可以使用各种不同类型的窗口来实现需求。接下来我们就从不同的角度，对Flink中内置的窗口做一个分类说明。</p><p>1）按照驱动类型分</p><p><img src="https://pic1.imgdb.cn/item/67871bf5d0e0a243d4f45ac6.png"></p><h4 id="2）按照窗口分配数据的规则分类"><a href="#2）按照窗口分配数据的规则分类" class="headerlink" title="2）按照窗口分配数据的规则分类"></a>2）按照窗口分配数据的规则分类</h4><h5 id="根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling-Window）、滑动窗口（Sliding-Window）、会话窗口（Session-Window），以及全局窗口（Global-Window）。"><a href="#根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling-Window）、滑动窗口（Sliding-Window）、会话窗口（Session-Window），以及全局窗口（Global-Window）。" class="headerlink" title="根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）、会话窗口（Session Window），以及全局窗口（Global Window）。"></a>根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）、会话窗口（Session Window），以及全局窗口（Global Window）。</h5><p><img src="https://pic1.imgdb.cn/item/67871d1ed0e0a243d4f45b16.png"></p><p><img src="https://pic1.imgdb.cn/item/67871d3fd0e0a243d4f45b18.png"></p><p><img src="https://pic1.imgdb.cn/item/67871d6ed0e0a243d4f45b3d.png"></p><p><img src="C:/Users/LHX/AppData/Roaming/Typora/typora-user-images/image-20250115103117635.png" alt="image-20250115103117635"></p><h3 id="1-1-3-窗口API概览"><a href="#1-1-3-窗口API概览" class="headerlink" title="1..1.3 窗口API概览"></a>1..1.3 窗口API概览</h3><h3 id="1）按键分区（Keyed）和非按键分区（Non-Keyed）"><a href="#1）按键分区（Keyed）和非按键分区（Non-Keyed）" class="headerlink" title="1）按键分区（Keyed）和非按键分区（Non-Keyed）"></a>1）按键分区（Keyed）和非按键分区（Non-Keyed）</h3><h4 id="在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。"><a href="#在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。" class="headerlink" title="在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。"></a>在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。</h4><h5 id="（1）按键分区窗口（Keyed-Windows）"><a href="#（1）按键分区窗口（Keyed-Windows）" class="headerlink" title="（1）按键分区窗口（Keyed Windows）"></a>（1）按键分区窗口（Keyed Windows）</h5><h5 id="经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical-streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。"><a href="#经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical-streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。" class="headerlink" title="经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。"></a>经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。</h5><h5 id="在代码实现上，我们需要先对DataStream调用-keyBy-进行按键分区，然后再调用-window-定义窗口。"><a href="#在代码实现上，我们需要先对DataStream调用-keyBy-进行按键分区，然后再调用-window-定义窗口。" class="headerlink" title="在代码实现上，我们需要先对DataStream调用.keyBy()进行按键分区，然后再调用.window()定义窗口。"></a>在代码实现上，我们需要先对DataStream调用.keyBy()进行按键分区，然后再调用.window()定义窗口。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br></pre></td></tr></table></figure><h5 id="（2）非按键分区（Non-Keyed-Windows）"><a href="#（2）非按键分区（Non-Keyed-Windows）" class="headerlink" title="（2）非按键分区（Non-Keyed Windows）"></a>（2）非按键分区（Non-Keyed Windows）</h5><h5 id="如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。"><a href="#如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。" class="headerlink" title="如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。"></a>如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。</h5><h5 id="在代码中，直接基于DataStream调用-windowAll-定义窗口。"><a href="#在代码中，直接基于DataStream调用-windowAll-定义窗口。" class="headerlink" title="在代码中，直接基于DataStream调用.windowAll()定义窗口。"></a>在代码中，直接基于DataStream调用.windowAll()定义窗口。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.windowAll(...)</span><br></pre></td></tr></table></figure><h5 id="注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。"><a href="#注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。" class="headerlink" title="注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。"></a>注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。</h5><h5 id="2）代码中窗口API的调用"><a href="#2）代码中窗口API的调用" class="headerlink" title="2）代码中窗口API的调用"></a>2）代码中窗口API的调用</h5><h5 id="窗口操作主要有两个部分：窗口分配器（Window-Assigners）和窗口函数（Window-Functions）。"><a href="#窗口操作主要有两个部分：窗口分配器（Window-Assigners）和窗口函数（Window-Functions）。" class="headerlink" title="窗口操作主要有两个部分：窗口分配器（Window Assigners）和窗口函数（Window Functions）。"></a>窗口操作主要有两个部分：窗口分配器（Window Assigners）和窗口函数（Window Functions）。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(&lt;key selector&gt;)</span><br><span class="line">       .window(&lt;window assigner&gt;)</span><br><span class="line">       .aggregate(&lt;window function&gt;)</span><br></pre></td></tr></table></figure><h5 id="其中-window-方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的-aggregate-方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只-aggregate-一种，我们接下来就详细展开讲解。"><a href="#其中-window-方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的-aggregate-方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只-aggregate-一种，我们接下来就详细展开讲解。" class="headerlink" title="其中.window()方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的.aggregate()方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只.aggregate()一种，我们接下来就详细展开讲解。"></a>其中.window()方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的.aggregate()方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只.aggregate()一种，我们接下来就详细展开讲解。</h5><h3 id="1-1-4-窗口分配器"><a href="#1-1-4-窗口分配器" class="headerlink" title="1.1.4 窗口分配器"></a><strong>1.1.4</strong> <strong>窗口分配器</strong></h3><h5 id="定义窗口分配器（Window-Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。"><a href="#定义窗口分配器（Window-Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。" class="headerlink" title="定义窗口分配器（Window Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。"></a>定义窗口分配器（Window Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。</h5><h5 id="窗口分配器最通用的定义方式，就是调用-window-方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用-windowAll-方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。"><a href="#窗口分配器最通用的定义方式，就是调用-window-方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用-windowAll-方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。" class="headerlink" title="窗口分配器最通用的定义方式，就是调用.window()方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用.windowAll()方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。"></a>窗口分配器最通用的定义方式，就是调用.window()方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用.windowAll()方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。</h5><h5 id="窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。"><a href="#窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。" class="headerlink" title="窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。"></a>窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。</h5><h4 id="1-1-4-1-时间窗口"><a href="#1-1-4-1-时间窗口" class="headerlink" title="1.1.4.1 时间窗口"></a>1.1.4.1 时间窗口</h4><h5 id="时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。"><a href="#时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。" class="headerlink" title="时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。"></a>时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。</h5><h5 id="（1）滚动处理时间窗口"><a href="#（1）滚动处理时间窗口" class="headerlink" title="（1）滚动处理时间窗口"></a>（1）滚动处理时间窗口</h5><h5 id="窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法-of-。"><a href="#窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法-of-。" class="headerlink" title="窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法.of()。"></a>窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法.of()。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h5 id="这里-of-方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。"><a href="#这里-of-方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。" class="headerlink" title="这里.of()方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。"></a>这里.of()方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。</h5><h5 id="另外，-of-还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。"><a href="#另外，-of-还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。" class="headerlink" title="另外，.of()还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。"></a>另外，.of()还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。</h5><h4 id="（2）滑动处理时间窗口"><a href="#（2）滑动处理时间窗口" class="headerlink" title="（2）滑动处理时间窗口"></a>（2）滑动处理时间窗口</h4><h5 id="窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法-of-。"><a href="#窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法-of-。" class="headerlink" title="窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法.of()。"></a>窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法.of()。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">SlidingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)，<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h5 id="这里-of-方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。"><a href="#这里-of-方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。" class="headerlink" title="这里.of()方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。"></a>这里.of()方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。</h5><h5 id="滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。"><a href="#滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。" class="headerlink" title="滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。"></a>滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。</h5><h4 id="（3）处理时间会话窗口"><a href="#（3）处理时间会话窗口" class="headerlink" title="（3）处理时间会话窗口"></a>（3）处理时间会话窗口</h4><h5 id="窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法-withGap-或者-withDynamicGap-。"><a href="#窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法-withGap-或者-withDynamicGap-。" class="headerlink" title="窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法.withGap()或者.withDynamicGap()。"></a>窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法.withGap()或者.withDynamicGap()。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">ProcessingTimeSessionWindows</span>.withGap(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h5 id="这里-withGap-方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session-gap。我们这里创建了静态会话超时时间为10秒的会话窗口。"><a href="#这里-withGap-方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session-gap。我们这里创建了静态会话超时时间为10秒的会话窗口。" class="headerlink" title="这里.withGap()方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session gap。我们这里创建了静态会话超时时间为10秒的会话窗口。"></a>这里.withGap()方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session gap。我们这里创建了静态会话超时时间为10秒的会话窗口。</h5><h5 id="另外，还可以调用withDynamicGap-方法定义session-gap的动态提取逻辑。"><a href="#另外，还可以调用withDynamicGap-方法定义session-gap的动态提取逻辑。" class="headerlink" title="另外，还可以调用withDynamicGap()方法定义session gap的动态提取逻辑。"></a>另外，还可以调用withDynamicGap()方法定义session gap的动态提取逻辑。</h5><h4 id="（4）滚动事件时间窗口"><a href="#（4）滚动事件时间窗口" class="headerlink" title="（4）滚动事件时间窗口"></a>（4）滚动事件时间窗口</h4><h5 id="窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。"><a href="#窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。" class="headerlink" title="窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。"></a>窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h4 id="（5）滑动事件时间窗口"><a href="#（5）滑动事件时间窗口" class="headerlink" title="（5）滑动事件时间窗口"></a>（5）滑动事件时间窗口</h4><h5 id="窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。"><a href="#窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。" class="headerlink" title="窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。"></a>窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)，<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h4 id="（6）事件时间会话窗口"><a href="#（6）事件时间会话窗口" class="headerlink" title="（6）事件时间会话窗口"></a>（6）事件时间会话窗口</h4><h5 id="窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。"><a href="#窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。" class="headerlink" title="窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。"></a>窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">EventTimeSessionWindows</span>.withGap(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h3 id="1-1-4-2-计数窗口"><a href="#1-1-4-2-计数窗口" class="headerlink" title="1.1.4.2 计数窗口"></a>1.1.4.2 计数窗口</h3><h5 id="计数窗口概念非常简单，本身底层是基于全局窗口（Global-Window）实现的。Flink为我们提供了非常方便的接口：直接调用-countWindow-方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。"><a href="#计数窗口概念非常简单，本身底层是基于全局窗口（Global-Window）实现的。Flink为我们提供了非常方便的接口：直接调用-countWindow-方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。" class="headerlink" title="计数窗口概念非常简单，本身底层是基于全局窗口（Global Window）实现的。Flink为我们提供了非常方便的接口：直接调用.countWindow()方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。"></a>计数窗口概念非常简单，本身底层是基于全局窗口（Global Window）实现的。Flink为我们提供了非常方便的接口：直接调用.countWindow()方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。</h5><h4 id="（1）滚动计数窗口"><a href="#（1）滚动计数窗口" class="headerlink" title="（1）滚动计数窗口"></a>（1）滚动计数窗口</h4><h4 id="滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。"><a href="#滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。" class="headerlink" title="滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。"></a>滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .countWindow(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h4 id="我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。"><a href="#我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。" class="headerlink" title="我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。"></a>我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。</h4><h4 id="（2）滑动计数窗口"><a href="#（2）滑动计数窗口" class="headerlink" title="（2）滑动计数窗口"></a>（2）滑动计数窗口</h4><h5 id="与滚动计数窗口类似，不过需要在-countWindow-调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。"><a href="#与滚动计数窗口类似，不过需要在-countWindow-调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。" class="headerlink" title="与滚动计数窗口类似，不过需要在.countWindow()调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。"></a>与滚动计数窗口类似，不过需要在.countWindow()调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .countWindow(<span class="number">10</span>，<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h5 id="我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。"><a href="#我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。" class="headerlink" title="我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。"></a>我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。</h5><h4 id="3）全局窗口"><a href="#3）全局窗口" class="headerlink" title="3）全局窗口"></a>3）全局窗口</h4><h5 id="全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用-window-，分配器由GlobalWindows类提供。"><a href="#全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用-window-，分配器由GlobalWindows类提供。" class="headerlink" title="全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用.window()，分配器由GlobalWindows类提供。"></a>全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用.window()，分配器由GlobalWindows类提供。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">GlobalWindows</span>.create());</span><br></pre></td></tr></table></figure><h5 id="需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。"><a href="#需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。" class="headerlink" title="需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。"></a>需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。</h5><h3 id="1-1-5-窗口函数"><a href="#1-1-5-窗口函数" class="headerlink" title="1.1.5 窗口函数"></a><strong>1.1.5</strong> <strong>窗口函数</strong></h3><p><img src="https://pic1.imgdb.cn/item/6787210fd0e0a243d4f45c59.png"></p><h5 id="窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。"><a href="#窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。" class="headerlink" title="窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。"></a>窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。</h5><h4 id="1-1-5-1-增量聚合函数（ReduceFunction-x2F-AggregateFunction）"><a href="#1-1-5-1-增量聚合函数（ReduceFunction-x2F-AggregateFunction）" class="headerlink" title="1.1.5.1 增量聚合函数（ReduceFunction &#x2F; AggregateFunction）"></a>1.1.5.1 增量聚合函数（ReduceFunction &#x2F; AggregateFunction）</h4><h5 id="窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。"><a href="#窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。" class="headerlink" title="窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。"></a>窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。</h5><h5 id="典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。"><a href="#典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。" class="headerlink" title="典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。"></a>典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。</h5><h5 id="1）归约函数（ReduceFunction）"><a href="#1）归约函数（ReduceFunction）" class="headerlink" title="1）归约函数（ReduceFunction）"></a>1）归约函数（ReduceFunction）</h5><h5 id="代码示例："><a href="#代码示例：" class="headerlink" title="代码示例："></a>代码示例：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WindowReduceDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> <span class="title class_">WaterSensorMapFunction</span>())</span><br><span class="line">                .keyBy(r -&gt; r.getId())</span><br><span class="line">                <span class="comment">// 设置滚动事件时间窗口</span></span><br><span class="line">                .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .reduce(<span class="keyword">new</span> <span class="title class_">ReduceFunction</span>&lt;WaterSensor&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> WaterSensor <span class="title function_">reduce</span><span class="params">(WaterSensor value1, WaterSensor value2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;调用reduce方法，之前的结果:&quot;</span>+value1 + <span class="string">&quot;,现在来的数据:&quot;</span>+value2);</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WaterSensor</span>(value1.getId(), System.currentTimeMillis(),value1.getVc()+value2.getVc());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2）聚合函数（AggregateFunction）"><a href="#2）聚合函数（AggregateFunction）" class="headerlink" title="2）聚合函数（AggregateFunction）"></a>2）聚合函数（AggregateFunction）</h4><h5 id="ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。"><a href="#ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。" class="headerlink" title="ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。"></a>ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。</h5><h5 id="Flink-Window-API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。"><a href="#Flink-Window-API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。" class="headerlink" title="Flink Window API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。"></a>Flink Window API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。</h5><h5 id="AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。"><a href="#AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。" class="headerlink" title="AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。"></a>AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。</h5><h5 id="接口中有四个方法："><a href="#接口中有四个方法：" class="headerlink" title="接口中有四个方法："></a>接口中有四个方法：</h5><ul><li>createAccumulator()：创建一个累加器，这就是为聚合创建了一个初始状态，每个聚合任务只会调用一次。</li><li>add()：将输入的元素添加到累加器中。</li><li>getResult()：从累加器中提取聚合的输出结果。</li><li>merge()：合并两个累加器，并将合并后的状态作为一个累加器返回。</li></ul><h5 id="所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator-为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add-方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult-方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。"><a href="#所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator-为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add-方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult-方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。" class="headerlink" title="所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator()为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add()方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult()方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。"></a>所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator()为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add()方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult()方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。</h5><h5 id="代码实现如下："><a href="#代码实现如下：" class="headerlink" title="代码实现如下："></a>代码实现如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">AggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AggregateFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">ClickSource</span>)</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计pv 和 uv 输出pv/uv的值</span></span><br><span class="line">    stream.keyBy(data =&gt; <span class="literal">true</span>)  <span class="comment">// 这里就是让所有数据进入同一个分组</span></span><br><span class="line">      .window( <span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">2</span>)))</span><br><span class="line">      .aggregate( <span class="keyword">new</span> <span class="type">PvUv</span> )</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义聚合函数, 用二元组(LOng, Set)表示中间聚合的(pv, uv)状态</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">PvUv</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">Event</span>, (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]), <span class="title">Double</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]) = (<span class="number">0</span>L, <span class="type">Set</span>[<span class="type">String</span>]())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每来一条数据,都回进行add叠加聚合</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(value: <span class="type">Event</span>, accumulator: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>])): (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]) = (accumulator._1 + <span class="number">1</span>, accumulator._2 + value.user)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回最终的计算结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResult</span></span>(accumulator: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>])): <span class="type">Double</span> = accumulator._1.toDouble /accumulator._2.size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(a: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]), b: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>])): (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]) = ???</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括-sum-x2F-max-x2F-maxBy-x2F-min-x2F-minBy-，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。"><a href="#另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括-sum-x2F-max-x2F-maxBy-x2F-min-x2F-minBy-，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。" class="headerlink" title="另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括.sum()&#x2F;max()&#x2F;maxBy()&#x2F;min()&#x2F;minBy()，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。"></a>另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括.sum()&#x2F;max()&#x2F;maxBy()&#x2F;min()&#x2F;minBy()，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。</h5><h4 id="1-1-5-2-全窗口函数（full-window-functions）"><a href="#1-1-5-2-全窗口函数（full-window-functions）" class="headerlink" title="1.1.5.2 全窗口函数（full window functions）"></a>1.1.5.2 全窗口函数（full window functions）</h4><h5 id="有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。"><a href="#有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。" class="headerlink" title="有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。"></a>有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。</h5><h4 id="所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。"><a href="#所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。" class="headerlink" title="所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。"></a>所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。</h4><h4 id="在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。"><a href="#在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。" class="headerlink" title="在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。"></a>在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。</h4><h5 id="1）窗口函数（WindowFunction）"><a href="#1）窗口函数（WindowFunction）" class="headerlink" title="1）窗口函数（WindowFunction）"></a>1）窗口函数（WindowFunction）</h5><h5 id="WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用-apply-方法，传入一个WindowFunction的实现类。"><a href="#WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用-apply-方法，传入一个WindowFunction的实现类。" class="headerlink" title="WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用.apply()方法，传入一个WindowFunction的实现类。"></a>WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用.apply()方法，传入一个WindowFunction的实现类。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .apply(<span class="keyword">new</span> <span class="type">MyWindowFunction</span>());</span><br></pre></td></tr></table></figure><h5 id="这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。"><a href="#这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。" class="headerlink" title="这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。"></a>这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。</h5><h5 id="不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。"><a href="#不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。" class="headerlink" title="不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。"></a>不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。</h5><h4 id="2）处理窗口函数（ProcessWindowFunction）"><a href="#2）处理窗口函数（ProcessWindowFunction）" class="headerlink" title="2）处理窗口函数（ProcessWindowFunction）"></a>2）处理窗口函数（ProcessWindowFunction）</h4><h5 id="ProcessWindowFunction是Window-API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing-time）和事件时间水位线（event-time-watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。"><a href="#ProcessWindowFunction是Window-API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing-time）和事件时间水位线（event-time-watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。" class="headerlink" title="ProcessWindowFunction是Window API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing time）和事件时间水位线（event time watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。"></a>ProcessWindowFunction是Window API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing time）和事件时间水位线（event time watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。</h5><h5 id="事实上，ProcessWindowFunction是Flink底层API——处理函数（process-function）中的一员，关于处理函数我们会在后续章节展开讲解。"><a href="#事实上，ProcessWindowFunction是Flink底层API——处理函数（process-function）中的一员，关于处理函数我们会在后续章节展开讲解。" class="headerlink" title="事实上，ProcessWindowFunction是Flink底层API——处理函数（process function）中的一员，关于处理函数我们会在后续章节展开讲解。"></a>事实上，ProcessWindowFunction是Flink底层API——处理函数（process function）中的一员，关于处理函数我们会在后续章节展开讲解。</h5><h5 id="代码实现如下：-1"><a href="#代码实现如下：-1" class="headerlink" title="代码实现如下："></a>代码实现如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> com.day03.<span class="type">AggregateFunctionTest</span>.<span class="type">PvUv</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.&#123;<span class="type">SlidingEventTimeWindows</span>, <span class="type">TumblingEventTimeWindows</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FullWindowFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource( <span class="keyword">new</span> <span class="type">ClickSource</span> )</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// pv是指页面被浏览的总次数，每次用户打开或刷新一个页面，都会增加一次Pv</span></span><br><span class="line">    <span class="comment">// uv是指一定时间内访问网站独立用户数，同一用户多次访问记为一次uv</span></span><br><span class="line">    <span class="comment">// 测试全窗口函数， 统计uv</span></span><br><span class="line">    stream.keyBy(data =&gt; <span class="string">&quot;key&quot;</span>)</span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">UvCountByWindow</span> )</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义实现ProcessWindowFunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UvCountByWindow</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Event</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">Event</span>], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 使用过一个Set进行去重操作</span></span><br><span class="line">      <span class="keyword">var</span> userSet = <span class="type">Set</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 从elements中提取所有数据，一次放入set中去重</span></span><br><span class="line">      elements.foreach(userSet += _.user)</span><br><span class="line">      <span class="keyword">val</span> uv = userSet.size</span><br><span class="line">      <span class="comment">// 提取窗口信息包装String输出</span></span><br><span class="line">      <span class="keyword">val</span> windowEnd = context.window.getEnd</span><br><span class="line">      <span class="keyword">val</span> windowsStart = context.window.getStart</span><br><span class="line"></span><br><span class="line">      out.collect(<span class="string">s&quot;窗口 <span class="subst">$windowsStart</span>~<span class="subst">$windowEnd</span> 的uv值为： <span class="subst">$uv</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-1-5-3-增量聚合和全窗口函数的结合使用"><a href="#1-1-5-3-增量聚合和全窗口函数的结合使用" class="headerlink" title="1.1.5.3 增量聚合和全窗口函数的结合使用"></a>1.1.5.3 增量聚合和全窗口函数的结合使用</h4><h6 id="在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window-API就给我们实现了这样的用法。"><a href="#在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window-API就给我们实现了这样的用法。" class="headerlink" title="在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window API就给我们实现了这样的用法。"></a>在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window API就给我们实现了这样的用法。</h6><h6 id="我们之前在调用WindowedStream的-reduce-和-aggregate-方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。"><a href="#我们之前在调用WindowedStream的-reduce-和-aggregate-方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。" class="headerlink" title="我们之前在调用WindowedStream的.reduce()和.aggregate()方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。"></a>我们之前在调用WindowedStream的.reduce()和.aggregate()方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">AggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UrlViewCountExample</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义统计输出结果的数据结构</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCount</span>(<span class="params">url: <span class="type">String</span>, count: <span class="type">Long</span>, windowStart: <span class="type">Long</span>, windowEnd: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource( <span class="keyword">new</span> <span class="type">ClickSource</span> )</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 结合使用增量聚合函数和全窗口函数，包装统计信息</span></span><br><span class="line">    stream.keyBy(_.url)</span><br><span class="line">      .window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">      .aggregate(<span class="keyword">new</span> <span class="type">UrlViewCountAgg</span>, <span class="keyword">new</span> <span class="type">UrlViewCountResult</span>)</span><br><span class="line">      .print</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现增强聚合函数</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountAgg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">Event</span>, <span class="type">Long</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">Long</span> = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(value: <span class="type">Event</span>, accumulator: <span class="type">Long</span>): <span class="type">Long</span> = accumulator + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResult</span></span>(accumulator: <span class="type">Long</span>): <span class="type">Long</span> = accumulator</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(a: <span class="type">Long</span>, b: <span class="type">Long</span>): <span class="type">Long</span> = ???</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现全窗口函数</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Long</span>, <span class="type">UrlViewCount</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(url: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">Long</span>], out: <span class="type">Collector</span>[<span class="type">UrlViewCount</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 提取需要的数据</span></span><br><span class="line">      <span class="keyword">val</span> count = elements.iterator.next()</span><br><span class="line">      <span class="keyword">val</span> start = context.window.getStart</span><br><span class="line">      <span class="keyword">val</span> end = context.window.getEnd</span><br><span class="line"></span><br><span class="line">      out.collect(<span class="type">UrlViewCount</span>(url, count, start, end))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-1-6-其他API"><a href="#1-1-6-其他API" class="headerlink" title="1.1.6 其他API"></a><strong>1.1.6</strong> <strong>其他API</strong></h3><h5 id="对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。"><a href="#对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。" class="headerlink" title="对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。"></a>对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。</h5><h4 id="1-1-6-1-触发器（Trigger）"><a href="#1-1-6-1-触发器（Trigger）" class="headerlink" title="1.1.6.1 触发器（Trigger）"></a>1.1.6.1 触发器（Trigger）</h4><h5 id="触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。"><a href="#触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。" class="headerlink" title="触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。"></a>触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。</h5><h5 id="基于WindowedStream调用-trigger-方法，就可以传入一个自定义的窗口触发器（Trigger）。"><a href="#基于WindowedStream调用-trigger-方法，就可以传入一个自定义的窗口触发器（Trigger）。" class="headerlink" title="基于WindowedStream调用.trigger()方法，就可以传入一个自定义的窗口触发器（Trigger）。"></a>基于WindowedStream调用.trigger()方法，就可以传入一个自定义的窗口触发器（Trigger）。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br><span class="line">       .trigger(<span class="keyword">new</span> <span class="type">MyTrigger</span>())</span><br></pre></td></tr></table></figure><h4 id="1-1-6-2-移除器（Evictor）"><a href="#1-1-6-2-移除器（Evictor）" class="headerlink" title="1.1.6.2 移除器（Evictor）"></a>1.1.6.2 移除器（Evictor）</h4><h5 id="移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用-evictor-方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。"><a href="#移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用-evictor-方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。" class="headerlink" title="移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用.evictor()方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。"></a>移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用.evictor()方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br><span class="line">       .evictor(<span class="keyword">new</span> <span class="type">MyEvictor</span>())</span><br></pre></td></tr></table></figure><h2 id="1-2-时间语义"><a href="#1-2-时间语义" class="headerlink" title="1.2 时间语义"></a><strong>1.2</strong> <strong>时间语义</strong></h2><h3 id="1-2-1-Flink中的时间语义"><a href="#1-2-1-Flink中的时间语义" class="headerlink" title="1**.2.1 Flink中的时间语义**"></a>1**.2.1 Flink中的时间语义**</h3><p><img src="https://pic1.imgdb.cn/item/678728cbd0e0a243d4f45ec2.png"></p><h3 id="1-2-2-哪种时间语义更重要"><a href="#1-2-2-哪种时间语义更重要" class="headerlink" title="1.2.2 哪种时间语义更重要"></a><strong>1.2.2</strong> <strong>哪种时间语义更重要</strong></h3><p><strong>1）从《星球大战》说起</strong></p><h4 id="为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。"><a href="#为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。" class="headerlink" title="为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。"></a>为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。</h4><p><img src="https://pic1.imgdb.cn/item/67872901d0e0a243d4f45ed1.png"></p><h5 id="如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。"><a href="#如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。" class="headerlink" title="如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。"></a>如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。</h5><h5 id="2）数据处理系统中的时间语义"><a href="#2）数据处理系统中的时间语义" class="headerlink" title="2）数据处理系统中的时间语义"></a><strong>2）数据处理系统中的时间语义</strong></h5><h5 id="在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。"><a href="#在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。" class="headerlink" title="在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。"></a>在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。</h5><h5 id="在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1-12版本开始，Flink已经将事件时间作为默认的时间语义了。"><a href="#在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1-12版本开始，Flink已经将事件时间作为默认的时间语义了。" class="headerlink" title="在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1.12版本开始，Flink已经将事件时间作为默认的时间语义了。"></a>在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1.12版本开始，Flink已经将事件时间作为默认的时间语义了。</h5><h2 id="1-3-水位线（Watermark）"><a href="#1-3-水位线（Watermark）" class="headerlink" title="1.3 水位线（Watermark）"></a><strong>1.3</strong> <strong>水位线（</strong>Watermark<strong>）</strong></h2><h3 id="1-3-1-事件时间和窗口"><a href="#1-3-1-事件时间和窗口" class="headerlink" title="1.3.1 事件时间和窗口"></a><strong>1.3.1</strong> <strong>事件时间和窗口</strong></h3><p><img src="https://pic1.imgdb.cn/item/6787295ad0e0a243d4f45f29.png"></p><h3 id="1-3-2-什么是水位线"><a href="#1-3-2-什么是水位线" class="headerlink" title="1.3.2 什么是水位线"></a><strong>1.3.2</strong> <strong>什么是水位线</strong></h3><p><img src="https://pic1.imgdb.cn/item/67872997d0e0a243d4f45f52.png"></p><p><img src="https://pic1.imgdb.cn/item/678729add0e0a243d4f45f58.png"></p><p><img src="https://pic1.imgdb.cn/item/678729ccd0e0a243d4f45f67.png"></p><p><img src="https://pic1.imgdb.cn/item/678729efd0e0a243d4f45f6f.png"></p><p><strong>1.3.3 水位线和窗口的工作原理</strong></p><p><img src="https://pic1.imgdb.cn/item/67872a10d0e0a243d4f45f76.png"></p><p><img src="https://pic1.imgdb.cn/item/67872a41d0e0a243d4f45f7a.png"></p><blockquote><p>[!CAUTION]</p><p><strong>注意：</strong>Flink中窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。另外，这里我们认为到达窗口结束时间时，窗口就触发计算并关闭，事实上“触发计算”和“窗口关闭”两个行为也可以分开，这部分内容我们会在后面详述。</p></blockquote><h3 id="1-3-4-生成水位线"><a href="#1-3-4-生成水位线" class="headerlink" title="1.3.4 生成水位线"></a><strong>1.3.4</strong> <strong>生成水位线</strong></h3><h4 id="1-3-4-1-生成水位线的总体原则"><a href="#1-3-4-1-生成水位线的总体原则" class="headerlink" title="1.3.4.1 生成水位线的总体原则"></a>1.3.4.1 生成水位线的总体原则</h4><h5 id="完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。"><a href="#完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。" class="headerlink" title="完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。"></a>完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。</h5><h5 id="如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。"><a href="#如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。" class="headerlink" title="如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。"></a>如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。</h5><h5 id="所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。"><a href="#所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。" class="headerlink" title="所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。"></a>所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。</h5><h4 id="1-3-4-2-水位线生成策略"><a href="#1-3-4-2-水位线生成策略" class="headerlink" title="1.3.4.2 水位线生成策略"></a>1.3.4.2 水位线生成策略</h4><h5 id="在Flink的DataStream-API中，有一个单独用于生成水位线的方法：-assignTimestampsAndWatermarks-，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下："><a href="#在Flink的DataStream-API中，有一个单独用于生成水位线的方法：-assignTimestampsAndWatermarks-，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下：" class="headerlink" title="在Flink的DataStream API中，有一个单独用于生成水位线的方法：.assignTimestampsAndWatermarks()，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下："></a>在Flink的DataStream API中，有一个单独用于生成水位线的方法：.assignTimestampsAndWatermarks()，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> <span class="title class_">ClickSource</span>());</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; withTimestampsAndWatermarks = </span><br><span class="line">stream.assignTimestampsAndWatermarks(&lt;watermark strategy&gt;);</span><br></pre></td></tr></table></figure><h5 id="说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。"><a href="#说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。" class="headerlink" title="说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。"></a>说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.有序流的水位线生成策略 forMonotonousTimestamps</span></span><br><span class="line">stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forMonotonousTimestamps[<span class="type">Event</span>]()</span><br><span class="line">  .withTimestampAssigner(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="comment">// 提取时间戳的方法</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">    &#125;</span><br><span class="line">  ))</span><br></pre></td></tr></table></figure><h4 id="1-3-4-3-Flink内置水位线"><a href="#1-3-4-3-Flink内置水位线" class="headerlink" title="1.3.4.3 Flink内置水位线"></a>1.3.4.3 Flink内置水位线</h4><p><strong>1）有序流中内置水位线设置</strong></p><h5 id="对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy-forMonotonousTimestamps-方法就可以实现。"><a href="#对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy-forMonotonousTimestamps-方法就可以实现。" class="headerlink" title="对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy.forMonotonousTimestamps()方法就可以实现。"></a>对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy.forMonotonousTimestamps()方法就可以实现。</h5><p><strong>2）乱序流中内置水位线设置</strong></p><h5 id="由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy-forBoundedOutOfOrderness-方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。"><a href="#由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy-forBoundedOutOfOrderness-方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。" class="headerlink" title="由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy. forBoundedOutOfOrderness()方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。"></a>由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy. forBoundedOutOfOrderness()方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。</h5><h4 id="3-4-4-自定义水位线生成器"><a href="#3-4-4-自定义水位线生成器" class="headerlink" title=".3.4.4 自定义水位线生成器"></a>.3.4.4 自定义水位线生成器</h4><p><strong>1）周期性水位线生成器（Periodic Generator）</strong></p><h5 id="周期性生成器一般是通过onEvent-观察判断输入的事件，而在onPeriodicEmit-里发出水位线。"><a href="#周期性生成器一般是通过onEvent-观察判断输入的事件，而在onPeriodicEmit-里发出水位线。" class="headerlink" title="周期性生成器一般是通过onEvent()观察判断输入的事件，而在onPeriodicEmit()里发出水位线。"></a>周期性生成器一般是通过onEvent()观察判断输入的事件，而在onPeriodicEmit()里发出水位线。</h5><h5 id="下面是一段自定义、有序流、无序流周期性生成水位线的代码："><a href="#下面是一段自定义、有序流、无序流周期性生成水位线的代码：" class="headerlink" title="下面是一段自定义、有序流、无序流周期性生成水位线的代码："></a>下面是一段自定义、有序流、无序流周期性生成水位线的代码：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">TimestampAssigner</span>, <span class="type">TimestampAssignerSupplier</span>, <span class="type">Watermark</span>, <span class="type">WatermarkGenerator</span>, <span class="type">WatermarkGeneratorSupplier</span>, <span class="type">WatermarkOutput</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 自动生成水位线的周期时间间隔</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">500</span>L)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.有序流的水位线生成策略 forMonotonousTimestamps</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forMonotonousTimestamps[<span class="type">Event</span>]()</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 提取时间戳的方法</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.乱序流的水位线生成策略  forBoundedOutOfOrderness</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness[<span class="type">Event</span>](<span class="type">Duration</span>.ofSeconds(<span class="number">2</span>))</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 水位线的设置有周期性和事件性，周期性就是隔一段时间检查一次，事件性就是来一个事件就检测一次</span></span><br><span class="line">    <span class="comment">// 自定义水位线 双new</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">WatermarkStrategy</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createTimestampAssigner</span></span>(context: <span class="type">TimestampAssignerSupplier</span>.<span class="type">Context</span>): <span class="type">TimestampAssigner</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 如果event中没有时间戳字段那就是，得替换掉后面的逻辑，换成自己的逻辑比如：System.currentTimeMillis() 使用当前系统时间</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createWatermarkGenerator</span></span>(context: <span class="type">WatermarkGeneratorSupplier</span>.<span class="type">Context</span>): <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 定义一个延迟时间</span></span><br><span class="line">          <span class="keyword">val</span> delay = <span class="number">5000</span>L</span><br><span class="line">          <span class="comment">// 定义属性保存最大时间戳 就算是0的时间来了也能正常处理</span></span><br><span class="line">          <span class="keyword">var</span> maxTs = <span class="type">Long</span>.<span class="type">MinValue</span> + delay + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">// 每来一条数据进行调用，有了这个下面那个周期性的就没用了</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEvent</span></span>(event: <span class="type">Event</span>, eventTimestamp: <span class="type">Long</span>, output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="comment">// 更新最大时间戳</span></span><br><span class="line">            maxTs = math.max(maxTs, event.timestamp)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 按照周期性进行调用</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onPeriodicEmit</span></span>(output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> watermark = <span class="keyword">new</span> <span class="type">Watermark</span>(maxTs - delay - <span class="number">1</span>)</span><br><span class="line">            output.emitWatermark(watermark)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="我们在onPeriodicEmit-里调用output-emitWatermark-，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。"><a href="#我们在onPeriodicEmit-里调用output-emitWatermark-，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。" class="headerlink" title="我们在onPeriodicEmit()里调用output.emitWatermark()，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。"></a>我们在onPeriodicEmit()里调用output.emitWatermark()，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。</h5><h5 id="如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms"><a href="#如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms" class="headerlink" title="如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms"></a>如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.getConfig().setAutoWatermarkInterval(<span class="number">400</span>L);</span><br></pre></td></tr></table></figure><p><strong>2）断点式水位线生成器（</strong>Punctuated Generator<strong>）</strong></p><h5 id="断点式生成器会不停地检测onEvent-中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。"><a href="#断点式生成器会不停地检测onEvent-中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。" class="headerlink" title="断点式生成器会不停地检测onEvent()中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。"></a>断点式生成器会不停地检测onEvent()中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。</h5><p><strong>3）在数据源中发送水位线</strong></p><h5 id="我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下："><a href="#我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下：" class="headerlink" title="我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下："></a>我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.fromSource(</span><br><span class="line">kafkaSource, <span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">3</span>)), <span class="string">&quot;kafkasource&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="1-3-5-水位线的传递"><a href="#1-3-5-水位线的传递" class="headerlink" title="1.3.5 水位线的传递"></a><strong>1.3.5</strong> <strong>水位线的传递</strong></h3><p><img src="https://pic1.imgdb.cn/item/67872c85d0e0a243d4f4601f.png"></p><h5 id="在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。"><a href="#在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。" class="headerlink" title="在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。"></a>在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。</h5><h5 id="水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。"><a href="#水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。" class="headerlink" title="水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。"></a>水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。</h5><h5 id="案例：1-3-4-3-中乱序流的watermark，将并行度设为2，观察现象。"><a href="#案例：1-3-4-3-中乱序流的watermark，将并行度设为2，观察现象。" class="headerlink" title="案例：1.3.4.3 中乱序流的watermark，将并行度设为2，观察现象。"></a>案例：1.3.4.3 中乱序流的watermark，将并行度设为2，观察现象。</h5><h5 id="在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。"><a href="#在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。" class="headerlink" title="在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。"></a>在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。</h5><h5 id="用1-3-4-6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下："><a href="#用1-3-4-6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下：" class="headerlink" title="用1.3.4.6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下："></a>用1.3.4.6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingProcessingTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkTest1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 自动生成水位线的周期时间间隔</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">500</span>L)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.socketTextStream(<span class="string">&quot;master&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.有序流的水位线生成策略 forMonotonousTimestamps</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forMonotonousTimestamps[<span class="type">Event</span>]()</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 提取时间戳的方法</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.乱序流的水位线生成策略  forBoundedOutOfOrderness</span></span><br><span class="line">    <span class="comment">// 这个5指的是最大延迟时间</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness[<span class="type">Event</span>](<span class="type">Duration</span>.ofSeconds(<span class="number">5</span>))</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line">      .keyBy(_.user)</span><br><span class="line">      .window(<span class="type">SlidingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">      .process( <span class="keyword">new</span> <span class="type">WatermarkWindowResult</span> )</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 水位线的设置有周期性和事件性，周期性就是隔一段时间检查一次，事件性就是来一个事件就检测一次</span></span><br><span class="line">    <span class="comment">// 自定义水位线 双new</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">WatermarkStrategy</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createTimestampAssigner</span></span>(context: <span class="type">TimestampAssignerSupplier</span>.<span class="type">Context</span>): <span class="type">TimestampAssigner</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 如果event中没有时间戳字段那就是，得替换掉后面的逻辑，换成自己的逻辑比如：System.currentTimeMillis() 使用当前系统时间</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createWatermarkGenerator</span></span>(context: <span class="type">WatermarkGeneratorSupplier</span>.<span class="type">Context</span>): <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 定义一个延迟时间</span></span><br><span class="line">          <span class="keyword">val</span> delay = <span class="number">5000</span>L</span><br><span class="line">          <span class="comment">// 定义属性保存最大时间戳 就算是0的时间来了也能正常处理</span></span><br><span class="line">          <span class="keyword">var</span> maxTs = <span class="type">Long</span>.<span class="type">MinValue</span> + delay + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">// 每来一条数据进行调用，有了这个下面那个周期性的就没用了</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEvent</span></span>(event: <span class="type">Event</span>, eventTimestamp: <span class="type">Long</span>, output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="comment">// 更新最大时间戳</span></span><br><span class="line">            maxTs = math.max(maxTs, event.timestamp)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 按照周期性进行调用</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onPeriodicEmit</span></span>(output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> watermark = <span class="keyword">new</span> <span class="type">Watermark</span>(maxTs - delay - <span class="number">1</span>)</span><br><span class="line">            output.emitWatermark(watermark)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义全窗口函数</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">WatermarkWindowResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Event</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(user: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">Event</span>], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 提取信息</span></span><br><span class="line">      <span class="keyword">val</span> start = context.window.getStart</span><br><span class="line">      <span class="keyword">val</span> end = context.window.getEnd</span><br><span class="line">      <span class="keyword">val</span> count = elements.size</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 增加水位线信息</span></span><br><span class="line">      <span class="keyword">val</span> currentWatermark = context.currentWatermark</span><br><span class="line"></span><br><span class="line">      out.collect(<span class="string">s&quot;窗口 <span class="subst">$start</span>~<span class="subst">$end</span>, 用户 <span class="subst">$user</span> 的活跃度为：<span class="subst">$count</span>, 水位线现在位于: <span class="subst">$currentWatermark</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>1.3.6 迟到数据的处理</strong></p><h4 id="1-3-6-1-推迟水印推进"><a href="#1-3-6-1-推迟水印推进" class="headerlink" title="1.3.6.1 推迟水印推进"></a>1.3.6.1 推迟水印推进</h4><h5 id="在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。"><a href="#在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。" class="headerlink" title="在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。"></a>在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><h5 id="1-3-6-2-设置窗口延迟关闭"><a href="#1-3-6-2-设置窗口延迟关闭" class="headerlink" title="1.3.6.2 设置窗口延迟关闭"></a>1.3.6.2 设置窗口延迟关闭</h5><h5 id="Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。"><a href="#Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。" class="headerlink" title="Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。"></a>Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。</h5><h6 id="以后每来一条迟到数据，就触发一次这条数据所在窗口计算-增量计算-。直到wartermark-超过了窗口结束时间-推迟时间，此时窗口会真正关闭。"><a href="#以后每来一条迟到数据，就触发一次这条数据所在窗口计算-增量计算-。直到wartermark-超过了窗口结束时间-推迟时间，此时窗口会真正关闭。" class="headerlink" title="以后每来一条迟到数据，就触发一次这条数据所在窗口计算(增量计算)。直到wartermark 超过了窗口结束时间+推迟时间，此时窗口会真正关闭。"></a>以后每来一条迟到数据，就触发一次这条数据所在窗口计算(增量计算)。直到wartermark 超过了窗口结束时间+推迟时间，此时窗口会真正关闭。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">.allowedLateness(<span class="type">Time</span>.seconds(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h5 id="注意"><a href="#注意" class="headerlink" title="注意:"></a>注意:</h5><h5 id="允许迟到只能运用在event-time上"><a href="#允许迟到只能运用在event-time上" class="headerlink" title="允许迟到只能运用在event time上"></a>允许迟到只能运用在event time上</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.windowAll(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">.allowedLateness(<span class="type">Time</span>.seconds(<span class="number">3</span>))</span><br><span class="line">.sideOutputLateData(lateWS)</span><br></pre></td></tr></table></figure><h5 id="完整案例代码如下："><a href="#完整案例代码如下：" class="headerlink" title="完整案例代码如下："></a>完整案例代码如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> com.day03.<span class="type">UrlViewCountExample</span>.&#123;<span class="type">UrlViewCountAgg</span>, <span class="type">UrlViewCountResult</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.<span class="type">WatermarkStrategy</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ProcessLateDataExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.socketTextStream(<span class="string">&quot;master&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong)</span><br><span class="line">      &#125;)</span><br><span class="line">      .assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness[<span class="type">Event</span>](<span class="type">Duration</span>.ofSeconds(<span class="number">5</span>)))</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个侧输出流的标签,泛型就是你要放什么数据你就要改成什么</span></span><br><span class="line">    <span class="keyword">val</span> outputTag = <span class="type">OutputTag</span>[<span class="type">Event</span>](<span class="string">&quot;late-data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = stream.keyBy(_.url)</span><br><span class="line">      .window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">      <span class="comment">// 指定窗口允许等待的时间</span></span><br><span class="line">      .allowedLateness(<span class="type">Time</span>.minutes(<span class="number">1</span>))</span><br><span class="line">      <span class="comment">// 将迟到数据输出到侧输出流</span></span><br><span class="line">      .sideOutputLateData(outputTag)</span><br><span class="line">      .aggregate(<span class="keyword">new</span> <span class="type">UrlViewCountAgg</span>, <span class="keyword">new</span> <span class="type">UrlViewCountResult</span>)</span><br><span class="line">    result.print(<span class="string">&quot;result&quot;</span>)</span><br><span class="line"></span><br><span class="line">    stream.print(<span class="string">&quot;input&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将侧输出流的数据进行打印输出</span></span><br><span class="line">    result.getSideOutput(outputTag).print(<span class="string">&quot;late data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h5 id=&quot;在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。&quot;&gt;&lt;a href=&quot;#在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们</summary>
      
    
    
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/categories/Flink/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink集群部署搭建</title>
    <link href="https://bigdata-yx.github.io/posts/pd12.html"/>
    <id>https://bigdata-yx.github.io/posts/pd12.html</id>
    <published>2025-01-14T07:38:45.000Z</published>
    <updated>2025-01-14T07:38:45.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink1-14-0集群部署"><a href="#Flink1-14-0集群部署" class="headerlink" title="Flink1.14.0集群部署"></a><strong>Flink1.14.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line"><span class="comment">#     将jobmanager.rpc.address的主机名改成master就好了</span></span><br><span class="line"><span class="comment"># 分发到其他主机，然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-17-0集群部署"><a href="#Flink1-17-0集群部署" class="headerlink" title="Flink1.17.0集群部署"></a><strong>Flink1.17.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line">    <span class="comment"># JobManager节点地址.</span></span><br><span class="line">    jobmanager.rpc.address: master</span><br><span class="line">    jobmanager.bind-host: 0.0.0.0</span><br><span class="line">    rest.address: master</span><br><span class="line">    rest.bind-address: 0.0.0.0</span><br><span class="line">    <span class="comment"># TaskManager节点地址.需要配置为当前机器名</span></span><br><span class="line">    taskmanager.bind-host: 0.0.0.0</span><br><span class="line">    taskmanager.host: master</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发到其他主机</span></span><br><span class="line"><span class="comment"># 修改slave1的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave1</span><br><span class="line"><span class="comment"># 修改slave2的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave2</span><br><span class="line"><span class="comment"># 然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-14-0高可用部署"><a href="#Flink1-14-0高可用部署" class="headerlink" title="Flink1.14.0高可用部署"></a><strong>Flink1.14.0高可用部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前面正常部署，后面在high中新增</span></span><br><span class="line"><span class="comment">#开启HA，使用文件系统作为快照存储</span></span><br><span class="line">state.backend: filesystem （选填）</span><br><span class="line"> </span><br><span class="line"><span class="comment">#启用检查点，可以将快照保存到HDFS （选填）</span></span><br><span class="line">state.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints</span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用zookeeper搭建高可用 （必填）</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储JobManager的元数据到HDFS （必填）</span></span><br><span class="line">high-availability.storageDir: hdfs://node1:8020/flink/ha/</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 配置ZK集群地址（必填）</span></span><br><span class="line">high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定flink在zokkeper的位置（选填）</span></span><br><span class="line"> high-availability.zookeeper.path.root: /flink</span><br><span class="line"> </span><br><span class="line">  然后将flink-shaded-hadoop-2-uber-2.8.3-10.0.jar复制到/root/software/flink/lib中</span><br><span class="line"><span class="comment">#  然后scp最后就可以启动了</span></span><br></pre></td></tr></table></figure><h1 id="Flink-On-Yarn"><a href="#Flink-On-Yarn" class="headerlink" title="Flink On Yarn"></a>Flink On Yarn</h1><h3 id="1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode"><a href="#1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode" class="headerlink" title="1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode"></a><strong>1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">配置环境变量，增加环境变量配置如下：</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure><h3 id="2-会话模式部署"><a href="#2-会话模式部署" class="headerlink" title="2 会话模式部署"></a>2 会话模式部署</h3><h3 id="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下："><a href="#YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下：" class="headerlink" title="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下："></a>YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下：</h3><h3 id="（1）启动集群"><a href="#（1）启动集群" class="headerlink" title="（1）启动集群"></a>（1）启动集群</h3><ul><li><h3 id="（1）启动Hadoop集群（HDFS、YARN）。"><a href="#（1）启动Hadoop集群（HDFS、YARN）。" class="headerlink" title="（1）启动Hadoop集群（HDFS、YARN）。"></a>（1）启动Hadoop集群（HDFS、YARN）。</h3></li><li><h3 id="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"><a href="#（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。" class="headerlink" title="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"></a>（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。</h3></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh -nm <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">-d：分离模式，如果你不想让Flink YARN客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，YARN session也可以后台运行。</span><br><span class="line">-jm（–jobManagerMemory）：配置JobManager所需内存，默认单位MB。</span><br><span class="line">-nm（–name）：配置在YARN UI界面上显示的任务名。</span><br><span class="line">-qu（–queue）：指定YARN队列名。</span><br><span class="line">-tm（–taskManager）：配置每个TaskManager所使用内存。</span><br></pre></td></tr></table></figure><h5 id="YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。"><a href="#YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。" class="headerlink" title="YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。"></a>YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2022-11-17 15:20:52,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface hadoop104:40825 of application <span class="string">&#x27;application_1668668287070_0005&#x27;</span>.</span><br><span class="line">JobManager Web Interface: http://hadoop104:40825</span><br></pre></td></tr></table></figure><h3 id="（2）通过命令行提交作业"><a href="#（2）通过命令行提交作业" class="headerlink" title="（2）通过命令行提交作业"></a><strong>（2）通过命令行提交作业</strong></h3><ul><li>① 将FlinkTutorial-1.0-SNAPSHOT.jar任务上传至集群。</li><li>② 执行以下命令将该任务提交到已经开启的Yarn-Session中运行。</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h3 id="3-单作业模式部署"><a href="#3-单作业模式部署" class="headerlink" title="3 单作业模式部署"></a><strong>3 单作业模式部署</strong></h3><h6 id="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"><a href="#在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群" class="headerlink" title="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"></a>在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群</h6><h6 id="（1）执行命令提交作业"><a href="#（1）执行命令提交作业" class="headerlink" title="（1）执行命令提交作业"></a>（1）执行命令提交作业</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -d -t yarn-per-job -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h6 id="注意：如果启动过程中报如下异常"><a href="#注意：如果启动过程中报如下异常" class="headerlink" title="注意：如果启动过程中报如下异常"></a>注意：如果启动过程中报如下异常</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread “Thread-5” java.lang.IllegalStateException: Trying to access closed classloader. Please check <span class="keyword">if</span> you store classloaders directly or indirectly <span class="keyword">in</span> static fields. If the stacktrace suggests that the leak occurs <span class="keyword">in</span> a third party library and cannot be fixed immediately, you can <span class="built_in">disable</span> this check with the configuration ‘classloader.check-leaked-classloader’.</span><br><span class="line">at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders</span><br></pre></td></tr></table></figure><h6 id="解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置"><a href="#解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置" class="headerlink" title="解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置"></a>解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim flink-conf.yaml</span><br><span class="line"></span><br><span class="line">classloader.check-leaked-classloader: <span class="literal">false</span></span><br></pre></td></tr></table></figure><h6 id="（2）可以使用命令行查看或取消作业，命令如下。"><a href="#（2）可以使用命令行查看或取消作业，命令如下。" class="headerlink" title="（2）可以使用命令行查看或取消作业，命令如下。"></a>（2）可以使用命令行查看或取消作业，命令如下。</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h3 id="4-应用模式部署"><a href="#4-应用模式部署" class="headerlink" title="4 应用模式部署"></a><strong>4 应用模式部署</strong></h3><h4 id="1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。"><a href="#1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。" class="headerlink" title="(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。"></a>(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar </span><br></pre></td></tr></table></figure><h5 id="2-在命令行中查看或取消作业。"><a href="#2-在命令行中查看或取消作业。" class="headerlink" title="(2)在命令行中查看或取消作业。"></a>(2)在命令行中查看或取消作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h4 id="2）上传HDFS提交"><a href="#2）上传HDFS提交" class="headerlink" title="2）上传HDFS提交"></a><strong>2）上传HDFS提交</strong></h4><h5 id="可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。"><a href="#可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。" class="headerlink" title="可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。"></a>可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。</h5><h6 id="（1）上传flink的lib和plugins到HDFS上"><a href="#（1）上传flink的lib和plugins到HDFS上" class="headerlink" title="（1）上传flink的lib和plugins到HDFS上"></a>（1）上传flink的lib和plugins到HDFS上</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -<span class="built_in">mkdir</span> /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put lib/ /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put plugins/ /flink-dist</span><br></pre></td></tr></table></figure><h5 id="（2）提交作业"><a href="#（2）提交作业" class="headerlink" title="（2）提交作业"></a>（2）提交作业</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application    -Dyarn.provided.lib.dirs=<span class="string">&quot;hdfs://hadoop102:8020/flink-dist&quot;</span>    -c com.atguigu.wc.SocketStreamWordCount  hdfs://hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Flink1-14-0集群部署&quot;&gt;&lt;a href=&quot;#Flink1-14-0集群部署&quot; class=&quot;headerlink&quot; title=&quot;Flink1.14.0集群部署&quot;&gt;&lt;/a&gt;&lt;strong&gt;Flink1.14.0集群部署&lt;/strong&gt;&lt;/h1&gt;&lt;fig</summary>
      
    
    
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/categories/Flink/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink的DataStreamAPI</title>
    <link href="https://bigdata-yx.github.io/posts/pd14.html"/>
    <id>https://bigdata-yx.github.io/posts/pd14.html</id>
    <published>2025-01-14T07:38:45.000Z</published>
    <updated>2025-01-14T07:38:45.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："><a href="#DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：" class="headerlink" title="DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："></a>DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：</h5><p><img src="https://pic1.imgdb.cn/item/67861fbed0e0a243d4f42964.png"></p><h5 id="在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法："><a href="#在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法：" class="headerlink" title="在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法："></a>在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.addSource(...);</span><br></pre></td></tr></table></figure><h5 id="方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。"><a href="#方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。" class="headerlink" title="方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。"></a>方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。</h5><h5 id="从Flink1-12开始，主要使用流批统一的新Source架构："><a href="#从Flink1-12开始，主要使用流批统一的新Source架构：" class="headerlink" title="从Flink1.12开始，主要使用流批统一的新Source架构："></a>从Flink1.12开始，主要使用流批统一的新Source架构：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStreamSource</span>&lt;<span class="type">String</span>&gt; stream = env.fromSource(…)</span><br></pre></td></tr></table></figure><h5 id="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"><a href="#Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。" class="headerlink" title="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"></a>Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。</h5><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作:"></a>准备工作:</h2><h5 id="为了方便练习，这里使用WaterSensor作为数据模型。"><a href="#为了方便练习，这里使用WaterSensor作为数据模型。" class="headerlink" title="为了方便练习，这里使用WaterSensor作为数据模型。"></a>为了方便练习，这里使用WaterSensor作为数据模型。</h5><table><thead><tr><th><strong>id</strong></th><th><strong>String</strong></th><th><strong>水位传感器类型</strong></th></tr></thead><tbody><tr><td><strong>ts</strong></td><td><strong>Long</strong></td><td><strong>传感器记录时间戳</strong></td></tr><tr><td><strong>vc</strong></td><td><strong>Integer</strong></td><td><strong>水位记录</strong></td></tr></tbody></table><h4 id="具体代码如下："><a href="#具体代码如下：" class="headerlink" title="具体代码如下："></a>具体代码如下：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensor</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String id;</span><br><span class="line">    <span class="keyword">public</span> Long ts;</span><br><span class="line">    <span class="keyword">public</span> Integer vc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">(String id, Long ts, Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getId</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setId</span><span class="params">(String id)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getTs</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setTs</span><span class="params">(Long ts)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">getVc</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setVc</span><span class="params">(Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;WaterSensor&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;id=&#x27;&quot;</span> + id + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, ts=&quot;</span> + ts +</span><br><span class="line">                <span class="string">&quot;, vc=&quot;</span> + vc +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span> == o) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="literal">null</span> || getClass() != o.getClass()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">WaterSensor</span> <span class="variable">that</span> <span class="operator">=</span> (WaterSensor) o;</span><br><span class="line">        <span class="keyword">return</span> Objects.equals(id, that.id) &amp;&amp;</span><br><span class="line">                Objects.equals(ts, that.ts) &amp;&amp;</span><br><span class="line">                Objects.equals(vc, that.vc);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Objects.hash(id, ts, vc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="这里需要注意，我们定义的WaterSensor，有这样几个特点："><a href="#这里需要注意，我们定义的WaterSensor，有这样几个特点：" class="headerlink" title="这里需要注意，我们定义的WaterSensor，有这样几个特点："></a>这里需要注意，我们定义的WaterSensor，有这样几个特点：</h4><ul><li>类是公有（public）的</li><li>有一个无参的构造方法</li><li>所有属性都是公有（public）的</li><li>所有属性的类型都是可以序列化的</li></ul><h6 id="Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"><a href="#Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。" class="headerlink" title="Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"></a>Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。</h6><h6 id="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"><a href="#我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。" class="headerlink" title="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"></a>我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。</h6><h3 id="1-从集合、文件、元素中读取数据"><a href="#1-从集合、文件、元素中读取数据" class="headerlink" title="1.从集合、文件、元素中读取数据"></a>1.从集合、文件、元素中读取数据</h3><h5 id="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"><a href="#最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。" class="headerlink" title="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"></a>最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceBoundedTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">user: <span class="type">String</span>, url: <span class="type">String</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从元素中读取数据</span></span><br><span class="line"><span class="comment">//    val stream: DataStream[Int] = env.fromElements(1, 2, 3, 4, 5, 6)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从集合中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> clicks = <span class="type">List</span>(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> stream2: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromCollection(clicks)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从文件中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> stream3: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\click.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印输出</span></span><br><span class="line">    stream.print(<span class="string">&quot;从元素中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream2.print(<span class="string">&quot;从集合中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream3.print(<span class="string">&quot;从文件中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2从Socket读取数据"><a href="#1-2从Socket读取数据" class="headerlink" title="1.2从Socket读取数据"></a>1.2从Socket读取数据</h3><ul><li><h6 id="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"><a href="#不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。" class="headerlink" title="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"></a>不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。</h6></li><li><h6 id="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"><a href="#我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。" class="headerlink" title="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"></a>我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。</h6></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>);</span><br></pre></td></tr></table></figure><h3 id="1-3从Kafka中读取数据"><a href="#1-3从Kafka中读取数据" class="headerlink" title="1.3从Kafka中读取数据"></a>1.3从Kafka中读取数据</h3><ul><li><h6 id="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"><a href="#Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。" class="headerlink" title="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"></a>Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。</h6></li><li><h6 id="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。"><a href="#所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。" class="headerlink" title="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。"></a>所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。</h6></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下："><a href="#代码如下：" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用Properties保存kafka连接的配置</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"></span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-从数据生成器读取数据"><a href="#1-4-从数据生成器读取数据" class="headerlink" title="1.4 从数据生成器读取数据"></a>1.4 从数据生成器读取数据</h3><h6 id="Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖："><a href="#Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖：" class="headerlink" title="Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖："></a>Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖：</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-datagen<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下：-1"><a href="#代码如下：-1" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DataGeneratorDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataGeneratorSource&lt;String&gt; dataGeneratorSource =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">DataGeneratorSource</span>&lt;&gt;(</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="keyword">public</span> String <span class="title function_">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                                <span class="keyword">return</span> <span class="string">&quot;Number:&quot;</span>+value;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        Long.MAX_VALUE,</span><br><span class="line">                        RateLimiterStrategy.perSecond(<span class="number">10</span>),</span><br><span class="line">                        Types.STRING</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class="string">&quot;datagenerator&quot;</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-5Flink支持的数据类型"><a href="#1-5Flink支持的数据类型" class="headerlink" title="1.5Flink支持的数据类型"></a>1.5Flink支持的数据类型</h3><ol><li><h5 id="Flink的类型系统"><a href="#Flink的类型系统" class="headerlink" title="Flink的类型系统"></a>Flink的类型系统</h5><p>Flink使用“类型信息”（TypeInformation）来统一表示数据类型。TypeInformation类是Flink中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。</p></li><li><h5 id="Flink支持的数据类型"><a href="#Flink支持的数据类型" class="headerlink" title="Flink支持的数据类型"></a>Flink支持的数据类型</h5><p>对于常见的Java和Scala数据类型，Flink都是支持的。Flink在内部，Flink对支持不同的类型进行了划分，这些类型可以在Types工具类中找到：</p></li></ol><ul><li><h5 id="（1）基本类型"><a href="#（1）基本类型" class="headerlink" title="（1）基本类型"></a>（1）基本类型</h5><p>所有Java基本类型及其包装类，再加上Void、String、Date、BigDecimal和BigInteger。</p></li><li><h5 id="（2）数组类型"><a href="#（2）数组类型" class="headerlink" title="（2）数组类型"></a>（2）数组类型</h5><p>包括基本类型数组（PRIMITIVE_ARRAY）和对象数组（OBJECT_ARRAY）。</p></li><li><h5 id="（3）复合数据类型"><a href="#（3）复合数据类型" class="headerlink" title="（3）复合数据类型"></a>（3）复合数据类型</h5><ol><li><p>Java元组类型（TUPLE）：这是Flink内置的元组类型，是Java API的一部分。最多25个字段，也就是从Tuple0~Tuple25，不支持空字段。</p></li><li><h5 id="Scala-样例类及Scala元组：不支持空字段。"><a href="#Scala-样例类及Scala元组：不支持空字段。" class="headerlink" title="Scala 样例类及Scala元组：不支持空字段。"></a>Scala 样例类及Scala元组：不支持空字段。</h5></li><li><h5 id="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"><a href="#行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。" class="headerlink" title="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"></a>行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。</h5></li><li><h5 id="POJO：Flink自定义的类似于Java-bean模式的类。"><a href="#POJO：Flink自定义的类似于Java-bean模式的类。" class="headerlink" title="POJO：Flink自定义的类似于Java bean模式的类。"></a>POJO：Flink自定义的类似于Java bean模式的类。</h5></li></ol></li></ul><h4 id="（4）辅助类型"><a href="#（4）辅助类型" class="headerlink" title="（4）辅助类型"></a>（4）辅助类型</h4><ul><li><h5 id="Option、Either、List、Map等。"><a href="#Option、Either、List、Map等。" class="headerlink" title="Option、Either、List、Map等。"></a>Option、Either、List、Map等。</h5></li></ul><h4 id="（5）泛型类型（GENERIC）"><a href="#（5）泛型类型（GENERIC）" class="headerlink" title="（5）泛型类型（GENERIC）"></a>（5）泛型类型（GENERIC）</h4><ul><li><h6 id="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"><a href="#Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。" class="headerlink" title="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"></a>Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。</h6></li><li><h6 id="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"><a href="#在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。" class="headerlink" title="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"></a>在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。</h6></li><li><h6 id="Flink对POJO类型的要求如下："><a href="#Flink对POJO类型的要求如下：" class="headerlink" title="Flink对POJO类型的要求如下："></a>Flink对POJO类型的要求如下：</h6><ol><li><h6 id="类是公有（public）的"><a href="#类是公有（public）的" class="headerlink" title="类是公有（public）的"></a>类是公有（public）的</h6></li><li><h6 id="有一个无参的构造方法"><a href="#有一个无参的构造方法" class="headerlink" title="有一个无参的构造方法"></a>有一个无参的构造方法</h6></li><li><h6 id="所有属性都是公有（public）的"><a href="#所有属性都是公有（public）的" class="headerlink" title="所有属性都是公有（public）的"></a>所有属性都是公有（public）的</h6></li><li><h6 id="所有属性的类型都是可以序列化的"><a href="#所有属性的类型都是可以序列化的" class="headerlink" title="所有属性的类型都是可以序列化的"></a>所有属性的类型都是可以序列化的</h6></li></ol></li></ul><h4 id="3）类型提示（Type-Hints）"><a href="#3）类型提示（Type-Hints）" class="headerlink" title="3）类型提示（Type Hints）"></a>3）类型提示（Type Hints）</h4><h6 id="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"><a href="#Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。" class="headerlink" title="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"></a>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</h6><h6 id="为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。"><a href="#为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。" class="headerlink" title="为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。"></a>为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。</h6><h6 id="回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"><a href="#回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。" class="headerlink" title="回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"></a>回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.map(word -&gt; <span class="type">Tuple2</span>.of(word, <span class="number">1</span>L))</span><br><span class="line">.returns(<span class="type">Types</span>.<span class="type">TUPLE</span>(<span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>));</span><br></pre></td></tr></table></figure><h6 id="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。"><a href="#Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。" class="headerlink" title="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。"></a>Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">returns(<span class="keyword">new</span> <span class="type">TypeHint</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">SomeType</span>&gt;&gt;()&#123;&#125;)</span><br></pre></td></tr></table></figure><h3 id="转换算子（Transformation）"><a href="#转换算子（Transformation）" class="headerlink" title="转换算子（Transformation）"></a>转换算子（Transformation）</h3><h5 id="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"><a href="#数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。" class="headerlink" title="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"></a>数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。</h5><p><img src="https://pic1.imgdb.cn/item/678624a7d0e0a243d4f42b7c.png"></p><h3 id="1-基本转换算子（map-x2F-filter-x2F-flatMap）"><a href="#1-基本转换算子（map-x2F-filter-x2F-flatMap）" class="headerlink" title="1.基本转换算子（map&#x2F; filter&#x2F; flatMap）"></a>1.基本转换算子（map&#x2F; filter&#x2F; flatMap）</h3><h4 id="1-1-1映射（map）"><a href="#1-1-1映射（map）" class="headerlink" title="1.1.1映射（map）"></a>1.1.1映射（map）</h4><h6 id="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"><a href="#map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。" class="headerlink" title="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"></a>map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。</h6><p><img src="https://pic1.imgdb.cn/item/6786250bd0e0a243d4f42bac.png"></p><h6 id="我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"><a href="#我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。" class="headerlink" title="我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"></a>我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。</h6><h6 id="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"><a href="#下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。" class="headerlink" title="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"></a>下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">MapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提取每次点击事件的用户名</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.map( _.user ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现mapFunction接口</span></span><br><span class="line">    stream.map(<span class="keyword">new</span> <span class="type">UserExtractor</span>).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserExtractor</span> <span class="keyword">extends</span> <span class="title">MapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"><a href="#上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。" class="headerlink" title="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"></a>上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。</h6><h3 id="1-1-2过滤（filter）"><a href="#1-1-2过滤（filter）" class="headerlink" title="1.1.2过滤（filter）"></a>1.1.2过滤（filter）</h3><h5 id="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"><a href="#filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。" class="headerlink" title="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"></a>filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。</h5><p><img src="https://pic1.imgdb.cn/item/6786255dd0e0a243d4f42bd8.png"></p><h6 id="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。"><a href="#进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。" class="headerlink" title="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。"></a>进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。</h6><h6 id="案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。"><a href="#案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。" class="headerlink" title="案例需求：下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。"></a><strong>案例需求：</strong>下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFilterTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过滤出用户为Marry的所有点击事件</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.filter( _.user == <span class="string">&quot;hkj&quot;</span> ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现FilterFunction</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">UserFilter</span> ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserFilter</span> <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.user == <span class="string">&quot;Bob&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-1-3-扁平映射（flatMap）"><a href="#1-1-3-扁平映射（flatMap）" class="headerlink" title="1.1.3 扁平映射（flatMap）"></a>1.1.3 扁平映射（flatMap）</h4><h6 id="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"><a href="#flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。" class="headerlink" title="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"></a>flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。</h6><p><img src="https://pic1.imgdb.cn/item/6786259fd0e0a243d4f42bf4.png"></p><h6 id="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"><a href="#同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。" class="headerlink" title="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"></a>同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。</h6><h6 id="案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。"><a href="#案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。" class="headerlink" title="案例需求：如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。"></a><strong>案例需求：</strong>如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。</h6><h6 id="实现代码如下："><a href="#实现代码如下：" class="headerlink" title="实现代码如下："></a>实现代码如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FlatMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFlatMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试灵活输出形式</span></span><br><span class="line">    stream.flatMap(<span class="keyword">new</span> <span class="type">MyFlatMap</span>).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// lam表达式</span></span><br><span class="line">    <span class="keyword">val</span> stream1 = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line">    stream1.flatMap( value =&gt; value.split(<span class="string">&quot;,&quot;</span>) ).map(value =&gt; (value, <span class="number">1</span>)).print(<span class="string">&quot;stream1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义实现flatMapFunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">FlatMapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: <span class="type">Event</span>, out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 如果当前数据是hkj的点击事件那就直接输出user</span></span><br><span class="line">      <span class="keyword">if</span> (value.user == <span class="string">&quot;hkj&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 如果当前数据是Bob的点击事件，那么就输出user和url</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (value.user == <span class="string">&quot;Bob&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">        out.collect(value.url)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2聚合算子（Aggregation）"><a href="#1-2聚合算子（Aggregation）" class="headerlink" title="1.2聚合算子（Aggregation）"></a>1.2聚合算子（Aggregation）</h3><h5 id="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"><a href="#计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。" class="headerlink" title="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"></a>计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。</h5><h4 id="1-2-1-按键分区（keyBy）"><a href="#1-2-1-按键分区（keyBy）" class="headerlink" title="1.2.1 按键分区（keyBy）"></a>1.2.1 按键分区（keyBy）</h4><ul><li><h6 id="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"><a href="#对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。" class="headerlink" title="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"></a>对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。</h6></li><li><h6 id="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"><a href="#keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。" class="headerlink" title="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"></a>keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。</h6></li><li><h6 id="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"><a href="#基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。" class="headerlink" title="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"></a>基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。</h6></li></ul><p><img src="https://pic1.imgdb.cn/item/67862611d0e0a243d4f42c2f.png"></p><h6 id="在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。"><a href="#在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。" class="headerlink" title="在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。"></a>在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。</h6><h6 id="keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"><a href="#keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。" class="headerlink" title="keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"></a>keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。</h6><h6 id="我们可以以id作为key做一个分区操作，代码实现如下："><a href="#我们可以以id作为key做一个分区操作，代码实现如下：" class="headerlink" title="我们可以以id作为key做一个分区操作，代码实现如下："></a>我们可以以id作为key做一个分区操作，代码实现如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.<span class="type">KeySelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformKeyByTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.keyBy( <span class="keyword">new</span> <span class="type">MyKeySelector</span> )</span><br><span class="line">      .maxBy(<span class="string">&quot;timestamp&quot;</span>)</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//    stream.keyBy( _.user).print(&quot;lam&quot;)</span></span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyKeySelector</span> <span class="keyword">extends</span> <span class="title">KeySelector</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKey</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"><a href="#需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。" class="headerlink" title="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"></a>需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。</h6><h6 id="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"><a href="#KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。" class="headerlink" title="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"></a>KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。</h6><h4 id="1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）"><a href="#1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）" class="headerlink" title="1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）"></a>1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）</h4><h6 id="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："><a href="#有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：" class="headerlink" title="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："></a>有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：</h6><ol><li><h6 id="sum-：在输入流上，对指定的字段做叠加求和的操作。"><a href="#sum-：在输入流上，对指定的字段做叠加求和的操作。" class="headerlink" title="sum()：在输入流上，对指定的字段做叠加求和的操作。"></a>sum()：在输入流上，对指定的字段做叠加求和的操作。</h6></li><li><h6 id="min-：在输入流上，对指定的字段求最小值。"><a href="#min-：在输入流上，对指定的字段求最小值。" class="headerlink" title="min()：在输入流上，对指定的字段求最小值。"></a>min()：在输入流上，对指定的字段求最小值。</h6></li><li><h6 id="max-：在输入流上，对指定的字段求最大值。"><a href="#max-：在输入流上，对指定的字段求最大值。" class="headerlink" title="max()：在输入流上，对指定的字段求最大值。"></a>max()：在输入流上，对指定的字段求最大值。</h6></li><li><h6 id="minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。"><a href="#minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。" class="headerlink" title="minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。"></a>minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。</h6></li><li><h6 id="maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。"><a href="#maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。" class="headerlink" title="maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。"></a>maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。</h6></li></ol><h5 id="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"><a href="#简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。" class="headerlink" title="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"></a>简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。</h5><h6 id="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"><a href="#对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。" class="headerlink" title="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"></a>对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。</h6><h6 id="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"><a href="#如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。" class="headerlink" title="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"></a>如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransAggregation</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_2&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_3&quot;</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        stream.keyBy(e -&gt; e.id).max(<span class="string">&quot;vc&quot;</span>);    <span class="comment">// 指定字段名称</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"><a href="#简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。" class="headerlink" title="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"></a>简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。</h5><h5 id="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"><a href="#一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。" class="headerlink" title="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"></a>一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。</h5><h4 id="1-2-3-归约聚合（reduce）"><a href="#1-2-3-归约聚合（reduce）" class="headerlink" title="1.2.3 归约聚合（reduce）"></a>1.2.3 归约聚合（reduce）</h4><h5 id="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"><a href="#reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。" class="headerlink" title="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"></a>reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。</h5><h5 id="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"><a href="#reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。" class="headerlink" title="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"></a>reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。</h5><h5 id="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："><a href="#调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：" class="headerlink" title="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："></a>调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ReduceFunction</span>&lt;T&gt; <span class="keyword">extends</span> <span class="title class_">Function</span>, Serializable &#123;</span><br><span class="line">    T <span class="title function_">reduce</span><span class="params">(T value1, T value2)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"><a href="#ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。" class="headerlink" title="ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"></a>ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。</h5><h5 id="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"><a href="#我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。" class="headerlink" title="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"></a>我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。</h5><h5 id="为了方便后续使用，定义一个WaterSensorMapFunction："><a href="#为了方便后续使用，定义一个WaterSensorMapFunction：" class="headerlink" title="为了方便后续使用，定义一个WaterSensorMapFunction："></a>为了方便后续使用，定义一个WaterSensorMapFunction：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensorMapFunction</span> <span class="keyword">implements</span> <span class="title class_">MapFunction</span>&lt;String,WaterSensor&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> WaterSensor <span class="title function_">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] datas = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WaterSensor</span>(datas[<span class="number">0</span>],Long.valueOf(datas[<span class="number">1</span>]) ,Integer.valueOf(datas[<span class="number">2</span>]) );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="案例：使用reduce实现max和maxBy的功能。"><a href="#案例：使用reduce实现max和maxBy的功能。" class="headerlink" title="案例：使用reduce实现max和maxBy的功能。"></a>案例：使用reduce实现max和maxBy的功能。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">ReduceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformReduceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduce归约聚合,提取当前最活跃用户</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    data =&gt; true 的作用</span></span><br><span class="line"><span class="comment">//    1.keyBy 的作用：keyBy 是 Flink 中的一个算子，用于将数据流按照指定的键进行分组。</span></span><br><span class="line"><span class="comment">//                  分组后，相同键的数据会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    2.data =&gt; true 的含义：</span></span><br><span class="line"><span class="comment">//              这里的 data =&gt; true 是一个匿名函数，它对每条数据返回固定的 true 值。</span></span><br><span class="line"><span class="comment">//              由于所有数据的键都是 true，因此所有数据都会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    3.为什么需要 data =&gt; true：</span></span><br><span class="line"><span class="comment">//          在统计最活跃用户时，需要将所有用户的活跃度数据集中到一个分区中，才能进行比较和筛选。</span></span><br><span class="line"><span class="comment">//          如果不使用 keyBy(data =&gt; true)，数据会分散在多个分区中，无法直接进行比较。</span></span><br><span class="line"></span><br><span class="line">    stream.map( data =&gt; (data.user, <span class="number">1</span>L) )</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .reduce( <span class="keyword">new</span> <span class="type">MySum</span> )  <span class="comment">// 统计每个用户的活跃度</span></span><br><span class="line">      .keyBy(data =&gt; <span class="literal">true</span>)  <span class="comment">// 将所有数据按照同样的key分到同一个组中</span></span><br><span class="line">      .reduce( (state, data) =&gt; <span class="keyword">if</span> (data._2 &gt;= state._2) data <span class="keyword">else</span> state ) <span class="comment">// 选取当前最活跃的用户</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MySum</span> <span class="keyword">extends</span> <span class="title">ReduceFunction</span>[(<span class="type">String</span>, <span class="type">Long</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(value1: (<span class="type">String</span>, <span class="type">Long</span>), value2: (<span class="type">String</span>, <span class="type">Long</span>)): (<span class="type">String</span>, <span class="type">Long</span>) = (value1._1, value2._2 + value1._2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"><a href="#reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。" class="headerlink" title="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"></a>reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。</h5><h3 id="1-3-3-用户自定义函数（UDF）"><a href="#1-3-3-用户自定义函数（UDF）" class="headerlink" title="1.3.3 用户自定义函数（UDF）"></a>1.3.3 用户自定义函数（UDF）</h3><h5 id="用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"><a href="#用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。" class="headerlink" title="用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"></a>用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。</h5><h5 id="用户自定义函数分为：函数类、匿名函数、富函数类。"><a href="#用户自定义函数分为：函数类、匿名函数、富函数类。" class="headerlink" title="用户自定义函数分为：函数类、匿名函数、富函数类。"></a>用户自定义函数分为：函数类、匿名函数、富函数类。</h5><h4 id="需求：用来从用户的点击数据中筛选包含“sensor-1”的内容："><a href="#需求：用来从用户的点击数据中筛选包含“sensor-1”的内容：" class="headerlink" title="需求：用来从用户的点击数据中筛选包含“sensor_1”的内容："></a><strong>需求：</strong>用来从用户的点击数据中筛选包含“sensor_1”的内容：</h4><h4 id="方式一：实现FilterFunction接口"><a href="#方式一：实现FilterFunction接口" class="headerlink" title="方式一：实现FilterFunction接口"></a><strong>方式一：</strong>实现FilterFunction接口</h4><h4 id="方式二：通过匿名类来实现FilterFunction接口："><a href="#方式二：通过匿名类来实现FilterFunction接口：" class="headerlink" title="方式二：通过匿名类来实现FilterFunction接口："></a><strong>方式二：</strong>通过匿名类来实现FilterFunction接口：</h4><h5 id="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"><a href="#方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。" class="headerlink" title="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"></a><strong>方式二的优化：</strong>为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。</h5><h5 id="方式三：采用匿名函数（Lambda）"><a href="#方式三：采用匿名函数（Lambda）" class="headerlink" title="方式三：采用匿名函数（Lambda）"></a><strong>方式三：</strong>采用匿名函数（Lambda）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformUDFTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试UDF的用法,筛选url中包含某个关键字hkjcpdd的Event事件</span></span><br><span class="line">    <span class="comment">// 1.实现一个自定义的函数类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">MyFilterFunction</span>(<span class="string">&quot;hkjcpdd&quot;</span>) ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 使用匿名类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">FilterFunction</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(<span class="string">&quot;hkjmjj&quot;</span>)</span><br><span class="line">    &#125; ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 使用lambda表达式</span></span><br><span class="line">    stream.filter( _.url.contains(<span class="string">&quot;hkjmjj&quot;</span>) ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义个filterfunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFilterFunction</span>(<span class="params">keyWord: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// contains是指是否包含某些字段</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(keyWord)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-2-富函数类（Rich-Function-Classes）"><a href="#1-3-2-富函数类（Rich-Function-Classes）" class="headerlink" title="1.3.2 富函数类（Rich Function Classes）"></a>1.3.2 富函数类（Rich Function Classes）</h4><h6 id="“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"><a href="#“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。" class="headerlink" title="“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"></a>“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。</h6><h6 id="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"><a href="#与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。" class="headerlink" title="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"></a>与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</h6><h6 id="Rich-Function有生命周期的概念。典型的生命周期方法有："><a href="#Rich-Function有生命周期的概念。典型的生命周期方法有：" class="headerlink" title="Rich Function有生命周期的概念。典型的生命周期方法有："></a>Rich Function有生命周期的概念。典型的生命周期方法有：</h6><ul><li><h6 id="open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。"><a href="#open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。" class="headerlink" title="open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。"></a>open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。</h6></li><li><h6 id="close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"><a href="#close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。" class="headerlink" title="close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"></a>close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。</h6></li></ul><h6 id="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。"><a href="#需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。" class="headerlink" title="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。"></a>需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。</h6><h6 id="来看一个例子说明："><a href="#来看一个例子说明：" class="headerlink" title="来看一个例子说明："></a>来看一个例子说明：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformRichFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义一个RichMapFunction，测试复函数类的功能</span></span><br><span class="line">    stream.map( <span class="keyword">new</span> <span class="type">MyRichMap</span>() ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRichMap</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichMapFunction</span>[<span class="type">Event</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务开始&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">Long</span> = value.timestamp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务结束&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-3-4-物理分区算子（Physical-Partitioning）"><a href="#5-3-4-物理分区算子（Physical-Partitioning）" class="headerlink" title="5.3.4 物理分区算子（Physical Partitioning）"></a>5.3.4 物理分区算子（Physical Partitioning）</h3><h5 id="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"><a href="#常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。" class="headerlink" title="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"></a>常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。</h5><h3 id="1-4-1-随机分区（shuffle）"><a href="#1-4-1-随机分区（shuffle）" class="headerlink" title="1.4.1 随机分区（shuffle）"></a>1.4.1 随机分区（shuffle）</h3><h5 id="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。"><a href="#最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。" class="headerlink" title="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。"></a>最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。</h5><h5 id="随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。"><a href="#随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。" class="headerlink" title="随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。"></a>随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。</h5><p><img src="https://pic1.imgdb.cn/item/67862896d0e0a243d4f42d1b.png"></p><h5 id="经过随机分区之后，得到的依然是一个DataStream。"><a href="#经过随机分区之后，得到的依然是一个DataStream。" class="headerlink" title="经过随机分区之后，得到的依然是一个DataStream。"></a>经过随机分区之后，得到的依然是一个DataStream。</h5><h5 id="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"><a href="#我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。" class="headerlink" title="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"></a>我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShuffleExample</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"> env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; stream = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>);;</span><br><span class="line"></span><br><span class="line">        stream.shuffle().print()</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-4-2-轮询分区（Round-Robin）"><a href="#1-4-2-轮询分区（Round-Robin）" class="headerlink" title="1.4.2 轮询分区（Round-Robin）"></a>1.4.2 轮询分区（Round-Robin）</h4><h5 id="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"><a href="#轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。" class="headerlink" title="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"></a>轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。</h5><p><img src="https://pic1.imgdb.cn/item/678628c7d0e0a243d4f42d3e.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rebalance()</span><br></pre></td></tr></table></figure><h4 id="1-4-3-重缩放分区（rescale）"><a href="#1-4-3-重缩放分区（rescale）" class="headerlink" title="1.4.3 重缩放分区（rescale）"></a>1.4.3 重缩放分区（rescale）</h4><h5 id="重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"><a href="#重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。" class="headerlink" title="重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"></a>重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。</h5><p><img src="https://pic1.imgdb.cn/item/678628e9d0e0a243d4f42d59.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rescale()</span><br></pre></td></tr></table></figure><h4 id="1-4-4-广播（broadcast）"><a href="#1-4-4-广播（broadcast）" class="headerlink" title="1.4.4 广播（broadcast）"></a>1.4.4 广播（broadcast）</h4><h5 id="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。"><a href="#这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。" class="headerlink" title="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。"></a>这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.broadcast()</span><br></pre></td></tr></table></figure><h4 id="1-4-5-全局分区（global）"><a href="#1-4-5-全局分区（global）" class="headerlink" title="1.4.5 全局分区（global）"></a>1.4.5 全局分区（global）</h4><h5 id="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"><a href="#全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。" class="headerlink" title="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"></a>全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.global()</span><br></pre></td></tr></table></figure><h4 id="1-4-6-自定义分区（Custom）"><a href="#1-4-6-自定义分区（Custom）" class="headerlink" title="1.4.6 自定义分区（Custom）"></a>1.4.6 自定义分区（Custom）</h4><h5 id="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。"><a href="#当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。" class="headerlink" title="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。"></a>当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。</h5><h4 id="1-自定义分区器"><a href="#1-自定义分区器" class="headerlink" title="1)自定义分区器"></a>1)自定义分区器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">ParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Calendar</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//SourceFunction是并行度1的</span></span><br><span class="line"><span class="comment">//ParallelSourceFunction就是并行多的</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClickSource</span> <span class="keyword">extends</span> <span class="title">ParallelSourceFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 标志位</span></span><br><span class="line">  <span class="keyword">var</span> running = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Event</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 随机数生成器</span></span><br><span class="line">    <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">    <span class="comment">// 定义数据随机选择的范围</span></span><br><span class="line">    <span class="keyword">val</span> users = <span class="type">Array</span>(<span class="string">&quot;Marry&quot;</span>, <span class="string">&quot;Hkj&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Cary&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> urls = <span class="type">Array</span>(<span class="string">&quot;./home&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="string">&quot;./fav&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>,<span class="string">&quot;./prod?id=3&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键流程， 用标志位作为循环的判断条件，不停的发出数据</span></span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line">      <span class="keyword">val</span> event = <span class="type">Event</span>(</span><br><span class="line">        users(random.nextInt(users.length)),</span><br><span class="line">        urls(random.nextInt(users.length)),</span><br><span class="line">        <span class="type">Calendar</span>.getInstance.getTimeInMillis</span><br><span class="line">      )</span><br><span class="line"><span class="comment">//      // 为要发送的数据分配时间戳</span></span><br><span class="line"><span class="comment">//      ctx.collectWithTimestamp(event, event.timestamp)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//      // 向下游直接发送水位线</span></span><br><span class="line"><span class="comment">//      ctx.emitWatermark(new Watermark(event.timestamp - 1L))</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 调用ctx的方法向下游发送数据</span></span><br><span class="line">      ctx.collect(event)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 每隔一秒发送过一条数据</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2）使用自定义分区器"><a href="#2）使用自定义分区器" class="headerlink" title="2）使用自定义分区器"></a>2）使用自定义分区器</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PartitionReblanceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取自定义的数据流</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">ClickSource</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 轮询重分区之后打印输出</span></span><br><span class="line">    stream.rebalance.print(<span class="string">&quot;shuffle&quot;</span>).setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-5分流"><a href="#1-3-5分流" class="headerlink" title="1.3.5分流"></a>1.3.5分流</h3><h4 id="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"><a href="#所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。" class="headerlink" title="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"></a>所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。</h4><p><img src="https://pic1.imgdb.cn/item/6786298ad0e0a243d4f42daf.png"></p><h4 id="1-3-5-1-简单实现"><a href="#1-3-5-1-简单实现" class="headerlink" title="1.3.5.1 简单实现"></a>1.3.5.1 简单实现</h4><p>其实根据条件筛选数据的需求，本身非常容易实现：只要针对同一条流多次独立调用.filter()方法进行筛选，就可以得到拆分之后的流了。</p><p><strong>案例需求：</strong>读取一个整数数字流，将数据流划分为奇数流和偶数流。</p><p><strong>代码实现：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByFilter</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">      </span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                                                           .map(Integer::valueOf);</span><br><span class="line">        <span class="comment">//将ds 分为两个流 ，一个是奇数流，一个是偶数流</span></span><br><span class="line">        <span class="comment">//使用filter 过滤两次</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds1 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">0</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds2 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;偶数&quot;</span>);</span><br><span class="line">        ds2.print(<span class="string">&quot;奇数&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"><a href="#这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？" class="headerlink" title="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"></a>这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？</h5><h4 id="1-3-5-2-使用侧输出流"><a href="#1-3-5-2-使用侧输出流" class="headerlink" title="1.3.5.2 使用侧输出流"></a>1.3.5.2 使用侧输出流</h4><p>关于处理函数中侧输出流的用法，我们已经在7.5节做了详细介绍。简单来说，只需要调用上下文ctx的.output()方法，就可以输出任意类型的数据了。而侧输出流的标记和提取，都离不开一个“输出标签”（OutputTag），指定了侧输出流的id和类型。</p><p><strong>代码实现：</strong>将WaterSensor按照Id类型进行分流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByOutputTag</span> &#123;    </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">              .map(<span class="keyword">new</span> <span class="title class_">WaterSensorMapFunction</span>());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s1 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s1&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s2 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s2&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">       <span class="comment">//返回的都是主流</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds1 = ds.process(<span class="keyword">new</span> <span class="title class_">ProcessFunction</span>&lt;WaterSensor, WaterSensor&gt;()</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="string">&quot;s1&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s1, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;s2&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s2, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">//主流</span></span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;主流，非s1,s2的传感器&quot;</span>);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s1DS = ds1.getSideOutput(s1);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s2DS = ds1.getSideOutput(s2);</span><br><span class="line"></span><br><span class="line">        s1DS.printToErr(<span class="string">&quot;s1&quot;</span>);</span><br><span class="line">        s2DS.printToErr(<span class="string">&quot;s2&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-6-基本合流操作"><a href="#1-3-6-基本合流操作" class="headerlink" title="1.3.6 基本合流操作"></a><strong>1.3.6</strong> <strong>基本合流操作</strong></h3><h5 id="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"><a href="#在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。" class="headerlink" title="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"></a>在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。</h5><h3 id="1-3-6-1"><a href="#1-3-6-1" class="headerlink" title="1.3.6.1"></a>1.3.6.1</h3><h4 id="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"><a href="#最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。" class="headerlink" title="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"></a>最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。</h4><p><img src="https://pic1.imgdb.cn/item/6786448fd0e0a243d4f434cf.png"></p><h5 id="在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："><a href="#在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：" class="headerlink" title="在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："></a>在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream1.union(stream2, stream3, ...)</span><br></pre></td></tr></table></figure><h4 id="注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"><a href="#注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。" class="headerlink" title="注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"></a>注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。</h4><p><strong>代码实现：</strong>我们可以用下面的代码做一个简单测试：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UnionExample</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds1 = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds2 = env.fromElements(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; ds3 = env.fromElements(<span class="string">&quot;2&quot;</span>, <span class="string">&quot;2&quot;</span>, <span class="string">&quot;3&quot;</span>);</span><br><span class="line"></span><br><span class="line">        ds1.union(ds2,ds3.map(Integer::valueOf))</span><br><span class="line">           .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-6-2-连接（Connect）"><a href="#1-3-6-2-连接（Connect）" class="headerlink" title="1.3.6.2 连接（Connect）"></a>1.3.6.2 连接（Connect）</h4><h4 id="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"><a href="#流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。" class="headerlink" title="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"></a>流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。</h4><h5 id="1）连接流（ConnectedStreams）"><a href="#1）连接流（ConnectedStreams）" class="headerlink" title="1）连接流（ConnectedStreams）"></a>1）连接流（ConnectedStreams）</h5><p><img src="https://pic1.imgdb.cn/item/67864547d0e0a243d4f4350b.png"></p><p><strong>代码实现：</strong>需要分为两步：首先基于一条DataStream调用.connect()方法，传入另外一条DataStream作为参数，将两条流连接起来，得到一个ConnectedStreams；然后再调用同处理方法得到DataStream。这里可以的调用的同处理方法有.map()&#x2F;.flatMap()，以及.process()方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;Integer&gt; source1 = env.fromElements(1, 2, 3);</span></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;String&gt; source2 = env.fromElements(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);</span></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; source1 = env</span><br><span class="line">                .socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(i -&gt; Integer.parseInt(i));</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source2 = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">8888</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 使用 connect 合流</span></span><br><span class="line"><span class="comment">         * 1、一次只能连接 2条流</span></span><br><span class="line"><span class="comment">         * 2、流的数据类型可以不一样</span></span><br><span class="line"><span class="comment">         * 3、 连接后可以调用 map、flatmap、process来处理，但是各处理各的</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ConnectedStreams&lt;Integer, String&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connect.map(<span class="keyword">new</span> <span class="title class_">CoMapFunction</span>&lt;Integer, String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map1</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于数字流:&quot;</span> + value.toString();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于字母流:&quot;</span> + value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码中，ConnectedStreams有两个类型参数，分别表示内部包含的两条流各自的数据类型；由于需要“一国两制”，因此调用.map()方法时传入的不再是一个简单的MapFunction，而是一个CoMapFunction，表示分别对两条流中的数据执行map操作。这个接口有三个类型参数，依次表示第一条流、第二条流，以及合并后的流中的数据类型。需要实现的方法也非常直白：.map1()就是对第一条流中数据的map操作，.map2()则是针对第二条流。</p><p>2）CoProcessFunction</p><p>与CoMapFunction类似，如果是调用.map()就需要传入一个CoMapFunction，需要实现map1()、map2()两个方法；而调用.process()时，传入的则是一个CoProcessFunction。它也是“处理函数”家族中的一员，用法非常相似。它需要实现的就是processElement1()、processElement2()两个方法，在每个数据到来时，会根据来源的流调用其中的一个方法进行处理。</p><p>值得一提的是，ConnectedStreams也可以直接调用.keyBy()进行按键分区的操作，得到的还是一个ConnectedStreams：</p><p>connectedStreams.keyBy(keySelector1, keySelector2);</p><p>这里传入两个参数keySelector1和keySelector2，是两条流中各自的键选择器；当然也可以直接传入键的位置值（keyPosition），或者键的字段名（field），这与普通的keyBy用法完全一致。ConnectedStreams进行keyBy操作，其实就是把两条流中key相同的数据放到了一起，然后针对来源的流再做各自处理，这在一些场景下非常有用。</p><p>案例需求：连接两条流，输出能根据id匹配上的数据（类似inner join效果）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectKeybyDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Tuple2&lt;Integer, String&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a1&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a2&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">2</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa1&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa2&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="string">&quot;bb&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="string">&quot;cc&quot;</span>, <span class="number">1</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 多并行度下，需要根据 关联条件 进行keyby，才能保证key相同的数据到一起去，才能匹配上</span></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connectKey = connect.keyBy(s1 -&gt; s1.f0, s2 -&gt; s2.f0);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connectKey.process(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">CoProcessFunction</span>&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 定义 HashMap，缓存来过的数据，key=id，value=list&lt;数据&gt;</span></span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple2&lt;Integer, String&gt;&gt;&gt; s1Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt;&gt; s2Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement1</span><span class="params">(Tuple2&lt;Integer, String&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s1数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple2&lt;Integer, String&gt;&gt; s1Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s1Values.add(value);</span><br><span class="line">                            s1Cache.put(id, s1Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s1Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s2的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple3&lt;Integer, String, Integer&gt; s2Element : s2Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + value + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + s2Element);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement2</span><span class="params">(Tuple3&lt;Integer, String, Integer&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s2数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; s2Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s2Values.add(value);</span><br><span class="line">                            s2Cache.put(id, s2Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s2Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s1的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple2&lt;Integer, String&gt; s1Element : s1Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + s1Element + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + value);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4输出算子"><a href="#1-4输出算子" class="headerlink" title="1.4输出算子"></a>1.4输出算子</h3><h4 id="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"><a href="#Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。" class="headerlink" title="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"></a>Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。</h4><p><img src="https://pic1.imgdb.cn/item/678645c8d0e0a243d4f43592.png"></p><h3 id="1-4-1-连接到外部系统"><a href="#1-4-1-连接到外部系统" class="headerlink" title="1.4.1 连接到外部系统"></a><strong>1.4.1</strong> <strong>连接到外部系统</strong></h3><h4 id="Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"><a href="#Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。" class="headerlink" title="Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"></a>Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。</h4><h5 id="Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。"><a href="#Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。" class="headerlink" title="Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。"></a>Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。</h5><h5 id="stream-addSink-new-SinkFunction-…"><a href="#stream-addSink-new-SinkFunction-…" class="headerlink" title="stream.addSink(new SinkFunction(…));"></a>stream.addSink(new SinkFunction(…));</h5><h5 id="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"><a href="#addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。" class="headerlink" title="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"></a>addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。</h5><h5 id="Flink1-12开始，同样重构了Sink架构，"><a href="#Flink1-12开始，同样重构了Sink架构，" class="headerlink" title="Flink1.12开始，同样重构了Sink架构，"></a>Flink1.12开始，同样重构了Sink架构，</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.sinkTo(…)</span><br></pre></td></tr></table></figure><h4 id="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："><a href="#当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：" class="headerlink" title="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："></a>当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：</h4><p><img src="https://pic1.imgdb.cn/item/678645f5d0e0a243d4f435e2.png"></p><h4 id="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"><a href="#我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。" class="headerlink" title="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"></a>我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。</h4><h4 id="除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"><a href="#除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。" class="headerlink" title="除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"></a>除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。</h4><p><img src="https://pic1.imgdb.cn/item/6786460cd0e0a243d4f43614.png"></p><h4 id="除此以外，就需要用户自定义实现sink连接器了。"><a href="#除此以外，就需要用户自定义实现sink连接器了。" class="headerlink" title="除此以外，就需要用户自定义实现sink连接器了。"></a>除此以外，就需要用户自定义实现sink连接器了。</h4><h3 id="5-4-2-输出到文件"><a href="#5-4-2-输出到文件" class="headerlink" title="5.4.2 输出到文件"></a><strong>5.4.2</strong> <strong>输出到文件</strong></h3><h4 id="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"><a href="#Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。" class="headerlink" title="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"></a>Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。</h4><h4 id="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："><a href="#FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：" class="headerlink" title="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："></a>FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：</h4><ul><li>行编码： FileSink.forRowFormat（basePath，rowEncoder）。</li><li>批量编码： FileSink.forBulkFormat（basePath，bulkWriterFactory）。</li></ul><h4 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ch.qos.logback.core.util.<span class="type">TimeUtil</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringEncoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.<span class="type">Path</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.<span class="type">StreamingFileSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToFileTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 直接以文本形式分布式的写入到文件中</span></span><br><span class="line">    <span class="comment">// SimpleStringEncoder作用是将String转成char方便写入</span></span><br><span class="line">    <span class="keyword">val</span> fileSink = <span class="type">StreamingFileSink</span></span><br><span class="line">      .forRowFormat(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;./output&quot;</span>), <span class="keyword">new</span> <span class="type">SimpleStringEncoder</span>[<span class="type">String</span>](<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    stream.broadcast.map(_.toString).addSink(fileSink)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-3-输出到Kafka"><a href="#1-4-3-输出到Kafka" class="headerlink" title="1.4.3 输出到Kafka"></a><strong>1.4.3</strong> 输出到Kafka</h3><h3 id="（1）添加Kafka-连接器依赖"><a href="#（1）添加Kafka-连接器依赖" class="headerlink" title="（1）添加Kafka 连接器依赖"></a>（1）添加Kafka 连接器依赖</h3><h3 id="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"><a href="#由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。" class="headerlink" title="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"></a>由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。</h3><h3 id="（2）启动Kafka集群"><a href="#（2）启动Kafka集群" class="headerlink" title="（2）启动Kafka集群"></a>（2）启动Kafka集群</h3><h3 id="（3）编写输出到Kafka的示例代码"><a href="#（3）编写输出到Kafka的示例代码" class="headerlink" title="（3）编写输出到Kafka的示例代码"></a>（3）编写输出到Kafka的示例代码</h3><h3 id="输出无key的record"><a href="#输出无key的record" class="headerlink" title="输出无key的record:"></a>输出无key的record:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.&#123;<span class="type">FlinkKafkaConsumer</span>, <span class="type">FlinkKafkaProducer</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;lhxcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="comment">// trim就是去空格</span></span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong).toString</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 讲数据写入到kafka</span></span><br><span class="line">    stream.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](<span class="string">&quot;master:9092&quot;</span>, <span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="然后开一个消费者查看是否到数据"><a href="#然后开一个消费者查看是否到数据" class="headerlink" title="然后开一个消费者查看是否到数据"></a>然后开一个消费者查看是否到数据</h3><h3 id="1-4-4-输出到MySQL（JDBC）"><a href="#1-4-4-输出到MySQL（JDBC）" class="headerlink" title="1.4.4 输出到MySQL（JDBC）"></a>1.4.4 输出到MySQL（JDBC）</h3><h3 id="写入数据的MySQL的测试步骤如下。"><a href="#写入数据的MySQL的测试步骤如下。" class="headerlink" title="写入数据的MySQL的测试步骤如下。"></a>写入数据的MySQL的测试步骤如下。</h3><h3 id="（1）添加依赖"><a href="#（1）添加依赖" class="headerlink" title="（1）添加依赖"></a>（1）添加依赖</h3><h3 id="添加MySQL驱动："><a href="#添加MySQL驱动：" class="headerlink" title="添加MySQL驱动："></a>添加MySQL驱动：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径："><a href="#官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径：" class="headerlink" title="官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径："></a>官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>apache-snapshots<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>apache snapshots<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.apache.org/content/repositories/snapshots/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="添加依赖："><a href="#添加依赖：" class="headerlink" title="添加依赖："></a>添加依赖：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.17-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："><a href="#如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：" class="headerlink" title="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："></a>如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">            &lt;id&gt;aliyunmaven&lt;/id&gt;</span><br><span class="line">            &lt;mirrorOf&gt;*,!apache-snapshots&lt;/mirrorOf&gt;</span><br><span class="line">            &lt;name&gt;阿里云公共仓库&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;https:<span class="comment">//maven.aliyun.com/repository/public&lt;/url&gt;</span></span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）启动MySQL，在test库下建表ws"><a href="#（2）启动MySQL，在test库下建表ws" class="headerlink" title="（2）启动MySQL，在test库下建表ws"></a>（2）启动MySQL，在test库下建表ws</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span>     </span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `ws` (</span><br><span class="line">  `id` <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `ts` <span class="type">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `vc` <span class="type">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (`id`)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8</span><br></pre></td></tr></table></figure><h4 id="（3）编写输出到MySQL的示例代码"><a href="#（3）编写输出到MySQL的示例代码" class="headerlink" title="（3）编写输出到MySQL的示例代码"></a>（3）编写输出到MySQL的示例代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.&#123;<span class="type">JdbcConnectionOptions</span>, <span class="type">JdbcSink</span>, <span class="type">JdbcStatementBuilder</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">PreparedStatement</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToMysqlTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    stream.addSink(<span class="type">JdbcSink</span>.sink(</span><br><span class="line">      <span class="string">&quot;insert into shop (name, area, dizhi, price) values(?, ?, ?, ?)&quot;</span>, <span class="comment">// 定义写入Mysql的语句</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcStatementBuilder</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(t: <span class="type">PreparedStatement</span>, u: <span class="type">Event</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          t.setString(<span class="number">1</span>, u.user)</span><br><span class="line">          t.setString(<span class="number">2</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">3</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">4</span>, u.url)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcConnectionOptions</span>.<span class="type">JdbcConnectionOptionsBuilder</span>()</span><br><span class="line">        .withUrl(<span class="string">&quot;jdbc:mysql://master:3306/test?useSSL=false&quot;</span>)</span><br><span class="line">        .withDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">        .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">        .withPassword(<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">        .build()</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"><a href="#（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。" class="headerlink" title="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"></a>（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。</h4><h3 id="1-4-5-自定义Sink输出"><a href="#1-4-5-自定义Sink输出" class="headerlink" title="1.4.5 自定义Sink输出"></a><strong>1.4.5</strong> <strong>自定义Sink输出</strong></h3><h4 id="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。"><a href="#如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。" class="headerlink" title="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。"></a>如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> <span class="type">MySinkFunction</span>&lt;<span class="type">String</span>&gt;());</span><br></pre></td></tr></table></figure><h4 id="在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"><a href="#在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。" class="headerlink" title="在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"></a>在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。</h4><h4 id="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"><a href="#这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。" class="headerlink" title="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"></a>这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h5 id=&quot;DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：&quot;&gt;&lt;a href=&quot;#DataStream-API是Flink的核心层API。一个Flink程序，其实就</summary>
      
    
    
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/categories/Flink/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Flink" scheme="https://bigdata-yx.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1083.html"/>
    <id>https://bigdata-yx.github.io/posts/1083.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据数据可视化学习路线"><a href="#大数据数据可视化学习路线" class="headerlink" title="大数据数据可视化学习路线"></a>大数据数据可视化学习路线</h1><h5 id="前提：具备一定的Python基础"><a href="#前提：具备一定的Python基础" class="headerlink" title="前提：具备一定的Python基础"></a>前提：具备一定的Python基础</h5><h2 id="1-python的基础学习，以及进阶学习"><a href="#1-python的基础学习，以及进阶学习" class="headerlink" title="1.python的基础学习，以及进阶学习"></a>1.python的基础学习，以及进阶学习</h2><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href>https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="2-numpy、pandas的入门学习（先学numpy）"><a href="#2-numpy、pandas的入门学习（先学numpy）" class="headerlink" title="2.numpy、pandas的入门学习（先学numpy）"></a>2.numpy、pandas的入门学习（先学numpy）</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="3-数据可视化matplotlib库的基础绘图"><a href="#3-数据可视化matplotlib库的基础绘图" class="headerlink" title="3.数据可视化matplotlib库的基础绘图"></a>3.数据可视化matplotlib库的基础绘图</h3><p>【千锋教育python数据可视化Matplotlib绘图教程，Matplotlib柱状图｜Matplotlib动态图｜Matplotlib散点图】<a href>https://www.bilibili.com/video/BV1nM411m7Cf?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看"><a href="#4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看" class="headerlink" title="4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)"></a>4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="5-数据可视化pyecharts可交互式基础绘图"><a href="#5-数据可视化pyecharts可交互式基础绘图" class="headerlink" title="5.数据可视化pyecharts可交互式基础绘图"></a>5.数据可视化pyecharts可交互式基础绘图</h3><p>【千锋教育PyEcharts数据可视化快速入门教程，大数据分析Python交互绘图实用利器】<a href>https://www.bilibili.com/video/BV1nM411F7GT?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="6-数据可视化tableau图标绘图"><a href="#6-数据可视化tableau图标绘图" class="headerlink" title="6.数据可视化tableau图标绘图"></a>6.数据可视化tableau图标绘图</h3><p>【【Tableau教程】Tableau零基础教程，带你解锁当下最受欢迎的数据可视化软件】<a href>https://www.bilibili.com/video/BV1E4411B7ef?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="7-数据交互式可视化进阶绘图前提基础html"><a href="#7-数据交互式可视化进阶绘图前提基础html" class="headerlink" title="7.数据交互式可视化进阶绘图前提基础html"></a>7.数据交互式可视化进阶绘图前提基础html</h3><p>【黑马程序员pink老师前端入门教程，零基础必看的h5(html5)+css3+移动端前端视频教程】<a href>https://www.bilibili.com/video/BV14J4114768?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="8-数据可视化echarts可交互式进阶绘图"><a href="#8-数据可视化echarts可交互式进阶绘图" class="headerlink" title="8.数据可视化echarts可交互式进阶绘图"></a>8.数据可视化echarts可交互式进阶绘图</h3><p>【电商平台数据可视化实时监控系统-Echarts-vue项目综合练习-pink老师推荐(持续更新)素材已经更新】<a href>https://www.bilibili.com/video/BV1bh41197p8?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="9-使用vue进行数据可视化前提基础Node-js"><a href="#9-使用vue进行数据可视化前提基础Node-js" class="headerlink" title="9.使用vue进行数据可视化前提基础Node.js"></a>9.使用vue进行数据可视化前提基础Node.js</h3><p>【黑马程序员Node.js全套入门教程，nodejs新教程含es6模块化+npm+express+webpack+promise等_Nodejs实战案例详解】<a href>https://www.bilibili.com/video/BV1a34y167AZ?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="10-使用vue进行数据可视化"><a href="#10-使用vue进行数据可视化" class="headerlink" title="10.使用vue进行数据可视化"></a>10.使用vue进行数据可视化</h3><p>【千锋Echarts+Vue3.0数据可视化项目构建_入门必备前端项目实战教程】<a href>https://www.bilibili.com/video/BV14u411D7qK?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据数据可视化学习路线&quot;&gt;&lt;a href=&quot;#大数据数据可视化学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据数据可视化学习路线&quot;&gt;&lt;/a&gt;大数据数据可视化学习路线&lt;/h1&gt;&lt;h5 id=&quot;前提：具备一定的Python基础&quot;&gt;&lt;a href</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>数据处理学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1082.html"/>
    <id>https://bigdata-yx.github.io/posts/1082.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><h1 id="大数据数据处理学习路线"><a href="#大数据数据处理学习路线" class="headerlink" title="大数据数据处理学习路线"></a>大数据数据处理学习路线</h1><p><strong>注意</strong>：下列无标注的全部要看</p><h2 id="第一阶段：基础部分"><a href="#第一阶段：基础部分" class="headerlink" title="第一阶段：基础部分"></a>第一阶段：基础部分</h2><ul><li>语言基础：python、java基础<ul><li>python：【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注释：第一阶段</li><li>java：【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>数据库基础：mysql<ul><li>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：基础篇</li></ul></li><li>linux：命令基础（）<ul><li>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></li></ul></li></ul><h2 id="第二部分：简单工具使用"><a href="#第二部分：简单工具使用" class="headerlink" title="第二部分：简单工具使用"></a>第二部分：简单工具使用</h2><ul><li><p>python第三方库：pandas、numpy、requests、bs4、jieba、snownlp</p><table><thead><tr><th>第三方库</th><th>链接</th></tr></thead><tbody><tr><td>pandas、numpy</td><td>【千锋教育python数据分析教程200集，Python数据分析师入门必备视频】<a href="https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E6%B3%A8%EF%BC%9Ap38-p117%EF%BC%89">https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58（注：p38-p117）</a></td></tr><tr><td>requests、bs4</td><td>【尚硅谷Python爬虫教程小白零基础速通（含python基础+爬虫案例）】<a href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a> (注：p52-最后)</td></tr><tr><td>jieba</td><td>【Python Jieba 中文分词工具】<a href="https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></td></tr><tr><td>snownlp</td><td>【Lecture 12 基于Snownlp的文本情感分析】<a href="https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E7%9C%8B%E5%AE%8C%E8%BF%99%E9%9B%86%E5%B0%B1%E8%A1%8C%E4%BA%86%EF%BC%89">https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58（看完这集就行了）</a></td></tr></tbody></table></li><li><p>excel：函数使用以及操作</p><ul><li>【2025必看！全网最新最细最实用Excel零基础入门到精通全套教程！专为零基础小白打造！内容富含Excel表格基础操作、实用函数讲解、项目实战等！】<a href="https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li><p>hadoop：hadoop的基本使用命令</p><ul><li><a href="https://blog.csdn.net/m0_43405302/article/details/122243263">hadoop的HDFS的shell命令大全（一篇文章就够了）_shell统计hdfs-CSDN博客</a></li></ul></li></ul><h2 id="第三部分：集群工具使用"><a href="#第三部分：集群工具使用" class="headerlink" title="第三部分：集群工具使用"></a>第三部分：集群工具使用</h2><ul><li><p>scala：scala语言基础</p><ul><li>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：看完前十章</li></ul></li><li><p>mapreduce：了解基本使用以及自定义方法</p><ul><li><p>【黑马程序员大数据Hadoop3.x全套教程，一套精通Hadoop的大数据入门教程】<a href="https://www.bilibili.com/video/BV11N411d7Zh?p=214&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV11N411d7Zh?p=214&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p171-214</p></li></ul></li><li><p>spark：掌握sparkcore sparksql</p><ul><li>【全网最全大数据Spark3.0教程 Spark3.0从入门到精通 黑马程序员大数据入门教程系列】<a href="https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：搭建部分不用看、只看sparkcore和sparksql</li></ul></li><li><p>hive：了解hive命令以及udf自定义函数</p><ul><li><p>【黑马程序员Hive全套教程，大数据Hive3.x数仓开发精讲到企业级实战应用】<a href="https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p0-p96</p></li></ul></li></ul><h2 id="第四部分：深入学习"><a href="#第四部分：深入学习" class="headerlink" title="第四部分：深入学习"></a>第四部分：深入学习</h2><ul><li>机器学习：了解机器学习原理以及sklearn使用<ul><li>【黑马程序员3天快速入门python机器学习】<a href="https://www.bilibili.com/video/BV1nt411r7tj?p=18&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1nt411r7tj?p=18&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>spark streaming：了解流式数据<ul><li>自己找视频或文档</li></ul></li><li>项目制作：尝试制作大数据项目</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;h1 id=&quot;大数据数据处理学习路线&quot;&gt;&lt;a href=&quot;#大数据数据处理学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据数据处理学习路线&quot;&gt;&lt;/a&gt;大数据数据处理学习路线&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：下列无标注</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>集群搭建学习路线</title>
    <link href="https://bigdata-yx.github.io/posts/1081.html"/>
    <id>https://bigdata-yx.github.io/posts/1081.html</id>
    <published>2025-01-13T08:12:05.000Z</published>
    <updated>2025-01-13T08:12:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据集群搭建学习路线"><a href="#大数据集群搭建学习路线" class="headerlink" title="大数据集群搭建学习路线"></a>大数据集群搭建学习路线</h1><h3 id="前提：熟练使用Linux的命令及其操作"><a href="#前提：熟练使用Linux的命令及其操作" class="headerlink" title="前提：熟练使用Linux的命令及其操作"></a>前提：熟练使用Linux的命令及其操作</h3><h3 id="0-CentosLinux的基操"><a href="#0-CentosLinux的基操" class="headerlink" title="0.CentosLinux的基操"></a>0.CentosLinux的基操</h3><p>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="1-Mysql数据库的入门学习"><a href="#1-Mysql数据库的入门学习" class="headerlink" title="1.Mysql数据库的入门学习"></a>1.Mysql数据库的入门学习</h3><p>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="2-Hadoop集群的搭建及其使用和Hive的使用"><a href="#2-Hadoop集群的搭建及其使用和Hive的使用" class="headerlink" title="2.Hadoop集群的搭建及其使用和Hive的使用"></a>2.Hadoop集群的搭建及其使用和Hive的使用</h3><p>【黑马程序员大数据入门到实战教程，大数据开发必会的Hadoop、Hive，云平台实战项目全套一网打尽】<a href="https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="3-Java的基础学习（上部）"><a href="#3-Java的基础学习（上部）" class="headerlink" title="3.Java的基础学习（上部）"></a>3.Java的基础学习（上部）</h3><p>【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="4-scala语言的学习"><a href="#4-scala语言的学习" class="headerlink" title="4.scala语言的学习"></a>4.scala语言的学习</h3><p>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="5-Python基础的学习"><a href="#5-Python基础的学习" class="headerlink" title="5.Python基础的学习"></a>5.Python基础的学习</h3><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="6-zookeeper组件的学习"><a href="#6-zookeeper组件的学习" class="headerlink" title="6.zookeeper组件的学习"></a>6.zookeeper组件的学习</h3><p>【黑马程序员Zookeeper视频教程，快速入门zookeeper技术】<a href="https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="7-kafka组件的利用"><a href="#7-kafka组件的利用" class="headerlink" title="7.kafka组件的利用"></a>7.kafka组件的利用</h3><p>【尚硅谷Kafka教程，2024新版kafka视频，零基础入门到实战】<a href="https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="8-Flume的使用"><a href="#8-Flume的使用" class="headerlink" title="8.Flume的使用"></a>8.Flume的使用</h3><p>【【海牛大数据】Flume教程（学Flume看这个一套就够了，从基础到扩展到实战案例到组件配合全都有）】<a href="https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="9-Sqoop的运用"><a href="#9-Sqoop的运用" class="headerlink" title="9.Sqoop的运用"></a>9.Sqoop的运用</h3><p>【【海牛大数据】Sqoop教程（命令详解、各组件融合、实战案例）】<a href="https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="10-Spark的搭建"><a href="#10-Spark的搭建" class="headerlink" title="10.Spark的搭建"></a>10.Spark的搭建</h3><h4 id="Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）"><a href="#Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）" class="headerlink" title="Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）"></a>Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）</h4><p><a href="https://blog.csdn.net/JunLeon/article/details/123625680">(超详细) Spark环境搭建（Local模式、 StandAlone模式、Spark On Yarn模式）-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_44480968/article/details/119580363">Spark配置高可用（HA）_spark ha-CSDN博客</a></p><h3 id="11-Flink搭建及其使用-由于python版的使用还不是特别完整"><a href="#11-Flink搭建及其使用-由于python版的使用还不是特别完整" class="headerlink" title="11.Flink搭建及其使用(由于python版的使用还不是特别完整)"></a>11.Flink搭建及其使用(由于python版的使用还不是特别完整)</h3><h4 id="scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="scala版：【【尚硅谷】Flink1.13教程（Scala版）】https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>scala版：【【尚硅谷】Flink1.13教程（Scala版）】<a href="https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h4 id="java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】<a href="https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h3 id="11-Redis的搭建及其使用"><a href="#11-Redis的搭建及其使用" class="headerlink" title="11.Redis的搭建及其使用"></a>11.Redis的搭建及其使用</h3><p>【黑马程序员Redis入门到实战教程，深度透析redis底层原理+redis分布式锁+企业解决方案+黑马点评实战项目】<a href="https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="12-Hbase的搭建及其使用"><a href="#12-Hbase的搭建及其使用" class="headerlink" title="12.Hbase的搭建及其使用"></a>12.Hbase的搭建及其使用</h3><p>【【好程序员】大数据全新分布式存储HBase精品课程】<a href="https://www.bilibili.com/video/BV1RM411i7XM?p=15&vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1RM411i7XM?p=15&amp;vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="13-Azkaban的搭建及其使用"><a href="#13-Azkaban的搭建及其使用" class="headerlink" title="13.Azkaban的搭建及其使用"></a>13.Azkaban的搭建及其使用</h3><p>【尚硅谷大数据Azkaban 3.x教程（全新发布）】<a href="https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;大数据集群搭建学习路线&quot;&gt;&lt;a href=&quot;#大数据集群搭建学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据集群搭建学习路线&quot;&gt;&lt;/a&gt;大数据集群搭建学习路线&lt;/h1&gt;&lt;h3 id=&quot;前提：熟练使用Linux的命令及其操作&quot;&gt;&lt;a href=</summary>
      
    
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
    
    <category term="学习路线" scheme="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop原理</title>
    <link href="https://bigdata-yx.github.io/posts/3985.html"/>
    <id>https://bigdata-yx.github.io/posts/3985.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop多map条件查询导入hdfs.md</title>
    <link href="https://bigdata-yx.github.io/posts/3992.html"/>
    <id>https://bigdata-yx.github.io/posts/3992.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多map条件查询导入hdfs"><a href="#多map条件查询导入hdfs" class="headerlink" title="多map条件查询导入hdfs"></a>多map条件查询导入hdfs</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect 数据库连接字符串 \</span><br><span class="line">--username 数据库用户名 \</span><br><span class="line">--password 数据库密码 \</span><br><span class="line">--target-dir hdfs位置 \</span><br><span class="line">--delete-target-dir \  <span class="comment"># 这个就是把目录删了，不然mapreduce会执行失败</span></span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \ <span class="comment"># 使用什么分隔符</span></span><br><span class="line">--num-mappers 3 \</span><br><span class="line">--split-by 切分数依据 \</span><br><span class="line">--query <span class="string">&#x27; SQL语句 and $CONDITIONS &#x27;</span></span><br></pre></td></tr></table></figure><h3 id="–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"><a href="#–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力" class="headerlink" title="–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"></a><strong>–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力</strong></h3><h3 id="CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理"><a href="#CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理" class="headerlink" title="$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理"></a><strong>$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理</strong></h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;多map条件查询导入hdfs&quot;&gt;&lt;a href=&quot;#多map条件查询导入hdfs&quot; class=&quot;headerlink&quot; title=&quot;多map条件查询导入hdfs&quot;&gt;&lt;/a&gt;多map条件查询导入hdfs&lt;/h2&gt;&lt;figure class=&quot;highlight </summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop实例命令</title>
    <link href="https://bigdata-yx.github.io/posts/3986.html"/>
    <id>https://bigdata-yx.github.io/posts/3986.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="创建hive的表根据mysql上的表进行创建-create-hive-table"><a href="#创建hive的表根据mysql上的表进行创建-create-hive-table" class="headerlink" title="创建hive的表根据mysql上的表进行创建(create-hive-table)"></a><strong>创建hive的表根据mysql上的表进行创建(create-hive-table)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table --connect jdbc:mysql://master:3306/sqoop_db --username root --password 123456 --table city --hive-table hkjcpdd.city</span><br></pre></td></tr></table></figure><h4 id="查看mysql上有什么表-list-tables"><a href="#查看mysql上有什么表-list-tables" class="headerlink" title="查看mysql上有什么表(list-tables)"></a><strong>查看mysql上有什么表(list-tables)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-tables --connect jdbc:mysql://master:3306/sys --username root --password 123456</span><br></pre></td></tr></table></figure><h3 id="查看mysql上有什么库-list-databases"><a href="#查看mysql上有什么库-list-databases" class="headerlink" title="查看mysql上有什么库(list-databases)"></a><strong>查看mysql上有什么库(list-databases)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://master:3306/ --username root --password 12345</span><br></pre></td></tr></table></figure><h3 id="使用sql进行操作-eval-query"><a href="#使用sql进行操作-eval-query" class="headerlink" title="使用sql进行操作(eval   query)"></a><strong>使用sql进行操作(eval   query)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">eval</span> --connect jdbc:mysql://master:3306/sqoop_db --username root --password  123456 --query <span class="string">&quot;select * from city limit 2;&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;创建hive的表根据mysql上的表进行创建-create-hive-table&quot;&gt;&lt;a href=&quot;#创建hive的表根据mysql上的表进行创建-create-hive-table&quot; class=&quot;headerlink&quot; title=&quot;创建hive的表根据my</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop导入到Hbase</title>
    <link href="https://bigdata-yx.github.io/posts/3988.html"/>
    <id>https://bigdata-yx.github.io/posts/3988.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-table hkjcpdd:city \</span><br><span class="line">--column-family cf \</span><br><span class="line">--hbase-row-key <span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --hbase-row-key:要求mysql表必须有主见，将主键作为rowkey,表示一行</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;l</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop导入其他格式文件</title>
    <link href="https://bigdata-yx.github.io/posts/3994.html"/>
    <id>https://bigdata-yx.github.io/posts/3994.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导入其他格式文件"><a href="#导入其他格式文件" class="headerlink" title="导入其他格式文件"></a>导入其他格式文件</h2><h3 id="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式"><a href="#导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式" class="headerlink" title="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)"></a><strong>导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /data/hkjcpdd \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-parquetfile \ <span class="comment">#文件格式</span></span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query <span class="string">&#x27;select * from city where id &lt; 10 and $CONDITIONS&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;导入其他格式文件&quot;&gt;&lt;a href=&quot;#导入其他格式文件&quot; class=&quot;headerlink&quot; title=&quot;导入其他格式文件&quot;&gt;&lt;/a&gt;导入其他格式文件&lt;/h2&gt;&lt;h3 id=&quot;导入不同格式，支持as-avrodatafile、as-parquetfile、a</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop应用案例</title>
    <link href="https://bigdata-yx.github.io/posts/3987.html"/>
    <id>https://bigdata-yx.github.io/posts/3987.html</id>
    <published>2025-01-13T08:07:49.000Z</published>
    <updated>2025-01-13T08:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="创建一个执行文件然后给予权限然后执行"><a href="#创建一个执行文件然后给予权限然后执行" class="headerlink" title="创建一个执行文件然后给予权限然后执行"></a><strong>创建一个执行文件然后给予权限然后执行</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">batch_date=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--usernmae root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir hive表hdfs目录 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query select * from city</span><br><span class="line"></span><br><span class="line">result=$?</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [<span class="variable">$result</span> != 0];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;执行失败&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd</span><br><span class="line"><span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span>  <span class="string">&quot;执行成功&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;创建一个执行文件然后给予权限然后执行&quot;&gt;&lt;a href=&quot;#创建一个执行文件然后给予权限然后执行&quot; class=&quot;headerlink&quot; title=&quot;创建一个执行文件然后给予权限然后执行&quot;&gt;&lt;/a&gt;&lt;strong&gt;创建一个执行文件然后给予权限然后执行&lt;/stro</summary>
      
    
    
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/categories/Sqoop/"/>
    
    
    <category term="示例" scheme="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/"/>
    
    <category term="Sqoop" scheme="https://bigdata-yx.github.io/tags/Sqoop/"/>
    
  </entry>
  
</feed>
