<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>曦</title>
    <link>https://bigdata-yx.github.io/</link>
    
    <image>
      <url>https://bigdata-yx.github.io/imgs/avatar.webp</url>
      <title>曦</title>
      <link>https://bigdata-yx.github.io/</link>
    </image>
    
    <atom:link href="https://bigdata-yx.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>曦-description</description>
    <pubDate>Tue, 14 Jan 2025 07:38:45 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Flink</title>
      <link>https://bigdata-yx.github.io/posts/pd11.html</link>
      <guid>https://bigdata-yx.github.io/posts/pd11.html</guid>
      <pubDate>Tue, 14 Jan 2025 07:38:45 GMT</pubDate>
      
      
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flink/">Flink</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flink/">Flink</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/pd11.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flink快速上手</title>
      <link>https://bigdata-yx.github.io/posts/pd13.html</link>
      <guid>https://bigdata-yx.github.io/posts/pd13.html</guid>
      <pubDate>Tue, 14 Jan 2025 07:38:45 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;Flink快速上手&quot;&gt;&lt;a href=&quot;#Flink快速上手&quot; class=&quot;headerlink&quot; title=&quot;Flink快速上手&quot;&gt;&lt;/a&gt;Flink快速上手&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;[!NOTE]&lt;/p&gt;
&lt;p&gt;前提准备好相关maven环境</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="Flink快速上手"><a href="#Flink快速上手" class="headerlink" title="Flink快速上手"></a>Flink快速上手</h1><blockquote><p>[!NOTE]</p><p>前提准备好相关maven环境和依赖</p></blockquote><h2 id="1-WordCount代码编写"><a href="#1-WordCount代码编写" class="headerlink" title="1.WordCount代码编写"></a>1.WordCount代码编写</h2><h5 id="需求：统计一段文字中，每个单词出现的频次。"><a href="#需求：统计一段文字中，每个单词出现的频次。" class="headerlink" title="需求：统计一段文字中，每个单词出现的频次。"></a>需求：统计一段文字中，每个单词出现的频次。</h5><h5 id="环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。"><a href="#环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。" class="headerlink" title="环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。"></a>环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。</h5><h3 id="1-1-批处理"><a href="#1-1-批处理" class="headerlink" title="1.1  批处理"></a>1.1  批处理</h3><h5 id="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"><a href="#批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。" class="headerlink" title="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"></a>批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。</h5><h4 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1)数据准备"></a>1)数据准备</h4><ul><li>（1）在工程根目录下新建一个input文件夹，并在下面创建文本文件words.txt</li><li>（2）在words.txt中输入一些文字，例如：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello flink</span><br><span class="line">hello world</span><br><span class="line">hello java</span><br></pre></td></tr></table></figure><h4 id="2-代码编写"><a href="#2-代码编写" class="headerlink" title="2)代码编写"></a>2)代码编写</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建一个执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineData = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据集进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineData.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneGroup = wordAndOne.groupBy(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合统计</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）输出"><a href="#（2）输出" class="headerlink" title="（2）输出"></a>（2）输出</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(flink,1)</span><br><span class="line">(world,1)</span><br><span class="line">(hello,3)</span><br><span class="line">(java,1)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>需要注意的是，这种代码的实现方式，是基于DataSet API的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上Flink本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的API来实现。所以从Flink 1.12开始，官方推荐的做法是直接使用DataStream API，在提交任务时通过将执行模式设为BATCH来进行批处理：</p><p>$ bin&#x2F;flink run -Dexecution.runtime-mode&#x3D;BATCH BatchWordCount.jar</p><p>这样，DataSet API就没什么用了，在实际应用中我们只要维护一套DataStream API就可以。这里只是为了方便大家理解，我们依然用DataSet API做了批处理的实现。</p></blockquote><h3 id="1-2流处理"><a href="#1-2流处理" class="headerlink" title="1.2流处理"></a>1.2流处理</h3><blockquote><p>[!CAUTION]</p><p>对于Flink而言，流才是整个处理逻辑的底层核心，所以流批统一之后的DataStream API更加强大，可以直接处理批处理和流处理的所有场景。</p></blockquote><h5 id="我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"><a href="#我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致" class="headerlink" title="我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"></a>我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BoundedStreamWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineDataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndGroup = wordAndOne.keyBy(_._1)</span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合操作</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行任务</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3&gt; (java,1)</span><br><span class="line">5&gt; (hello,1)</span><br><span class="line">5&gt; (hello,2)</span><br><span class="line">5&gt; (hello,3)</span><br><span class="line">13&gt; (flink,1)</span><br><span class="line">9&gt; (world,1)</span><br></pre></td></tr></table></figure><h3 id="主要观察与批处理程序BatchWordCount的不同："><a href="#主要观察与批处理程序BatchWordCount的不同：" class="headerlink" title="主要观察与批处理程序BatchWordCount的不同："></a>主要观察与批处理程序BatchWordCount的不同：</h3><ul><li>创建执行环境的不同，流处理程序使用的是StreamExecutionEnvironment</li><li>转换处理之后，得到的数据对象类型不同</li><li>分组操作调用的是keyBy方法，可以传入一个匿名函数作为键选择器（KeySelector），指定当前分组的key是什么</li><li>代码末尾需要调用env的execute方法，开始执行任务</li></ul><h4 id="2）读取socket文本流"><a href="#2）读取socket文本流" class="headerlink" title="2）读取socket文本流"></a>2）读取socket文本流</h4><blockquote><p>[!NOTE]</p><p>在实际的生产环境中，真正的数据流其实是无界的，有开始却没有结束，这就要求我们需要持续地处理捕获的数据。为了模拟这种场景，可以监听socket端口，然后向该端口不断的发送数据。</p></blockquote><h4 id="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："><a href="#（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：" class="headerlink" title="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："></a>（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.<span class="type">ParameterTool</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCOunt</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> parameterTool = <span class="type">ParameterTool</span>.fromArgs(args)</span><br><span class="line">    <span class="keyword">val</span> hostname = parameterTool.get(<span class="string">&quot;host&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> port = parameterTool.getInt(<span class="string">&quot;port&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineDataStream = env.socketTextStream(hostname, port)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineAndOneGroup = wordAndOne.keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sum = lineAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"><a href="#（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试" class="headerlink" title="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"></a>（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ nc -lk 7777</span><br></pre></td></tr></table></figure><blockquote><p>[!IMPORTANT]</p><p>注意：要先启动端口，后启动StreamWordCount程序，否则会报超时连接异常。</p></blockquote><h4 id="（3）启动StreamWordCount程序"><a href="#（3）启动StreamWordCount程序" class="headerlink" title="（3）启动StreamWordCount程序"></a>（3）启动StreamWordCount程序</h4><blockquote><p>[!WARNING]</p><p>我们会发现程序启动之后没有任何输出、也不会退出。这是正常的，因为Flink的流处理是事件驱动的，当前程序会一直处于监听状态，只有接收到数据才会执行任务、输出统计结果。</p></blockquote><h5 id="（4）从hadoop102发送数据"><a href="#（4）从hadoop102发送数据" class="headerlink" title="（4）从hadoop102发送数据"></a>（4）从hadoop102发送数据</h5><h5 id="①在hadoop102主机中，输入“hello-flink”，输出如下内容"><a href="#①在hadoop102主机中，输入“hello-flink”，输出如下内容" class="headerlink" title="①在hadoop102主机中，输入“hello flink”，输出如下内容"></a>①在hadoop102主机中，输入“hello flink”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13&gt; (flink,1)</span><br><span class="line">5&gt; (hello,1)</span><br></pre></td></tr></table></figure><h5 id="②再输入“hello-world”，输出如下内容"><a href="#②再输入“hello-world”，输出如下内容" class="headerlink" title="②再输入“hello world”，输出如下内容"></a>②再输入“hello world”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2&gt; (world,1)</span><br><span class="line">5&gt; (hello,2)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p><p>因为对于flatMap里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flink/">Flink</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flink/">Flink</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/pd13.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flink集群部署搭建</title>
      <link>https://bigdata-yx.github.io/posts/pd12.html</link>
      <guid>https://bigdata-yx.github.io/posts/pd12.html</guid>
      <pubDate>Tue, 14 Jan 2025 07:38:45 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;Flink1-14-0集群部署&quot;&gt;&lt;a href=&quot;#Flink1-14-0集群部署&quot; class=&quot;headerlink&quot; title=&quot;Flink1.14.0集群部署&quot;&gt;&lt;/a&gt;&lt;strong&gt;Flink1.14.0集群部署&lt;/strong&gt;&lt;/h1&gt;&lt;fig</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="Flink1-14-0集群部署"><a href="#Flink1-14-0集群部署" class="headerlink" title="Flink1.14.0集群部署"></a><strong>Flink1.14.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line"><span class="comment">#     将jobmanager.rpc.address的主机名改成master就好了</span></span><br><span class="line"><span class="comment"># 分发到其他主机，然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-17-0集群部署"><a href="#Flink1-17-0集群部署" class="headerlink" title="Flink1.17.0集群部署"></a><strong>Flink1.17.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line">    <span class="comment"># JobManager节点地址.</span></span><br><span class="line">    jobmanager.rpc.address: master</span><br><span class="line">    jobmanager.bind-host: 0.0.0.0</span><br><span class="line">    rest.address: master</span><br><span class="line">    rest.bind-address: 0.0.0.0</span><br><span class="line">    <span class="comment"># TaskManager节点地址.需要配置为当前机器名</span></span><br><span class="line">    taskmanager.bind-host: 0.0.0.0</span><br><span class="line">    taskmanager.host: master</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发到其他主机</span></span><br><span class="line"><span class="comment"># 修改slave1的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave1</span><br><span class="line"><span class="comment"># 修改slave2的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave2</span><br><span class="line"><span class="comment"># 然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-14-0高可用部署"><a href="#Flink1-14-0高可用部署" class="headerlink" title="Flink1.14.0高可用部署"></a><strong>Flink1.14.0高可用部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前面正常部署，后面在high中新增</span></span><br><span class="line"><span class="comment">#开启HA，使用文件系统作为快照存储</span></span><br><span class="line">state.backend: filesystem （选填）</span><br><span class="line"> </span><br><span class="line"><span class="comment">#启用检查点，可以将快照保存到HDFS （选填）</span></span><br><span class="line">state.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints</span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用zookeeper搭建高可用 （必填）</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储JobManager的元数据到HDFS （必填）</span></span><br><span class="line">high-availability.storageDir: hdfs://node1:8020/flink/ha/</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 配置ZK集群地址（必填）</span></span><br><span class="line">high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定flink在zokkeper的位置（选填）</span></span><br><span class="line"> high-availability.zookeeper.path.root: /flink</span><br><span class="line"> </span><br><span class="line">  然后将flink-shaded-hadoop-2-uber-2.8.3-10.0.jar复制到/root/software/flink/lib中</span><br><span class="line"><span class="comment">#  然后scp最后就可以启动了</span></span><br></pre></td></tr></table></figure><h1 id="Flink-On-Yarn"><a href="#Flink-On-Yarn" class="headerlink" title="Flink On Yarn"></a>Flink On Yarn</h1><h3 id="1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode"><a href="#1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode" class="headerlink" title="1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode"></a><strong>1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">配置环境变量，增加环境变量配置如下：</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure><h3 id="2-会话模式部署"><a href="#2-会话模式部署" class="headerlink" title="2 会话模式部署"></a>2 会话模式部署</h3><h3 id="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下："><a href="#YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下：" class="headerlink" title="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下："></a>YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下：</h3><h3 id="（1）启动集群"><a href="#（1）启动集群" class="headerlink" title="（1）启动集群"></a>（1）启动集群</h3><ul><li><h3 id="（1）启动Hadoop集群（HDFS、YARN）。"><a href="#（1）启动Hadoop集群（HDFS、YARN）。" class="headerlink" title="（1）启动Hadoop集群（HDFS、YARN）。"></a>（1）启动Hadoop集群（HDFS、YARN）。</h3></li><li><h3 id="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"><a href="#（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。" class="headerlink" title="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"></a>（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。</h3></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh -nm <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">-d：分离模式，如果你不想让Flink YARN客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，YARN session也可以后台运行。</span><br><span class="line">-jm（–jobManagerMemory）：配置JobManager所需内存，默认单位MB。</span><br><span class="line">-nm（–name）：配置在YARN UI界面上显示的任务名。</span><br><span class="line">-qu（–queue）：指定YARN队列名。</span><br><span class="line">-tm（–taskManager）：配置每个TaskManager所使用内存。</span><br></pre></td></tr></table></figure><h5 id="YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。"><a href="#YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。" class="headerlink" title="YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。"></a>YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2022-11-17 15:20:52,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface hadoop104:40825 of application <span class="string">&#x27;application_1668668287070_0005&#x27;</span>.</span><br><span class="line">JobManager Web Interface: http://hadoop104:40825</span><br></pre></td></tr></table></figure><h3 id="（2）通过命令行提交作业"><a href="#（2）通过命令行提交作业" class="headerlink" title="（2）通过命令行提交作业"></a><strong>（2）通过命令行提交作业</strong></h3><ul><li>① 将FlinkTutorial-1.0-SNAPSHOT.jar任务上传至集群。</li><li>② 执行以下命令将该任务提交到已经开启的Yarn-Session中运行。</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h3 id="3-单作业模式部署"><a href="#3-单作业模式部署" class="headerlink" title="3 单作业模式部署"></a><strong>3 单作业模式部署</strong></h3><h6 id="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"><a href="#在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群" class="headerlink" title="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"></a>在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群</h6><h6 id="（1）执行命令提交作业"><a href="#（1）执行命令提交作业" class="headerlink" title="（1）执行命令提交作业"></a>（1）执行命令提交作业</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -d -t yarn-per-job -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h6 id="注意：如果启动过程中报如下异常"><a href="#注意：如果启动过程中报如下异常" class="headerlink" title="注意：如果启动过程中报如下异常"></a>注意：如果启动过程中报如下异常</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread “Thread-5” java.lang.IllegalStateException: Trying to access closed classloader. Please check <span class="keyword">if</span> you store classloaders directly or indirectly <span class="keyword">in</span> static fields. If the stacktrace suggests that the leak occurs <span class="keyword">in</span> a third party library and cannot be fixed immediately, you can <span class="built_in">disable</span> this check with the configuration ‘classloader.check-leaked-classloader’.</span><br><span class="line">at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders</span><br></pre></td></tr></table></figure><h6 id="解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置"><a href="#解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置" class="headerlink" title="解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置"></a>解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim flink-conf.yaml</span><br><span class="line"></span><br><span class="line">classloader.check-leaked-classloader: <span class="literal">false</span></span><br></pre></td></tr></table></figure><h6 id="（2）可以使用命令行查看或取消作业，命令如下。"><a href="#（2）可以使用命令行查看或取消作业，命令如下。" class="headerlink" title="（2）可以使用命令行查看或取消作业，命令如下。"></a>（2）可以使用命令行查看或取消作业，命令如下。</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h3 id="4-应用模式部署"><a href="#4-应用模式部署" class="headerlink" title="4 应用模式部署"></a><strong>4 应用模式部署</strong></h3><h4 id="1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。"><a href="#1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。" class="headerlink" title="(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。"></a>(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar </span><br></pre></td></tr></table></figure><h5 id="2-在命令行中查看或取消作业。"><a href="#2-在命令行中查看或取消作业。" class="headerlink" title="(2)在命令行中查看或取消作业。"></a>(2)在命令行中查看或取消作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h4 id="2）上传HDFS提交"><a href="#2）上传HDFS提交" class="headerlink" title="2）上传HDFS提交"></a><strong>2）上传HDFS提交</strong></h4><h5 id="可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。"><a href="#可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。" class="headerlink" title="可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。"></a>可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。</h5><h6 id="（1）上传flink的lib和plugins到HDFS上"><a href="#（1）上传flink的lib和plugins到HDFS上" class="headerlink" title="（1）上传flink的lib和plugins到HDFS上"></a>（1）上传flink的lib和plugins到HDFS上</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -<span class="built_in">mkdir</span> /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put lib/ /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put plugins/ /flink-dist</span><br></pre></td></tr></table></figure><h5 id="（2）提交作业"><a href="#（2）提交作业" class="headerlink" title="（2）提交作业"></a>（2）提交作业</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application    -Dyarn.provided.lib.dirs=<span class="string">&quot;hdfs://hadoop102:8020/flink-dist&quot;</span>    -c com.atguigu.wc.SocketStreamWordCount  hdfs://hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flink/">Flink</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flink/">Flink</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/pd12.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flink的DataStreamAPI</title>
      <link>https://bigdata-yx.github.io/posts/pd14.html</link>
      <guid>https://bigdata-yx.github.io/posts/pd14.html</guid>
      <pubDate>Tue, 14 Jan 2025 07:38:45 GMT</pubDate>
      
        
        
      <description>&lt;h5 id=&quot;DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：&quot;&gt;&lt;a href=&quot;#DataStream-API是Flink的核心层API。一个Flink程序，其实就</description>
        
      
      
      
      <content:encoded><![CDATA[<h5 id="DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："><a href="#DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：" class="headerlink" title="DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："></a>DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：</h5><p><img src="https://pic1.imgdb.cn/item/67861fbed0e0a243d4f42964.png"></p><h5 id="在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法："><a href="#在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法：" class="headerlink" title="在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法："></a>在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.addSource(...);</span><br></pre></td></tr></table></figure><h5 id="方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。"><a href="#方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。" class="headerlink" title="方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。"></a>方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。</h5><h5 id="从Flink1-12开始，主要使用流批统一的新Source架构："><a href="#从Flink1-12开始，主要使用流批统一的新Source架构：" class="headerlink" title="从Flink1.12开始，主要使用流批统一的新Source架构："></a>从Flink1.12开始，主要使用流批统一的新Source架构：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStreamSource</span>&lt;<span class="type">String</span>&gt; stream = env.fromSource(…)</span><br></pre></td></tr></table></figure><h5 id="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"><a href="#Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。" class="headerlink" title="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"></a>Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。</h5><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作:"></a>准备工作:</h2><h5 id="为了方便练习，这里使用WaterSensor作为数据模型。"><a href="#为了方便练习，这里使用WaterSensor作为数据模型。" class="headerlink" title="为了方便练习，这里使用WaterSensor作为数据模型。"></a>为了方便练习，这里使用WaterSensor作为数据模型。</h5><table><thead><tr><th><strong>id</strong></th><th><strong>String</strong></th><th><strong>水位传感器类型</strong></th></tr></thead><tbody><tr><td><strong>ts</strong></td><td><strong>Long</strong></td><td><strong>传感器记录时间戳</strong></td></tr><tr><td><strong>vc</strong></td><td><strong>Integer</strong></td><td><strong>水位记录</strong></td></tr></tbody></table><h4 id="具体代码如下："><a href="#具体代码如下：" class="headerlink" title="具体代码如下："></a>具体代码如下：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensor</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String id;</span><br><span class="line">    <span class="keyword">public</span> Long ts;</span><br><span class="line">    <span class="keyword">public</span> Integer vc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">(String id, Long ts, Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getId</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setId</span><span class="params">(String id)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getTs</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setTs</span><span class="params">(Long ts)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">getVc</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setVc</span><span class="params">(Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;WaterSensor&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;id=&#x27;&quot;</span> + id + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, ts=&quot;</span> + ts +</span><br><span class="line">                <span class="string">&quot;, vc=&quot;</span> + vc +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span> == o) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="literal">null</span> || getClass() != o.getClass()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">WaterSensor</span> <span class="variable">that</span> <span class="operator">=</span> (WaterSensor) o;</span><br><span class="line">        <span class="keyword">return</span> Objects.equals(id, that.id) &amp;&amp;</span><br><span class="line">                Objects.equals(ts, that.ts) &amp;&amp;</span><br><span class="line">                Objects.equals(vc, that.vc);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Objects.hash(id, ts, vc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="这里需要注意，我们定义的WaterSensor，有这样几个特点："><a href="#这里需要注意，我们定义的WaterSensor，有这样几个特点：" class="headerlink" title="这里需要注意，我们定义的WaterSensor，有这样几个特点："></a>这里需要注意，我们定义的WaterSensor，有这样几个特点：</h4><ul><li>类是公有（public）的</li><li>有一个无参的构造方法</li><li>所有属性都是公有（public）的</li><li>所有属性的类型都是可以序列化的</li></ul><h6 id="Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"><a href="#Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。" class="headerlink" title="Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"></a>Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。</h6><h6 id="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"><a href="#我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。" class="headerlink" title="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"></a>我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。</h6><h3 id="1-从集合、文件、元素中读取数据"><a href="#1-从集合、文件、元素中读取数据" class="headerlink" title="1.从集合、文件、元素中读取数据"></a>1.从集合、文件、元素中读取数据</h3><h5 id="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"><a href="#最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。" class="headerlink" title="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"></a>最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceBoundedTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">user: <span class="type">String</span>, url: <span class="type">String</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从元素中读取数据</span></span><br><span class="line"><span class="comment">//    val stream: DataStream[Int] = env.fromElements(1, 2, 3, 4, 5, 6)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从集合中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> clicks = <span class="type">List</span>(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> stream2: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromCollection(clicks)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从文件中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> stream3: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\click.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印输出</span></span><br><span class="line">    stream.print(<span class="string">&quot;从元素中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream2.print(<span class="string">&quot;从集合中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream3.print(<span class="string">&quot;从文件中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2从Socket读取数据"><a href="#1-2从Socket读取数据" class="headerlink" title="1.2从Socket读取数据"></a>1.2从Socket读取数据</h3><ul><li><h6 id="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"><a href="#不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。" class="headerlink" title="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"></a>不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。</h6></li><li><h6 id="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"><a href="#我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。" class="headerlink" title="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"></a>我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。</h6></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>);</span><br></pre></td></tr></table></figure><h3 id="1-3从Kafka中读取数据"><a href="#1-3从Kafka中读取数据" class="headerlink" title="1.3从Kafka中读取数据"></a>1.3从Kafka中读取数据</h3><ul><li><h6 id="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"><a href="#Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。" class="headerlink" title="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"></a>Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。</h6></li><li><h6 id="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。"><a href="#所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。" class="headerlink" title="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。"></a>所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。</h6></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下："><a href="#代码如下：" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用Properties保存kafka连接的配置</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"></span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-从数据生成器读取数据"><a href="#1-4-从数据生成器读取数据" class="headerlink" title="1.4 从数据生成器读取数据"></a>1.4 从数据生成器读取数据</h3><h6 id="Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖："><a href="#Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖：" class="headerlink" title="Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖："></a>Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖：</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-datagen<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下：-1"><a href="#代码如下：-1" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DataGeneratorDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataGeneratorSource&lt;String&gt; dataGeneratorSource =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">DataGeneratorSource</span>&lt;&gt;(</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="keyword">public</span> String <span class="title function_">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                                <span class="keyword">return</span> <span class="string">&quot;Number:&quot;</span>+value;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        Long.MAX_VALUE,</span><br><span class="line">                        RateLimiterStrategy.perSecond(<span class="number">10</span>),</span><br><span class="line">                        Types.STRING</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class="string">&quot;datagenerator&quot;</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-5Flink支持的数据类型"><a href="#1-5Flink支持的数据类型" class="headerlink" title="1.5Flink支持的数据类型"></a>1.5Flink支持的数据类型</h3><ol><li><h5 id="Flink的类型系统"><a href="#Flink的类型系统" class="headerlink" title="Flink的类型系统"></a>Flink的类型系统</h5><p>Flink使用“类型信息”（TypeInformation）来统一表示数据类型。TypeInformation类是Flink中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。</p></li><li><h5 id="Flink支持的数据类型"><a href="#Flink支持的数据类型" class="headerlink" title="Flink支持的数据类型"></a>Flink支持的数据类型</h5><p>对于常见的Java和Scala数据类型，Flink都是支持的。Flink在内部，Flink对支持不同的类型进行了划分，这些类型可以在Types工具类中找到：</p></li></ol><ul><li><h5 id="（1）基本类型"><a href="#（1）基本类型" class="headerlink" title="（1）基本类型"></a>（1）基本类型</h5><p>所有Java基本类型及其包装类，再加上Void、String、Date、BigDecimal和BigInteger。</p></li><li><h5 id="（2）数组类型"><a href="#（2）数组类型" class="headerlink" title="（2）数组类型"></a>（2）数组类型</h5><p>包括基本类型数组（PRIMITIVE_ARRAY）和对象数组（OBJECT_ARRAY）。</p></li><li><h5 id="（3）复合数据类型"><a href="#（3）复合数据类型" class="headerlink" title="（3）复合数据类型"></a>（3）复合数据类型</h5><ol><li><p>Java元组类型（TUPLE）：这是Flink内置的元组类型，是Java API的一部分。最多25个字段，也就是从Tuple0~Tuple25，不支持空字段。</p></li><li><h5 id="Scala-样例类及Scala元组：不支持空字段。"><a href="#Scala-样例类及Scala元组：不支持空字段。" class="headerlink" title="Scala 样例类及Scala元组：不支持空字段。"></a>Scala 样例类及Scala元组：不支持空字段。</h5></li><li><h5 id="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"><a href="#行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。" class="headerlink" title="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"></a>行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。</h5></li><li><h5 id="POJO：Flink自定义的类似于Java-bean模式的类。"><a href="#POJO：Flink自定义的类似于Java-bean模式的类。" class="headerlink" title="POJO：Flink自定义的类似于Java bean模式的类。"></a>POJO：Flink自定义的类似于Java bean模式的类。</h5></li></ol></li></ul><h4 id="（4）辅助类型"><a href="#（4）辅助类型" class="headerlink" title="（4）辅助类型"></a>（4）辅助类型</h4><ul><li><h5 id="Option、Either、List、Map等。"><a href="#Option、Either、List、Map等。" class="headerlink" title="Option、Either、List、Map等。"></a>Option、Either、List、Map等。</h5></li></ul><h4 id="（5）泛型类型（GENERIC）"><a href="#（5）泛型类型（GENERIC）" class="headerlink" title="（5）泛型类型（GENERIC）"></a>（5）泛型类型（GENERIC）</h4><ul><li><h6 id="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"><a href="#Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。" class="headerlink" title="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"></a>Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。</h6></li><li><h6 id="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"><a href="#在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。" class="headerlink" title="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"></a>在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。</h6></li><li><h6 id="Flink对POJO类型的要求如下："><a href="#Flink对POJO类型的要求如下：" class="headerlink" title="Flink对POJO类型的要求如下："></a>Flink对POJO类型的要求如下：</h6><ol><li><h6 id="类是公有（public）的"><a href="#类是公有（public）的" class="headerlink" title="类是公有（public）的"></a>类是公有（public）的</h6></li><li><h6 id="有一个无参的构造方法"><a href="#有一个无参的构造方法" class="headerlink" title="有一个无参的构造方法"></a>有一个无参的构造方法</h6></li><li><h6 id="所有属性都是公有（public）的"><a href="#所有属性都是公有（public）的" class="headerlink" title="所有属性都是公有（public）的"></a>所有属性都是公有（public）的</h6></li><li><h6 id="所有属性的类型都是可以序列化的"><a href="#所有属性的类型都是可以序列化的" class="headerlink" title="所有属性的类型都是可以序列化的"></a>所有属性的类型都是可以序列化的</h6></li></ol></li></ul><h4 id="3）类型提示（Type-Hints）"><a href="#3）类型提示（Type-Hints）" class="headerlink" title="3）类型提示（Type Hints）"></a>3）类型提示（Type Hints）</h4><h6 id="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"><a href="#Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。" class="headerlink" title="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"></a>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</h6><h6 id="为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。"><a href="#为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。" class="headerlink" title="为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。"></a>为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。</h6><h6 id="回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"><a href="#回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。" class="headerlink" title="回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"></a>回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.map(word -&gt; <span class="type">Tuple2</span>.of(word, <span class="number">1</span>L))</span><br><span class="line">.returns(<span class="type">Types</span>.<span class="type">TUPLE</span>(<span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>));</span><br></pre></td></tr></table></figure><h6 id="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。"><a href="#Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。" class="headerlink" title="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。"></a>Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">returns(<span class="keyword">new</span> <span class="type">TypeHint</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">SomeType</span>&gt;&gt;()&#123;&#125;)</span><br></pre></td></tr></table></figure><h3 id="转换算子（Transformation）"><a href="#转换算子（Transformation）" class="headerlink" title="转换算子（Transformation）"></a>转换算子（Transformation）</h3><h5 id="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"><a href="#数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。" class="headerlink" title="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"></a>数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。</h5><p><img src="https://pic1.imgdb.cn/item/678624a7d0e0a243d4f42b7c.png"></p><h3 id="1-基本转换算子（map-x2F-filter-x2F-flatMap）"><a href="#1-基本转换算子（map-x2F-filter-x2F-flatMap）" class="headerlink" title="1.基本转换算子（map&#x2F; filter&#x2F; flatMap）"></a>1.基本转换算子（map&#x2F; filter&#x2F; flatMap）</h3><h4 id="1-1-1映射（map）"><a href="#1-1-1映射（map）" class="headerlink" title="1.1.1映射（map）"></a>1.1.1映射（map）</h4><h6 id="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"><a href="#map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。" class="headerlink" title="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"></a>map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。</h6><p><img src="https://pic1.imgdb.cn/item/6786250bd0e0a243d4f42bac.png"></p><h6 id="我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"><a href="#我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。" class="headerlink" title="我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"></a>我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。</h6><h6 id="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"><a href="#下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。" class="headerlink" title="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"></a>下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">MapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提取每次点击事件的用户名</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.map( _.user ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现mapFunction接口</span></span><br><span class="line">    stream.map(<span class="keyword">new</span> <span class="type">UserExtractor</span>).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserExtractor</span> <span class="keyword">extends</span> <span class="title">MapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"><a href="#上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。" class="headerlink" title="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"></a>上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。</h6><h3 id="1-1-2过滤（filter）"><a href="#1-1-2过滤（filter）" class="headerlink" title="1.1.2过滤（filter）"></a>1.1.2过滤（filter）</h3><h5 id="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"><a href="#filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。" class="headerlink" title="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"></a>filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。</h5><p><img src="https://pic1.imgdb.cn/item/6786255dd0e0a243d4f42bd8.png"></p><h6 id="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。"><a href="#进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。" class="headerlink" title="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。"></a>进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。</h6><h6 id="案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。"><a href="#案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。" class="headerlink" title="案例需求：下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。"></a><strong>案例需求：</strong>下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFilterTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过滤出用户为Marry的所有点击事件</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.filter( _.user == <span class="string">&quot;hkj&quot;</span> ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现FilterFunction</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">UserFilter</span> ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserFilter</span> <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.user == <span class="string">&quot;Bob&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-1-3-扁平映射（flatMap）"><a href="#1-1-3-扁平映射（flatMap）" class="headerlink" title="1.1.3 扁平映射（flatMap）"></a>1.1.3 扁平映射（flatMap）</h4><h6 id="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"><a href="#flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。" class="headerlink" title="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"></a>flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。</h6><p><img src="https://pic1.imgdb.cn/item/6786259fd0e0a243d4f42bf4.png"></p><h6 id="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"><a href="#同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。" class="headerlink" title="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"></a>同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。</h6><h6 id="案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。"><a href="#案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。" class="headerlink" title="案例需求：如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。"></a><strong>案例需求：</strong>如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。</h6><h6 id="实现代码如下："><a href="#实现代码如下：" class="headerlink" title="实现代码如下："></a>实现代码如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FlatMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFlatMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试灵活输出形式</span></span><br><span class="line">    stream.flatMap(<span class="keyword">new</span> <span class="type">MyFlatMap</span>).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// lam表达式</span></span><br><span class="line">    <span class="keyword">val</span> stream1 = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line">    stream1.flatMap( value =&gt; value.split(<span class="string">&quot;,&quot;</span>) ).map(value =&gt; (value, <span class="number">1</span>)).print(<span class="string">&quot;stream1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义实现flatMapFunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">FlatMapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: <span class="type">Event</span>, out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 如果当前数据是hkj的点击事件那就直接输出user</span></span><br><span class="line">      <span class="keyword">if</span> (value.user == <span class="string">&quot;hkj&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 如果当前数据是Bob的点击事件，那么就输出user和url</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (value.user == <span class="string">&quot;Bob&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">        out.collect(value.url)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2聚合算子（Aggregation）"><a href="#1-2聚合算子（Aggregation）" class="headerlink" title="1.2聚合算子（Aggregation）"></a>1.2聚合算子（Aggregation）</h3><h5 id="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"><a href="#计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。" class="headerlink" title="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"></a>计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。</h5><h4 id="1-2-1-按键分区（keyBy）"><a href="#1-2-1-按键分区（keyBy）" class="headerlink" title="1.2.1 按键分区（keyBy）"></a>1.2.1 按键分区（keyBy）</h4><ul><li><h6 id="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"><a href="#对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。" class="headerlink" title="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"></a>对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。</h6></li><li><h6 id="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"><a href="#keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。" class="headerlink" title="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"></a>keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。</h6></li><li><h6 id="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"><a href="#基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。" class="headerlink" title="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"></a>基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。</h6></li></ul><p><img src="https://pic1.imgdb.cn/item/67862611d0e0a243d4f42c2f.png"></p><h6 id="在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。"><a href="#在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。" class="headerlink" title="在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。"></a>在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。</h6><h6 id="keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"><a href="#keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。" class="headerlink" title="keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"></a>keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。</h6><h6 id="我们可以以id作为key做一个分区操作，代码实现如下："><a href="#我们可以以id作为key做一个分区操作，代码实现如下：" class="headerlink" title="我们可以以id作为key做一个分区操作，代码实现如下："></a>我们可以以id作为key做一个分区操作，代码实现如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.<span class="type">KeySelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformKeyByTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.keyBy( <span class="keyword">new</span> <span class="type">MyKeySelector</span> )</span><br><span class="line">      .maxBy(<span class="string">&quot;timestamp&quot;</span>)</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//    stream.keyBy( _.user).print(&quot;lam&quot;)</span></span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyKeySelector</span> <span class="keyword">extends</span> <span class="title">KeySelector</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKey</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"><a href="#需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。" class="headerlink" title="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"></a>需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。</h6><h6 id="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"><a href="#KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。" class="headerlink" title="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"></a>KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。</h6><h4 id="1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）"><a href="#1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）" class="headerlink" title="1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）"></a>1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）</h4><h6 id="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："><a href="#有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：" class="headerlink" title="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："></a>有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：</h6><ol><li><h6 id="sum-：在输入流上，对指定的字段做叠加求和的操作。"><a href="#sum-：在输入流上，对指定的字段做叠加求和的操作。" class="headerlink" title="sum()：在输入流上，对指定的字段做叠加求和的操作。"></a>sum()：在输入流上，对指定的字段做叠加求和的操作。</h6></li><li><h6 id="min-：在输入流上，对指定的字段求最小值。"><a href="#min-：在输入流上，对指定的字段求最小值。" class="headerlink" title="min()：在输入流上，对指定的字段求最小值。"></a>min()：在输入流上，对指定的字段求最小值。</h6></li><li><h6 id="max-：在输入流上，对指定的字段求最大值。"><a href="#max-：在输入流上，对指定的字段求最大值。" class="headerlink" title="max()：在输入流上，对指定的字段求最大值。"></a>max()：在输入流上，对指定的字段求最大值。</h6></li><li><h6 id="minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。"><a href="#minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。" class="headerlink" title="minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。"></a>minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。</h6></li><li><h6 id="maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。"><a href="#maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。" class="headerlink" title="maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。"></a>maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。</h6></li></ol><h5 id="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"><a href="#简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。" class="headerlink" title="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"></a>简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。</h5><h6 id="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"><a href="#对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。" class="headerlink" title="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"></a>对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。</h6><h6 id="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"><a href="#如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。" class="headerlink" title="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"></a>如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransAggregation</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_2&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_3&quot;</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        stream.keyBy(e -&gt; e.id).max(<span class="string">&quot;vc&quot;</span>);    <span class="comment">// 指定字段名称</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"><a href="#简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。" class="headerlink" title="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"></a>简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。</h5><h5 id="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"><a href="#一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。" class="headerlink" title="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"></a>一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。</h5><h4 id="1-2-3-归约聚合（reduce）"><a href="#1-2-3-归约聚合（reduce）" class="headerlink" title="1.2.3 归约聚合（reduce）"></a>1.2.3 归约聚合（reduce）</h4><h5 id="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"><a href="#reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。" class="headerlink" title="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"></a>reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。</h5><h5 id="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"><a href="#reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。" class="headerlink" title="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"></a>reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。</h5><h5 id="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："><a href="#调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：" class="headerlink" title="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："></a>调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ReduceFunction</span>&lt;T&gt; <span class="keyword">extends</span> <span class="title class_">Function</span>, Serializable &#123;</span><br><span class="line">    T <span class="title function_">reduce</span><span class="params">(T value1, T value2)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"><a href="#ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。" class="headerlink" title="ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"></a>ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。</h5><h5 id="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"><a href="#我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。" class="headerlink" title="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"></a>我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。</h5><h5 id="为了方便后续使用，定义一个WaterSensorMapFunction："><a href="#为了方便后续使用，定义一个WaterSensorMapFunction：" class="headerlink" title="为了方便后续使用，定义一个WaterSensorMapFunction："></a>为了方便后续使用，定义一个WaterSensorMapFunction：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensorMapFunction</span> <span class="keyword">implements</span> <span class="title class_">MapFunction</span>&lt;String,WaterSensor&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> WaterSensor <span class="title function_">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] datas = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WaterSensor</span>(datas[<span class="number">0</span>],Long.valueOf(datas[<span class="number">1</span>]) ,Integer.valueOf(datas[<span class="number">2</span>]) );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="案例：使用reduce实现max和maxBy的功能。"><a href="#案例：使用reduce实现max和maxBy的功能。" class="headerlink" title="案例：使用reduce实现max和maxBy的功能。"></a>案例：使用reduce实现max和maxBy的功能。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">ReduceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformReduceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduce归约聚合,提取当前最活跃用户</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    data =&gt; true 的作用</span></span><br><span class="line"><span class="comment">//    1.keyBy 的作用：keyBy 是 Flink 中的一个算子，用于将数据流按照指定的键进行分组。</span></span><br><span class="line"><span class="comment">//                  分组后，相同键的数据会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    2.data =&gt; true 的含义：</span></span><br><span class="line"><span class="comment">//              这里的 data =&gt; true 是一个匿名函数，它对每条数据返回固定的 true 值。</span></span><br><span class="line"><span class="comment">//              由于所有数据的键都是 true，因此所有数据都会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    3.为什么需要 data =&gt; true：</span></span><br><span class="line"><span class="comment">//          在统计最活跃用户时，需要将所有用户的活跃度数据集中到一个分区中，才能进行比较和筛选。</span></span><br><span class="line"><span class="comment">//          如果不使用 keyBy(data =&gt; true)，数据会分散在多个分区中，无法直接进行比较。</span></span><br><span class="line"></span><br><span class="line">    stream.map( data =&gt; (data.user, <span class="number">1</span>L) )</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .reduce( <span class="keyword">new</span> <span class="type">MySum</span> )  <span class="comment">// 统计每个用户的活跃度</span></span><br><span class="line">      .keyBy(data =&gt; <span class="literal">true</span>)  <span class="comment">// 将所有数据按照同样的key分到同一个组中</span></span><br><span class="line">      .reduce( (state, data) =&gt; <span class="keyword">if</span> (data._2 &gt;= state._2) data <span class="keyword">else</span> state ) <span class="comment">// 选取当前最活跃的用户</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MySum</span> <span class="keyword">extends</span> <span class="title">ReduceFunction</span>[(<span class="type">String</span>, <span class="type">Long</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(value1: (<span class="type">String</span>, <span class="type">Long</span>), value2: (<span class="type">String</span>, <span class="type">Long</span>)): (<span class="type">String</span>, <span class="type">Long</span>) = (value1._1, value2._2 + value1._2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"><a href="#reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。" class="headerlink" title="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"></a>reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。</h5><h3 id="1-3-3-用户自定义函数（UDF）"><a href="#1-3-3-用户自定义函数（UDF）" class="headerlink" title="1.3.3 用户自定义函数（UDF）"></a>1.3.3 用户自定义函数（UDF）</h3><h5 id="用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"><a href="#用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。" class="headerlink" title="用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"></a>用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。</h5><h5 id="用户自定义函数分为：函数类、匿名函数、富函数类。"><a href="#用户自定义函数分为：函数类、匿名函数、富函数类。" class="headerlink" title="用户自定义函数分为：函数类、匿名函数、富函数类。"></a>用户自定义函数分为：函数类、匿名函数、富函数类。</h5><h4 id="需求：用来从用户的点击数据中筛选包含“sensor-1”的内容："><a href="#需求：用来从用户的点击数据中筛选包含“sensor-1”的内容：" class="headerlink" title="需求：用来从用户的点击数据中筛选包含“sensor_1”的内容："></a><strong>需求：</strong>用来从用户的点击数据中筛选包含“sensor_1”的内容：</h4><h4 id="方式一：实现FilterFunction接口"><a href="#方式一：实现FilterFunction接口" class="headerlink" title="方式一：实现FilterFunction接口"></a><strong>方式一：</strong>实现FilterFunction接口</h4><h4 id="方式二：通过匿名类来实现FilterFunction接口："><a href="#方式二：通过匿名类来实现FilterFunction接口：" class="headerlink" title="方式二：通过匿名类来实现FilterFunction接口："></a><strong>方式二：</strong>通过匿名类来实现FilterFunction接口：</h4><h5 id="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"><a href="#方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。" class="headerlink" title="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"></a><strong>方式二的优化：</strong>为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。</h5><h5 id="方式三：采用匿名函数（Lambda）"><a href="#方式三：采用匿名函数（Lambda）" class="headerlink" title="方式三：采用匿名函数（Lambda）"></a><strong>方式三：</strong>采用匿名函数（Lambda）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformUDFTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试UDF的用法,筛选url中包含某个关键字hkjcpdd的Event事件</span></span><br><span class="line">    <span class="comment">// 1.实现一个自定义的函数类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">MyFilterFunction</span>(<span class="string">&quot;hkjcpdd&quot;</span>) ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 使用匿名类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">FilterFunction</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(<span class="string">&quot;hkjmjj&quot;</span>)</span><br><span class="line">    &#125; ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 使用lambda表达式</span></span><br><span class="line">    stream.filter( _.url.contains(<span class="string">&quot;hkjmjj&quot;</span>) ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义个filterfunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFilterFunction</span>(<span class="params">keyWord: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// contains是指是否包含某些字段</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(keyWord)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-2-富函数类（Rich-Function-Classes）"><a href="#1-3-2-富函数类（Rich-Function-Classes）" class="headerlink" title="1.3.2 富函数类（Rich Function Classes）"></a>1.3.2 富函数类（Rich Function Classes）</h4><h6 id="“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"><a href="#“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。" class="headerlink" title="“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"></a>“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。</h6><h6 id="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"><a href="#与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。" class="headerlink" title="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"></a>与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</h6><h6 id="Rich-Function有生命周期的概念。典型的生命周期方法有："><a href="#Rich-Function有生命周期的概念。典型的生命周期方法有：" class="headerlink" title="Rich Function有生命周期的概念。典型的生命周期方法有："></a>Rich Function有生命周期的概念。典型的生命周期方法有：</h6><ul><li><h6 id="open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。"><a href="#open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。" class="headerlink" title="open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。"></a>open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。</h6></li><li><h6 id="close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"><a href="#close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。" class="headerlink" title="close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"></a>close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。</h6></li></ul><h6 id="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。"><a href="#需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。" class="headerlink" title="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。"></a>需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。</h6><h6 id="来看一个例子说明："><a href="#来看一个例子说明：" class="headerlink" title="来看一个例子说明："></a>来看一个例子说明：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformRichFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义一个RichMapFunction，测试复函数类的功能</span></span><br><span class="line">    stream.map( <span class="keyword">new</span> <span class="type">MyRichMap</span>() ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRichMap</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichMapFunction</span>[<span class="type">Event</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务开始&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">Long</span> = value.timestamp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务结束&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-3-4-物理分区算子（Physical-Partitioning）"><a href="#5-3-4-物理分区算子（Physical-Partitioning）" class="headerlink" title="5.3.4 物理分区算子（Physical Partitioning）"></a>5.3.4 物理分区算子（Physical Partitioning）</h3><h5 id="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"><a href="#常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。" class="headerlink" title="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"></a>常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。</h5><h3 id="1-4-1-随机分区（shuffle）"><a href="#1-4-1-随机分区（shuffle）" class="headerlink" title="1.4.1 随机分区（shuffle）"></a>1.4.1 随机分区（shuffle）</h3><h5 id="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。"><a href="#最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。" class="headerlink" title="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。"></a>最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。</h5><h5 id="随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。"><a href="#随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。" class="headerlink" title="随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。"></a>随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。</h5><p><img src="https://pic1.imgdb.cn/item/67862896d0e0a243d4f42d1b.png"></p><h5 id="经过随机分区之后，得到的依然是一个DataStream。"><a href="#经过随机分区之后，得到的依然是一个DataStream。" class="headerlink" title="经过随机分区之后，得到的依然是一个DataStream。"></a>经过随机分区之后，得到的依然是一个DataStream。</h5><h5 id="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"><a href="#我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。" class="headerlink" title="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"></a>我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShuffleExample</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"> env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; stream = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>);;</span><br><span class="line"></span><br><span class="line">        stream.shuffle().print()</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-4-2-轮询分区（Round-Robin）"><a href="#1-4-2-轮询分区（Round-Robin）" class="headerlink" title="1.4.2 轮询分区（Round-Robin）"></a>1.4.2 轮询分区（Round-Robin）</h4><h5 id="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"><a href="#轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。" class="headerlink" title="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"></a>轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。</h5><p><img src="https://pic1.imgdb.cn/item/678628c7d0e0a243d4f42d3e.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rebalance()</span><br></pre></td></tr></table></figure><h4 id="1-4-3-重缩放分区（rescale）"><a href="#1-4-3-重缩放分区（rescale）" class="headerlink" title="1.4.3 重缩放分区（rescale）"></a>1.4.3 重缩放分区（rescale）</h4><h5 id="重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"><a href="#重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。" class="headerlink" title="重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"></a>重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。</h5><p><img src="https://pic1.imgdb.cn/item/678628e9d0e0a243d4f42d59.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rescale()</span><br></pre></td></tr></table></figure><h4 id="1-4-4-广播（broadcast）"><a href="#1-4-4-广播（broadcast）" class="headerlink" title="1.4.4 广播（broadcast）"></a>1.4.4 广播（broadcast）</h4><h5 id="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。"><a href="#这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。" class="headerlink" title="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。"></a>这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.broadcast()</span><br></pre></td></tr></table></figure><h4 id="1-4-5-全局分区（global）"><a href="#1-4-5-全局分区（global）" class="headerlink" title="1.4.5 全局分区（global）"></a>1.4.5 全局分区（global）</h4><h5 id="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"><a href="#全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。" class="headerlink" title="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"></a>全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.global()</span><br></pre></td></tr></table></figure><h4 id="1-4-6-自定义分区（Custom）"><a href="#1-4-6-自定义分区（Custom）" class="headerlink" title="1.4.6 自定义分区（Custom）"></a>1.4.6 自定义分区（Custom）</h4><h5 id="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。"><a href="#当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。" class="headerlink" title="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。"></a>当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。</h5><h4 id="1-自定义分区器"><a href="#1-自定义分区器" class="headerlink" title="1)自定义分区器"></a>1)自定义分区器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">ParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Calendar</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//SourceFunction是并行度1的</span></span><br><span class="line"><span class="comment">//ParallelSourceFunction就是并行多的</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClickSource</span> <span class="keyword">extends</span> <span class="title">ParallelSourceFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 标志位</span></span><br><span class="line">  <span class="keyword">var</span> running = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Event</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 随机数生成器</span></span><br><span class="line">    <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">    <span class="comment">// 定义数据随机选择的范围</span></span><br><span class="line">    <span class="keyword">val</span> users = <span class="type">Array</span>(<span class="string">&quot;Marry&quot;</span>, <span class="string">&quot;Hkj&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Cary&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> urls = <span class="type">Array</span>(<span class="string">&quot;./home&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="string">&quot;./fav&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>,<span class="string">&quot;./prod?id=3&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键流程， 用标志位作为循环的判断条件，不停的发出数据</span></span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line">      <span class="keyword">val</span> event = <span class="type">Event</span>(</span><br><span class="line">        users(random.nextInt(users.length)),</span><br><span class="line">        urls(random.nextInt(users.length)),</span><br><span class="line">        <span class="type">Calendar</span>.getInstance.getTimeInMillis</span><br><span class="line">      )</span><br><span class="line"><span class="comment">//      // 为要发送的数据分配时间戳</span></span><br><span class="line"><span class="comment">//      ctx.collectWithTimestamp(event, event.timestamp)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//      // 向下游直接发送水位线</span></span><br><span class="line"><span class="comment">//      ctx.emitWatermark(new Watermark(event.timestamp - 1L))</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 调用ctx的方法向下游发送数据</span></span><br><span class="line">      ctx.collect(event)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 每隔一秒发送过一条数据</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2）使用自定义分区器"><a href="#2）使用自定义分区器" class="headerlink" title="2）使用自定义分区器"></a>2）使用自定义分区器</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PartitionReblanceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取自定义的数据流</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">ClickSource</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 轮询重分区之后打印输出</span></span><br><span class="line">    stream.rebalance.print(<span class="string">&quot;shuffle&quot;</span>).setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-5分流"><a href="#1-3-5分流" class="headerlink" title="1.3.5分流"></a>1.3.5分流</h3><h4 id="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"><a href="#所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。" class="headerlink" title="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"></a>所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。</h4><p><img src="https://pic1.imgdb.cn/item/6786298ad0e0a243d4f42daf.png"></p><h4 id="1-3-5-1-简单实现"><a href="#1-3-5-1-简单实现" class="headerlink" title="1.3.5.1 简单实现"></a>1.3.5.1 简单实现</h4><p>其实根据条件筛选数据的需求，本身非常容易实现：只要针对同一条流多次独立调用.filter()方法进行筛选，就可以得到拆分之后的流了。</p><p><strong>案例需求：</strong>读取一个整数数字流，将数据流划分为奇数流和偶数流。</p><p><strong>代码实现：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByFilter</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">      </span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                                                           .map(Integer::valueOf);</span><br><span class="line">        <span class="comment">//将ds 分为两个流 ，一个是奇数流，一个是偶数流</span></span><br><span class="line">        <span class="comment">//使用filter 过滤两次</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds1 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">0</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds2 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;偶数&quot;</span>);</span><br><span class="line">        ds2.print(<span class="string">&quot;奇数&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"><a href="#这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？" class="headerlink" title="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"></a>这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？</h5><h4 id="1-3-5-2-使用侧输出流"><a href="#1-3-5-2-使用侧输出流" class="headerlink" title="1.3.5.2 使用侧输出流"></a>1.3.5.2 使用侧输出流</h4><p>关于处理函数中侧输出流的用法，我们已经在7.5节做了详细介绍。简单来说，只需要调用上下文ctx的.output()方法，就可以输出任意类型的数据了。而侧输出流的标记和提取，都离不开一个“输出标签”（OutputTag），指定了侧输出流的id和类型。</p><p><strong>代码实现：</strong>将WaterSensor按照Id类型进行分流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByOutputTag</span> &#123;    </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">              .map(<span class="keyword">new</span> <span class="title class_">WaterSensorMapFunction</span>());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s1 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s1&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s2 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s2&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">       <span class="comment">//返回的都是主流</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds1 = ds.process(<span class="keyword">new</span> <span class="title class_">ProcessFunction</span>&lt;WaterSensor, WaterSensor&gt;()</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="string">&quot;s1&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s1, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;s2&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s2, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">//主流</span></span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;主流，非s1,s2的传感器&quot;</span>);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s1DS = ds1.getSideOutput(s1);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s2DS = ds1.getSideOutput(s2);</span><br><span class="line"></span><br><span class="line">        s1DS.printToErr(<span class="string">&quot;s1&quot;</span>);</span><br><span class="line">        s2DS.printToErr(<span class="string">&quot;s2&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-6-基本合流操作"><a href="#1-3-6-基本合流操作" class="headerlink" title="1.3.6 基本合流操作"></a><strong>1.3.6</strong> <strong>基本合流操作</strong></h3><h5 id="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"><a href="#在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。" class="headerlink" title="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"></a>在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。</h5><h3 id="1-3-6-1"><a href="#1-3-6-1" class="headerlink" title="1.3.6.1"></a>1.3.6.1</h3><h4 id="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"><a href="#最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。" class="headerlink" title="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"></a>最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。</h4><p><img src="https://pic1.imgdb.cn/item/6786448fd0e0a243d4f434cf.png"></p><h5 id="在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："><a href="#在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：" class="headerlink" title="在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："></a>在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream1.union(stream2, stream3, ...)</span><br></pre></td></tr></table></figure><h4 id="注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"><a href="#注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。" class="headerlink" title="注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"></a>注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。</h4><p><strong>代码实现：</strong>我们可以用下面的代码做一个简单测试：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UnionExample</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds1 = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds2 = env.fromElements(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; ds3 = env.fromElements(<span class="string">&quot;2&quot;</span>, <span class="string">&quot;2&quot;</span>, <span class="string">&quot;3&quot;</span>);</span><br><span class="line"></span><br><span class="line">        ds1.union(ds2,ds3.map(Integer::valueOf))</span><br><span class="line">           .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-6-2-连接（Connect）"><a href="#1-3-6-2-连接（Connect）" class="headerlink" title="1.3.6.2 连接（Connect）"></a>1.3.6.2 连接（Connect）</h4><h4 id="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"><a href="#流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。" class="headerlink" title="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"></a>流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。</h4><h5 id="1）连接流（ConnectedStreams）"><a href="#1）连接流（ConnectedStreams）" class="headerlink" title="1）连接流（ConnectedStreams）"></a>1）连接流（ConnectedStreams）</h5><p><img src="https://pic1.imgdb.cn/item/67864547d0e0a243d4f4350b.png"></p><p><strong>代码实现：</strong>需要分为两步：首先基于一条DataStream调用.connect()方法，传入另外一条DataStream作为参数，将两条流连接起来，得到一个ConnectedStreams；然后再调用同处理方法得到DataStream。这里可以的调用的同处理方法有.map()&#x2F;.flatMap()，以及.process()方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;Integer&gt; source1 = env.fromElements(1, 2, 3);</span></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;String&gt; source2 = env.fromElements(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);</span></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; source1 = env</span><br><span class="line">                .socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(i -&gt; Integer.parseInt(i));</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source2 = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">8888</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 使用 connect 合流</span></span><br><span class="line"><span class="comment">         * 1、一次只能连接 2条流</span></span><br><span class="line"><span class="comment">         * 2、流的数据类型可以不一样</span></span><br><span class="line"><span class="comment">         * 3、 连接后可以调用 map、flatmap、process来处理，但是各处理各的</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ConnectedStreams&lt;Integer, String&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connect.map(<span class="keyword">new</span> <span class="title class_">CoMapFunction</span>&lt;Integer, String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map1</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于数字流:&quot;</span> + value.toString();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于字母流:&quot;</span> + value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码中，ConnectedStreams有两个类型参数，分别表示内部包含的两条流各自的数据类型；由于需要“一国两制”，因此调用.map()方法时传入的不再是一个简单的MapFunction，而是一个CoMapFunction，表示分别对两条流中的数据执行map操作。这个接口有三个类型参数，依次表示第一条流、第二条流，以及合并后的流中的数据类型。需要实现的方法也非常直白：.map1()就是对第一条流中数据的map操作，.map2()则是针对第二条流。</p><p>2）CoProcessFunction</p><p>与CoMapFunction类似，如果是调用.map()就需要传入一个CoMapFunction，需要实现map1()、map2()两个方法；而调用.process()时，传入的则是一个CoProcessFunction。它也是“处理函数”家族中的一员，用法非常相似。它需要实现的就是processElement1()、processElement2()两个方法，在每个数据到来时，会根据来源的流调用其中的一个方法进行处理。</p><p>值得一提的是，ConnectedStreams也可以直接调用.keyBy()进行按键分区的操作，得到的还是一个ConnectedStreams：</p><p>connectedStreams.keyBy(keySelector1, keySelector2);</p><p>这里传入两个参数keySelector1和keySelector2，是两条流中各自的键选择器；当然也可以直接传入键的位置值（keyPosition），或者键的字段名（field），这与普通的keyBy用法完全一致。ConnectedStreams进行keyBy操作，其实就是把两条流中key相同的数据放到了一起，然后针对来源的流再做各自处理，这在一些场景下非常有用。</p><p>案例需求：连接两条流，输出能根据id匹配上的数据（类似inner join效果）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectKeybyDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Tuple2&lt;Integer, String&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a1&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a2&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">2</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa1&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa2&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="string">&quot;bb&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="string">&quot;cc&quot;</span>, <span class="number">1</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 多并行度下，需要根据 关联条件 进行keyby，才能保证key相同的数据到一起去，才能匹配上</span></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connectKey = connect.keyBy(s1 -&gt; s1.f0, s2 -&gt; s2.f0);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connectKey.process(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">CoProcessFunction</span>&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 定义 HashMap，缓存来过的数据，key=id，value=list&lt;数据&gt;</span></span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple2&lt;Integer, String&gt;&gt;&gt; s1Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt;&gt; s2Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement1</span><span class="params">(Tuple2&lt;Integer, String&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s1数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple2&lt;Integer, String&gt;&gt; s1Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s1Values.add(value);</span><br><span class="line">                            s1Cache.put(id, s1Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s1Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s2的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple3&lt;Integer, String, Integer&gt; s2Element : s2Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + value + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + s2Element);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement2</span><span class="params">(Tuple3&lt;Integer, String, Integer&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s2数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; s2Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s2Values.add(value);</span><br><span class="line">                            s2Cache.put(id, s2Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s2Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s1的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple2&lt;Integer, String&gt; s1Element : s1Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + s1Element + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + value);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4输出算子"><a href="#1-4输出算子" class="headerlink" title="1.4输出算子"></a>1.4输出算子</h3><h4 id="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"><a href="#Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。" class="headerlink" title="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"></a>Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。</h4><p><img src="https://pic1.imgdb.cn/item/678645c8d0e0a243d4f43592.png"></p><h3 id="1-4-1-连接到外部系统"><a href="#1-4-1-连接到外部系统" class="headerlink" title="1.4.1 连接到外部系统"></a><strong>1.4.1</strong> <strong>连接到外部系统</strong></h3><h4 id="Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"><a href="#Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。" class="headerlink" title="Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"></a>Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。</h4><h5 id="Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。"><a href="#Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。" class="headerlink" title="Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。"></a>Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。</h5><h5 id="stream-addSink-new-SinkFunction-…"><a href="#stream-addSink-new-SinkFunction-…" class="headerlink" title="stream.addSink(new SinkFunction(…));"></a>stream.addSink(new SinkFunction(…));</h5><h5 id="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"><a href="#addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。" class="headerlink" title="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"></a>addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。</h5><h5 id="Flink1-12开始，同样重构了Sink架构，"><a href="#Flink1-12开始，同样重构了Sink架构，" class="headerlink" title="Flink1.12开始，同样重构了Sink架构，"></a>Flink1.12开始，同样重构了Sink架构，</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.sinkTo(…)</span><br></pre></td></tr></table></figure><h4 id="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："><a href="#当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：" class="headerlink" title="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："></a>当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：</h4><p><img src="https://pic1.imgdb.cn/item/678645f5d0e0a243d4f435e2.png"></p><h4 id="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"><a href="#我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。" class="headerlink" title="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"></a>我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。</h4><h4 id="除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"><a href="#除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。" class="headerlink" title="除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"></a>除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。</h4><p><img src="https://pic1.imgdb.cn/item/6786460cd0e0a243d4f43614.png"></p><h4 id="除此以外，就需要用户自定义实现sink连接器了。"><a href="#除此以外，就需要用户自定义实现sink连接器了。" class="headerlink" title="除此以外，就需要用户自定义实现sink连接器了。"></a>除此以外，就需要用户自定义实现sink连接器了。</h4><h3 id="5-4-2-输出到文件"><a href="#5-4-2-输出到文件" class="headerlink" title="5.4.2 输出到文件"></a><strong>5.4.2</strong> <strong>输出到文件</strong></h3><h4 id="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"><a href="#Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。" class="headerlink" title="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"></a>Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。</h4><h4 id="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："><a href="#FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：" class="headerlink" title="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："></a>FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：</h4><ul><li>行编码： FileSink.forRowFormat（basePath，rowEncoder）。</li><li>批量编码： FileSink.forBulkFormat（basePath，bulkWriterFactory）。</li></ul><h4 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ch.qos.logback.core.util.<span class="type">TimeUtil</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringEncoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.<span class="type">Path</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.<span class="type">StreamingFileSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToFileTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 直接以文本形式分布式的写入到文件中</span></span><br><span class="line">    <span class="comment">// SimpleStringEncoder作用是将String转成char方便写入</span></span><br><span class="line">    <span class="keyword">val</span> fileSink = <span class="type">StreamingFileSink</span></span><br><span class="line">      .forRowFormat(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;./output&quot;</span>), <span class="keyword">new</span> <span class="type">SimpleStringEncoder</span>[<span class="type">String</span>](<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    stream.broadcast.map(_.toString).addSink(fileSink)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-3-输出到Kafka"><a href="#1-4-3-输出到Kafka" class="headerlink" title="1.4.3 输出到Kafka"></a><strong>1.4.3</strong> 输出到Kafka</h3><h3 id="（1）添加Kafka-连接器依赖"><a href="#（1）添加Kafka-连接器依赖" class="headerlink" title="（1）添加Kafka 连接器依赖"></a>（1）添加Kafka 连接器依赖</h3><h3 id="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"><a href="#由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。" class="headerlink" title="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"></a>由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。</h3><h3 id="（2）启动Kafka集群"><a href="#（2）启动Kafka集群" class="headerlink" title="（2）启动Kafka集群"></a>（2）启动Kafka集群</h3><h3 id="（3）编写输出到Kafka的示例代码"><a href="#（3）编写输出到Kafka的示例代码" class="headerlink" title="（3）编写输出到Kafka的示例代码"></a>（3）编写输出到Kafka的示例代码</h3><h3 id="输出无key的record"><a href="#输出无key的record" class="headerlink" title="输出无key的record:"></a>输出无key的record:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.&#123;<span class="type">FlinkKafkaConsumer</span>, <span class="type">FlinkKafkaProducer</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;lhxcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="comment">// trim就是去空格</span></span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong).toString</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 讲数据写入到kafka</span></span><br><span class="line">    stream.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](<span class="string">&quot;master:9092&quot;</span>, <span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="然后开一个消费者查看是否到数据"><a href="#然后开一个消费者查看是否到数据" class="headerlink" title="然后开一个消费者查看是否到数据"></a>然后开一个消费者查看是否到数据</h3><h3 id="1-4-4-输出到MySQL（JDBC）"><a href="#1-4-4-输出到MySQL（JDBC）" class="headerlink" title="1.4.4 输出到MySQL（JDBC）"></a>1.4.4 输出到MySQL（JDBC）</h3><h3 id="写入数据的MySQL的测试步骤如下。"><a href="#写入数据的MySQL的测试步骤如下。" class="headerlink" title="写入数据的MySQL的测试步骤如下。"></a>写入数据的MySQL的测试步骤如下。</h3><h3 id="（1）添加依赖"><a href="#（1）添加依赖" class="headerlink" title="（1）添加依赖"></a>（1）添加依赖</h3><h3 id="添加MySQL驱动："><a href="#添加MySQL驱动：" class="headerlink" title="添加MySQL驱动："></a>添加MySQL驱动：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径："><a href="#官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径：" class="headerlink" title="官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径："></a>官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>apache-snapshots<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>apache snapshots<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.apache.org/content/repositories/snapshots/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="添加依赖："><a href="#添加依赖：" class="headerlink" title="添加依赖："></a>添加依赖：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.17-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："><a href="#如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：" class="headerlink" title="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："></a>如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">            &lt;id&gt;aliyunmaven&lt;/id&gt;</span><br><span class="line">            &lt;mirrorOf&gt;*,!apache-snapshots&lt;/mirrorOf&gt;</span><br><span class="line">            &lt;name&gt;阿里云公共仓库&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;https:<span class="comment">//maven.aliyun.com/repository/public&lt;/url&gt;</span></span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）启动MySQL，在test库下建表ws"><a href="#（2）启动MySQL，在test库下建表ws" class="headerlink" title="（2）启动MySQL，在test库下建表ws"></a>（2）启动MySQL，在test库下建表ws</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span>     </span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `ws` (</span><br><span class="line">  `id` <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `ts` <span class="type">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `vc` <span class="type">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (`id`)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8</span><br></pre></td></tr></table></figure><h4 id="（3）编写输出到MySQL的示例代码"><a href="#（3）编写输出到MySQL的示例代码" class="headerlink" title="（3）编写输出到MySQL的示例代码"></a>（3）编写输出到MySQL的示例代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.&#123;<span class="type">JdbcConnectionOptions</span>, <span class="type">JdbcSink</span>, <span class="type">JdbcStatementBuilder</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">PreparedStatement</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToMysqlTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    stream.addSink(<span class="type">JdbcSink</span>.sink(</span><br><span class="line">      <span class="string">&quot;insert into shop (name, area, dizhi, price) values(?, ?, ?, ?)&quot;</span>, <span class="comment">// 定义写入Mysql的语句</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcStatementBuilder</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(t: <span class="type">PreparedStatement</span>, u: <span class="type">Event</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          t.setString(<span class="number">1</span>, u.user)</span><br><span class="line">          t.setString(<span class="number">2</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">3</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">4</span>, u.url)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcConnectionOptions</span>.<span class="type">JdbcConnectionOptionsBuilder</span>()</span><br><span class="line">        .withUrl(<span class="string">&quot;jdbc:mysql://master:3306/test?useSSL=false&quot;</span>)</span><br><span class="line">        .withDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">        .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">        .withPassword(<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">        .build()</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"><a href="#（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。" class="headerlink" title="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"></a>（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。</h4><h3 id="1-4-5-自定义Sink输出"><a href="#1-4-5-自定义Sink输出" class="headerlink" title="1.4.5 自定义Sink输出"></a><strong>1.4.5</strong> <strong>自定义Sink输出</strong></h3><h4 id="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。"><a href="#如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。" class="headerlink" title="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。"></a>如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> <span class="type">MySinkFunction</span>&lt;<span class="type">String</span>&gt;());</span><br></pre></td></tr></table></figure><h4 id="在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"><a href="#在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。" class="headerlink" title="在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"></a>在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。</h4><h4 id="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"><a href="#这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。" class="headerlink" title="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"></a>这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。</h4>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flink/">Flink</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flink/">Flink</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/pd14.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数据可视化学习路线</title>
      <link>https://bigdata-yx.github.io/posts/1083.html</link>
      <guid>https://bigdata-yx.github.io/posts/1083.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:12:05 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;大数据数据可视化学习路线&quot;&gt;&lt;a href=&quot;#大数据数据可视化学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据数据可视化学习路线&quot;&gt;&lt;/a&gt;大数据数据可视化学习路线&lt;/h1&gt;&lt;h5 id=&quot;前提：具备一定的Python基础&quot;&gt;&lt;a href</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="大数据数据可视化学习路线"><a href="#大数据数据可视化学习路线" class="headerlink" title="大数据数据可视化学习路线"></a>大数据数据可视化学习路线</h1><h5 id="前提：具备一定的Python基础"><a href="#前提：具备一定的Python基础" class="headerlink" title="前提：具备一定的Python基础"></a>前提：具备一定的Python基础</h5><h2 id="1-python的基础学习，以及进阶学习"><a href="#1-python的基础学习，以及进阶学习" class="headerlink" title="1.python的基础学习，以及进阶学习"></a>1.python的基础学习，以及进阶学习</h2><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href>https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="2-numpy、pandas的入门学习（先学numpy）"><a href="#2-numpy、pandas的入门学习（先学numpy）" class="headerlink" title="2.numpy、pandas的入门学习（先学numpy）"></a>2.numpy、pandas的入门学习（先学numpy）</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="3-数据可视化matplotlib库的基础绘图"><a href="#3-数据可视化matplotlib库的基础绘图" class="headerlink" title="3.数据可视化matplotlib库的基础绘图"></a>3.数据可视化matplotlib库的基础绘图</h3><p>【千锋教育python数据可视化Matplotlib绘图教程，Matplotlib柱状图｜Matplotlib动态图｜Matplotlib散点图】<a href>https://www.bilibili.com/video/BV1nM411m7Cf?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看"><a href="#4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看" class="headerlink" title="4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)"></a>4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="5-数据可视化pyecharts可交互式基础绘图"><a href="#5-数据可视化pyecharts可交互式基础绘图" class="headerlink" title="5.数据可视化pyecharts可交互式基础绘图"></a>5.数据可视化pyecharts可交互式基础绘图</h3><p>【千锋教育PyEcharts数据可视化快速入门教程，大数据分析Python交互绘图实用利器】<a href>https://www.bilibili.com/video/BV1nM411F7GT?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="6-数据可视化tableau图标绘图"><a href="#6-数据可视化tableau图标绘图" class="headerlink" title="6.数据可视化tableau图标绘图"></a>6.数据可视化tableau图标绘图</h3><p>【【Tableau教程】Tableau零基础教程，带你解锁当下最受欢迎的数据可视化软件】<a href>https://www.bilibili.com/video/BV1E4411B7ef?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="7-数据交互式可视化进阶绘图前提基础html"><a href="#7-数据交互式可视化进阶绘图前提基础html" class="headerlink" title="7.数据交互式可视化进阶绘图前提基础html"></a>7.数据交互式可视化进阶绘图前提基础html</h3><p>【黑马程序员pink老师前端入门教程，零基础必看的h5(html5)+css3+移动端前端视频教程】<a href>https://www.bilibili.com/video/BV14J4114768?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="8-数据可视化echarts可交互式进阶绘图"><a href="#8-数据可视化echarts可交互式进阶绘图" class="headerlink" title="8.数据可视化echarts可交互式进阶绘图"></a>8.数据可视化echarts可交互式进阶绘图</h3><p>【电商平台数据可视化实时监控系统-Echarts-vue项目综合练习-pink老师推荐(持续更新)素材已经更新】<a href>https://www.bilibili.com/video/BV1bh41197p8?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="9-使用vue进行数据可视化前提基础Node-js"><a href="#9-使用vue进行数据可视化前提基础Node-js" class="headerlink" title="9.使用vue进行数据可视化前提基础Node.js"></a>9.使用vue进行数据可视化前提基础Node.js</h3><p>【黑马程序员Node.js全套入门教程，nodejs新教程含es6模块化+npm+express+webpack+promise等_Nodejs实战案例详解】<a href>https://www.bilibili.com/video/BV1a34y167AZ?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="10-使用vue进行数据可视化"><a href="#10-使用vue进行数据可视化" class="headerlink" title="10.使用vue进行数据可视化"></a>10.使用vue进行数据可视化</h3><p>【千锋Echarts+Vue3.0数据可视化项目构建_入门必备前端项目实战教程】<a href>https://www.bilibili.com/video/BV14u411D7qK?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/">学习路线</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/">学习路线</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/1083.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>集群搭建学习路线</title>
      <link>https://bigdata-yx.github.io/posts/1081.html</link>
      <guid>https://bigdata-yx.github.io/posts/1081.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:12:05 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;大数据集群搭建学习路线&quot;&gt;&lt;a href=&quot;#大数据集群搭建学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据集群搭建学习路线&quot;&gt;&lt;/a&gt;大数据集群搭建学习路线&lt;/h1&gt;&lt;h3 id=&quot;前提：熟练使用Linux的命令及其操作&quot;&gt;&lt;a href=</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="大数据集群搭建学习路线"><a href="#大数据集群搭建学习路线" class="headerlink" title="大数据集群搭建学习路线"></a>大数据集群搭建学习路线</h1><h3 id="前提：熟练使用Linux的命令及其操作"><a href="#前提：熟练使用Linux的命令及其操作" class="headerlink" title="前提：熟练使用Linux的命令及其操作"></a>前提：熟练使用Linux的命令及其操作</h3><h3 id="0-CentosLinux的基操"><a href="#0-CentosLinux的基操" class="headerlink" title="0.CentosLinux的基操"></a>0.CentosLinux的基操</h3><p>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="1-Mysql数据库的入门学习"><a href="#1-Mysql数据库的入门学习" class="headerlink" title="1.Mysql数据库的入门学习"></a>1.Mysql数据库的入门学习</h3><p>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="2-Hadoop集群的搭建及其使用和Hive的使用"><a href="#2-Hadoop集群的搭建及其使用和Hive的使用" class="headerlink" title="2.Hadoop集群的搭建及其使用和Hive的使用"></a>2.Hadoop集群的搭建及其使用和Hive的使用</h3><p>【黑马程序员大数据入门到实战教程，大数据开发必会的Hadoop、Hive，云平台实战项目全套一网打尽】<a href="https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="3-Java的基础学习（上部）"><a href="#3-Java的基础学习（上部）" class="headerlink" title="3.Java的基础学习（上部）"></a>3.Java的基础学习（上部）</h3><p>【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="4-scala语言的学习"><a href="#4-scala语言的学习" class="headerlink" title="4.scala语言的学习"></a>4.scala语言的学习</h3><p>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="5-Python基础的学习"><a href="#5-Python基础的学习" class="headerlink" title="5.Python基础的学习"></a>5.Python基础的学习</h3><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="6-zookeeper组件的学习"><a href="#6-zookeeper组件的学习" class="headerlink" title="6.zookeeper组件的学习"></a>6.zookeeper组件的学习</h3><p>【黑马程序员Zookeeper视频教程，快速入门zookeeper技术】<a href="https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="7-kafka组件的利用"><a href="#7-kafka组件的利用" class="headerlink" title="7.kafka组件的利用"></a>7.kafka组件的利用</h3><p>【尚硅谷Kafka教程，2024新版kafka视频，零基础入门到实战】<a href="https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="8-Flume的使用"><a href="#8-Flume的使用" class="headerlink" title="8.Flume的使用"></a>8.Flume的使用</h3><p>【【海牛大数据】Flume教程（学Flume看这个一套就够了，从基础到扩展到实战案例到组件配合全都有）】<a href="https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="9-Sqoop的运用"><a href="#9-Sqoop的运用" class="headerlink" title="9.Sqoop的运用"></a>9.Sqoop的运用</h3><p>【【海牛大数据】Sqoop教程（命令详解、各组件融合、实战案例）】<a href="https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="10-Spark的搭建"><a href="#10-Spark的搭建" class="headerlink" title="10.Spark的搭建"></a>10.Spark的搭建</h3><h4 id="Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）"><a href="#Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）" class="headerlink" title="Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）"></a>Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）</h4><p><a href="https://blog.csdn.net/JunLeon/article/details/123625680">(超详细) Spark环境搭建（Local模式、 StandAlone模式、Spark On Yarn模式）-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_44480968/article/details/119580363">Spark配置高可用（HA）_spark ha-CSDN博客</a></p><h3 id="11-Flink搭建及其使用-由于python版的使用还不是特别完整"><a href="#11-Flink搭建及其使用-由于python版的使用还不是特别完整" class="headerlink" title="11.Flink搭建及其使用(由于python版的使用还不是特别完整)"></a>11.Flink搭建及其使用(由于python版的使用还不是特别完整)</h3><h4 id="scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="scala版：【【尚硅谷】Flink1.13教程（Scala版）】https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>scala版：【【尚硅谷】Flink1.13教程（Scala版）】<a href="https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h4 id="java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】<a href="https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h3 id="11-Redis的搭建及其使用"><a href="#11-Redis的搭建及其使用" class="headerlink" title="11.Redis的搭建及其使用"></a>11.Redis的搭建及其使用</h3><p>【黑马程序员Redis入门到实战教程，深度透析redis底层原理+redis分布式锁+企业解决方案+黑马点评实战项目】<a href="https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="12-Hbase的搭建及其使用"><a href="#12-Hbase的搭建及其使用" class="headerlink" title="12.Hbase的搭建及其使用"></a>12.Hbase的搭建及其使用</h3><p>【【好程序员】大数据全新分布式存储HBase精品课程】<a href="https://www.bilibili.com/video/BV1RM411i7XM?p=15&vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1RM411i7XM?p=15&amp;vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="13-Azkaban的搭建及其使用"><a href="#13-Azkaban的搭建及其使用" class="headerlink" title="13.Azkaban的搭建及其使用"></a>13.Azkaban的搭建及其使用</h3><p>【尚硅谷大数据Azkaban 3.x教程（全新发布）】<a href="https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/">学习路线</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/">学习路线</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/1081.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数据处理学习路线</title>
      <link>https://bigdata-yx.github.io/posts/1082.html</link>
      <guid>https://bigdata-yx.github.io/posts/1082.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:12:05 GMT</pubDate>
      
        
        
      <description>&lt;ul&gt;
&lt;li&gt;&lt;h1 id=&quot;大数据数据处理学习路线&quot;&gt;&lt;a href=&quot;#大数据数据处理学习路线&quot; class=&quot;headerlink&quot; title=&quot;大数据数据处理学习路线&quot;&gt;&lt;/a&gt;大数据数据处理学习路线&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：下列无标注</description>
        
      
      
      
      <content:encoded><![CDATA[<ul><li><h1 id="大数据数据处理学习路线"><a href="#大数据数据处理学习路线" class="headerlink" title="大数据数据处理学习路线"></a>大数据数据处理学习路线</h1><p><strong>注意</strong>：下列无标注的全部要看</p><h2 id="第一阶段：基础部分"><a href="#第一阶段：基础部分" class="headerlink" title="第一阶段：基础部分"></a>第一阶段：基础部分</h2><ul><li>语言基础：python、java基础<ul><li>python：【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注释：第一阶段</li><li>java：【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>数据库基础：mysql<ul><li>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：基础篇</li></ul></li><li>linux：命令基础（）<ul><li>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></li></ul></li></ul><h2 id="第二部分：简单工具使用"><a href="#第二部分：简单工具使用" class="headerlink" title="第二部分：简单工具使用"></a>第二部分：简单工具使用</h2><ul><li><p>python第三方库：pandas、numpy、requests、bs4、jieba、snownlp</p><table><thead><tr><th>第三方库</th><th>链接</th></tr></thead><tbody><tr><td>pandas、numpy</td><td>【千锋教育python数据分析教程200集，Python数据分析师入门必备视频】<a href="https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E6%B3%A8%EF%BC%9Ap38-p117%EF%BC%89">https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58（注：p38-p117）</a></td></tr><tr><td>requests、bs4</td><td>【尚硅谷Python爬虫教程小白零基础速通（含python基础+爬虫案例）】<a href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a> (注：p52-最后)</td></tr><tr><td>jieba</td><td>【Python Jieba 中文分词工具】<a href="https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></td></tr><tr><td>snownlp</td><td>【Lecture 12 基于Snownlp的文本情感分析】<a href="https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E7%9C%8B%E5%AE%8C%E8%BF%99%E9%9B%86%E5%B0%B1%E8%A1%8C%E4%BA%86%EF%BC%89">https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58（看完这集就行了）</a></td></tr></tbody></table></li><li><p>excel：函数使用以及操作</p><ul><li>【2025必看！全网最新最细最实用Excel零基础入门到精通全套教程！专为零基础小白打造！内容富含Excel表格基础操作、实用函数讲解、项目实战等！】<a href="https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li><p>hadoop：hadoop的基本使用命令</p><ul><li><a href="https://blog.csdn.net/m0_43405302/article/details/122243263">hadoop的HDFS的shell命令大全（一篇文章就够了）_shell统计hdfs-CSDN博客</a></li></ul></li></ul><h2 id="第三部分：集群工具使用"><a href="#第三部分：集群工具使用" class="headerlink" title="第三部分：集群工具使用"></a>第三部分：集群工具使用</h2><ul><li><p>scala：scala语言基础</p><ul><li>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：看完前十章</li></ul></li><li><p>mapreduce：了解基本使用以及自定义方法</p><ul><li><p>【黑马程序员大数据Hadoop3.x全套教程，一套精通Hadoop的大数据入门教程】<a href="https://www.bilibili.com/video/BV11N411d7Zh?p=214&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV11N411d7Zh?p=214&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p171-214</p></li></ul></li><li><p>spark：掌握sparkcore sparksql</p><ul><li>【全网最全大数据Spark3.0教程 Spark3.0从入门到精通 黑马程序员大数据入门教程系列】<a href="https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：搭建部分不用看、只看sparkcore和sparksql</li></ul></li><li><p>hive：了解hive命令以及udf自定义函数</p><ul><li><p>【黑马程序员Hive全套教程，大数据Hive3.x数仓开发精讲到企业级实战应用】<a href="https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p0-p96</p></li></ul></li></ul><h2 id="第四部分：深入学习"><a href="#第四部分：深入学习" class="headerlink" title="第四部分：深入学习"></a>第四部分：深入学习</h2><ul><li>机器学习：了解机器学习原理以及sklearn使用<ul><li>【黑马程序员3天快速入门python机器学习】<a href="https://www.bilibili.com/video/BV1nt411r7tj?p=18&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1nt411r7tj?p=18&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>spark streaming：了解流式数据<ul><li>自己找视频或文档</li></ul></li><li>项目制作：尝试制作大数据项目</li></ul></li></ul>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/">学习路线</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/">学习路线</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/1082.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop原理</title>
      <link>https://bigdata-yx.github.io/posts/3985.html</link>
      <guid>https://bigdata-yx.github.io/posts/3985.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;&lt;img src=&quot;https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png&quot;&gt;&lt;/p&gt;
</description>
        
      
      
      
      <content:encoded><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png"></p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3985.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop实例命令</title>
      <link>https://bigdata-yx.github.io/posts/3986.html</link>
      <guid>https://bigdata-yx.github.io/posts/3986.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h4 id=&quot;创建hive的表根据mysql上的表进行创建-create-hive-table&quot;&gt;&lt;a href=&quot;#创建hive的表根据mysql上的表进行创建-create-hive-table&quot; class=&quot;headerlink&quot; title=&quot;创建hive的表根据my</description>
        
      
      
      
      <content:encoded><![CDATA[<h4 id="创建hive的表根据mysql上的表进行创建-create-hive-table"><a href="#创建hive的表根据mysql上的表进行创建-create-hive-table" class="headerlink" title="创建hive的表根据mysql上的表进行创建(create-hive-table)"></a><strong>创建hive的表根据mysql上的表进行创建(create-hive-table)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table --connect jdbc:mysql://master:3306/sqoop_db --username root --password 123456 --table city --hive-table hkjcpdd.city</span><br></pre></td></tr></table></figure><h4 id="查看mysql上有什么表-list-tables"><a href="#查看mysql上有什么表-list-tables" class="headerlink" title="查看mysql上有什么表(list-tables)"></a><strong>查看mysql上有什么表(list-tables)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-tables --connect jdbc:mysql://master:3306/sys --username root --password 123456</span><br></pre></td></tr></table></figure><h3 id="查看mysql上有什么库-list-databases"><a href="#查看mysql上有什么库-list-databases" class="headerlink" title="查看mysql上有什么库(list-databases)"></a><strong>查看mysql上有什么库(list-databases)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://master:3306/ --username root --password 12345</span><br></pre></td></tr></table></figure><h3 id="使用sql进行操作-eval-query"><a href="#使用sql进行操作-eval-query" class="headerlink" title="使用sql进行操作(eval   query)"></a><strong>使用sql进行操作(eval   query)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">eval</span> --connect jdbc:mysql://master:3306/sqoop_db --username root --password  123456 --query <span class="string">&quot;select * from city limit 2;&quot;</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3986.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop多map条件查询导入hdfs.md</title>
      <link>https://bigdata-yx.github.io/posts/3992.html</link>
      <guid>https://bigdata-yx.github.io/posts/3992.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;多map条件查询导入hdfs&quot;&gt;&lt;a href=&quot;#多map条件查询导入hdfs&quot; class=&quot;headerlink&quot; title=&quot;多map条件查询导入hdfs&quot;&gt;&lt;/a&gt;多map条件查询导入hdfs&lt;/h2&gt;&lt;figure class=&quot;highlight </description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="多map条件查询导入hdfs"><a href="#多map条件查询导入hdfs" class="headerlink" title="多map条件查询导入hdfs"></a>多map条件查询导入hdfs</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect 数据库连接字符串 \</span><br><span class="line">--username 数据库用户名 \</span><br><span class="line">--password 数据库密码 \</span><br><span class="line">--target-dir hdfs位置 \</span><br><span class="line">--delete-target-dir \  <span class="comment"># 这个就是把目录删了，不然mapreduce会执行失败</span></span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \ <span class="comment"># 使用什么分隔符</span></span><br><span class="line">--num-mappers 3 \</span><br><span class="line">--split-by 切分数依据 \</span><br><span class="line">--query <span class="string">&#x27; SQL语句 and $CONDITIONS &#x27;</span></span><br></pre></td></tr></table></figure><h3 id="–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"><a href="#–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力" class="headerlink" title="–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"></a><strong>–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力</strong></h3><h3 id="CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理"><a href="#CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理" class="headerlink" title="$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理"></a><strong>$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理</strong></h3>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3992.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop导入其他格式文件</title>
      <link>https://bigdata-yx.github.io/posts/3994.html</link>
      <guid>https://bigdata-yx.github.io/posts/3994.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;导入其他格式文件&quot;&gt;&lt;a href=&quot;#导入其他格式文件&quot; class=&quot;headerlink&quot; title=&quot;导入其他格式文件&quot;&gt;&lt;/a&gt;导入其他格式文件&lt;/h2&gt;&lt;h3 id=&quot;导入不同格式，支持as-avrodatafile、as-parquetfile、a</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="导入其他格式文件"><a href="#导入其他格式文件" class="headerlink" title="导入其他格式文件"></a>导入其他格式文件</h2><h3 id="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式"><a href="#导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式" class="headerlink" title="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)"></a><strong>导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /data/hkjcpdd \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-parquetfile \ <span class="comment">#文件格式</span></span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query <span class="string">&#x27;select * from city where id &lt; 10 and $CONDITIONS&#x27;</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3994.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop导入到Hbase</title>
      <link>https://bigdata-yx.github.io/posts/3988.html</link>
      <guid>https://bigdata-yx.github.io/posts/3988.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;l</description>
        
      
      
      
      <content:encoded><![CDATA[<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-table hkjcpdd:city \</span><br><span class="line">--column-family cf \</span><br><span class="line">--hbase-row-key <span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --hbase-row-key:要求mysql表必须有主见，将主键作为rowkey,表示一行</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3988.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop应用案例</title>
      <link>https://bigdata-yx.github.io/posts/3987.html</link>
      <guid>https://bigdata-yx.github.io/posts/3987.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;创建一个执行文件然后给予权限然后执行&quot;&gt;&lt;a href=&quot;#创建一个执行文件然后给予权限然后执行&quot; class=&quot;headerlink&quot; title=&quot;创建一个执行文件然后给予权限然后执行&quot;&gt;&lt;/a&gt;&lt;strong&gt;创建一个执行文件然后给予权限然后执行&lt;/stro</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="创建一个执行文件然后给予权限然后执行"><a href="#创建一个执行文件然后给予权限然后执行" class="headerlink" title="创建一个执行文件然后给予权限然后执行"></a><strong>创建一个执行文件然后给予权限然后执行</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">batch_date=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--usernmae root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir hive表hdfs目录 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query select * from city</span><br><span class="line"></span><br><span class="line">result=$?</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [<span class="variable">$result</span> != 0];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;执行失败&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd</span><br><span class="line"><span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span>  <span class="string">&quot;执行成功&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd </span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3987.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop</title>
      <link>https://bigdata-yx.github.io/posts/3989.html</link>
      <guid>https://bigdata-yx.github.io/posts/3989.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;数据的导入导出&quot;&gt;&lt;a href=&quot;#数据的导入导出&quot; class=&quot;headerlink&quot; title=&quot;数据的导入导出&quot;&gt;&lt;/a&gt;数据的导入导出&lt;/h2&gt;&lt;p&gt;导入：import&lt;/p&gt;
&lt;p&gt;导出：export&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https:</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="数据的导入导出"><a href="#数据的导入导出" class="headerlink" title="数据的导入导出"></a>数据的导入导出</h2><p>导入：import</p><p>导出：export</p><p><img src="https://pic1.imgdb.cn/item/6784f9d1d0e0a243d4f3f083.png"></p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3989.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop搭建</title>
      <link>https://bigdata-yx.github.io/posts/3984.html</link>
      <guid>https://bigdata-yx.github.io/posts/3984.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;Sqoop搭建&quot;&gt;&lt;a href=&quot;#Sqoop搭建&quot; class=&quot;headerlink&quot; title=&quot;Sqoop搭建&quot;&gt;&lt;/a&gt;Sqoop搭建&lt;/h2&gt;&lt;h3 id=&quot;1-解压&quot;&gt;&lt;a href=&quot;#1-解压&quot; class=&quot;headerlink&quot; titl</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="Sqoop搭建"><a href="#Sqoop搭建" class="headerlink" title="Sqoop搭建"></a>Sqoop搭建</h2><h3 id="1-解压"><a href="#1-解压" class="headerlink" title="1.解压"></a>1.解压</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /root/software</span><br></pre></td></tr></table></figure><h3 id="更改名字"><a href="#更改名字" class="headerlink" title="更改名字"></a>更改名字</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/software</span><br><span class="line"><span class="built_in">mv</span> sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop</span><br></pre></td></tr></table></figure><h3 id="2-添加环境变量"><a href="#2-添加环境变量" class="headerlink" title="2.添加环境变量"></a>2.添加环境变量</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">export</span> SQOOP_HOME=/root/software/sqoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SQOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure><h3 id="3-配置-Sqoop-环境变量文件"><a href="#3-配置-Sqoop-环境变量文件" class="headerlink" title="3.配置 Sqoop 环境变量文件"></a>3.配置 Sqoop 环境变量文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切换到 Sqoop 配置文件目录</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$SQOOP_HOME</span>/conf</span><br><span class="line"><span class="comment"># 复制 Sqoop 环境变量模板文件</span></span><br><span class="line"><span class="built_in">cp</span> sqoop-env-template.sh sqoop-env.sh </span><br><span class="line"><span class="comment"># 编辑文件，指定相关路径</span></span><br><span class="line">vi sqoop-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最底下的配置加上，没有装的就不用去掉#号</span></span><br></pre></td></tr></table></figure><h3 id="4-MySQL-驱动"><a href="#4-MySQL-驱动" class="headerlink" title="4. MySQL 驱动"></a>4. MySQL 驱动</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拷贝 MySQL 驱动到 Sqoop 中的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> /opt/software/mysql-connector-java-5.1.37-bin.jar <span class="variable">$SQOOP_HOME</span>/lib</span><br></pre></td></tr></table></figure><h3 id="5-拷贝-Hive-文件"><a href="#5-拷贝-Hive-文件" class="headerlink" title="5. 拷贝 Hive 文件"></a>5. 拷贝 Hive 文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了后续方便操作 Hive，我们需要将 Hive 的驱动放入 Sqoop 的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> hive/lib/hive-common-3.1.2.jar sqoop/lib/</span><br></pre></td></tr></table></figure><h3 id="6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）"><a href="#6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）" class="headerlink" title="6.验证（输入 sqoop version，出现如下版本信息表示安装成功）"></a>6.验证（输入 <code>sqoop version</code>，出现如下版本信息表示安装成功）</h3><h3 id="7-展示Mysql中sys库下的所有表"><a href="#7-展示Mysql中sys库下的所有表" class="headerlink" title="7.展示Mysql中sys库下的所有表"></a>7.展示Mysql中sys库下的所有表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list<span class="operator">-</span>tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/sys \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123456</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3984.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sqoop导出</title>
      <link>https://bigdata-yx.github.io/posts/3990.html</link>
      <guid>https://bigdata-yx.github.io/posts/3990.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;1-从hdfs导出到mysql里&quot;&gt;&lt;a href=&quot;#1-从hdfs导出到mysql里&quot; class=&quot;headerlink&quot; title=&quot;1.从hdfs导出到mysql里&quot;&gt;&lt;/a&gt;&lt;strong&gt;1.从hdfs导出到mysql里&lt;/strong&gt;&lt;/h3&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="1-从hdfs导出到mysql里"><a href="#1-从hdfs导出到mysql里" class="headerlink" title="1.从hdfs导出到mysql里"></a><strong>1.从hdfs导出到mysql里</strong></h3><ol><li><h4 id="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"><a href="#要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表" class="headerlink" title="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"></a>要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表</h4></li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir hdfs路径指定到文件 \  <span class="comment"># 要导出的文件</span></span><br><span class="line">--table emp \    <span class="comment"># 导出到哪张Mysql表</span></span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--input-fields-terminated-by <span class="string">&#x27;|&#x27;</span>  <span class="comment"># 分隔符</span></span><br></pre></td></tr></table></figure><h3 id="2-从hive导出到Mysql中"><a href="#2-从hive导出到Mysql中" class="headerlink" title="2.从hive导出到Mysql中"></a><strong>2.从hive导出到Mysql中</strong></h3><h4 id="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"><a href="#sqoop的export命令支持insert、update到关系型数据库，但是不支持merge" class="headerlink" title="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"></a><strong>sqoop的export命令支持insert、update到关系型数据库，但是不支持merge</strong></h4><h4 id="1-hive表导入Mysql数据库insert（直接写入）"><a href="#1-hive表导入Mysql数据库insert（直接写入）" class="headerlink" title="1.hive表导入Mysql数据库insert（直接写入）"></a><strong>1.hive表导入Mysql数据库insert（直接写入）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--table cityhkjcpdd </span><br><span class="line">--num-mappers 4 </span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="2-从hive表导入mysql数据库update（追加替换相同）"><a href="#2-从hive表导入mysql数据库update（追加替换相同）" class="headerlink" title="2.从hive表导入mysql数据库update（追加替换相同）"></a><strong>2.从hive表导入mysql数据库update（追加替换相同）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--update-key <span class="built_in">id</span> \    <span class="comment"># 根据id来进行去重</span></span><br><span class="line">--fields-terminated-by <span class="string">&#x27;\t&#x27;</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3990.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>全量导入hive表和增量导入hive表</title>
      <link>https://bigdata-yx.github.io/posts/3991.html</link>
      <guid>https://bigdata-yx.github.io/posts/3991.html</guid>
      <pubDate>Mon, 13 Jan 2025 08:07:49 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;全量导入：&quot;&gt;&lt;a href=&quot;#全量导入：&quot; class=&quot;headerlink&quot; title=&quot;全量导入：&quot;&gt;&lt;/a&gt;&lt;strong&gt;全量导入：&lt;/strong&gt;&lt;/h3&gt;&lt;figure class=&quot;highlight sh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td </description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="全量导入："><a href="#全量导入：" class="headerlink" title="全量导入："></a><strong>全量导入：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-import \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table hive的库.hive的表</span><br></pre></td></tr></table></figure><h4 id="增量导入："><a href="#增量导入：" class="headerlink" title="增量导入："></a><strong>增量导入：</strong></h4><h5 id="1-append方式"><a href="#1-append方式" class="headerlink" title="1.append方式"></a><strong>1.append方式</strong></h5><h5 id="2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"><a href="#2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）" class="headerlink" title="2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"></a>2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）</h5><p><img src="https://pic1.imgdb.cn/item/6784fbd6d0e0a243d4f3f0e9.png"></p><h2 id="这个是第一种方式：incremental-append"><a href="#这个是第一种方式：incremental-append" class="headerlink" title="这个是第一种方式：incremental append"></a><strong>这个是第一种方式：incremental append</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line"></span><br><span class="line">--target-dir hdfs路径 \  这个路径是表的路径可以在hive中show create table city;就可以看到hdfs的位置了</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column <span class="built_in">id</span> \</span><br><span class="line">--last-value 1  这个就是看你的数字哪一行，如果是1那就从2开始增量数据</span><br></pre></td></tr></table></figure><p>参数解释：</p><p>1)incremental : append或lastmodified，使用lastmodified方式导入数据要指定增量数据是要 –append（追加）还是要 –merge-key（合并）</p><p>2)check-column&lt;字段&gt;: 作为增量导入判断的列名</p><p>3)last-value val : 指定某一个值，用于标记增量导入的位置，这个值的数据不会被导入列表中，只用于标记当前表中最后的值。</p><h3 id="第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）"><a href="#第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）" class="headerlink" title="第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）"></a><strong>第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--append</span><br><span class="line"></span><br><span class="line">--注意：last-value的设置把包括2020-10-10 13:00:00 时间的数据做增量导入</span><br></pre></td></tr></table></figure><h3 id="第三种导入方式–incremental-lastmodified-–append-（进行合并的）"><a href="#第三种导入方式–incremental-lastmodified-–append-（进行合并的）" class="headerlink" title="第三种导入方式–incremental lastmodified –append （进行合并的）"></a><strong>第三种导入方式–incremental lastmodified –append （进行合并的）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--merge-key <span class="built_in">id</span> <span class="comment"># 根据id来进行操作</span></span><br><span class="line"></span><br><span class="line">--incremental lastmodified --merge-key的作用：修改过得数据和新增的数据（前提是满足last-value的条件）都会导入尽力啊，并且重复的数据（不需要满足last-value的条件）都会进行合并</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Sqoop/">Sqoop</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Sqoop/">Sqoop</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/3991.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark RDD</title>
      <link>https://bigdata-yx.github.io/posts/fa63.html</link>
      <guid>https://bigdata-yx.github.io/posts/fa63.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:59:03 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心&quot;&gt;&lt;a href=&quot;#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心"><a href="#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心" class="headerlink" title="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心"></a><strong>RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心</strong></h3><h4 id><a href="#" class="headerlink" title></a></h4><h4 id="Dataset-数据集"><a href="#Dataset-数据集" class="headerlink" title="Dataset:(数据集)"></a><strong>Dataset</strong><strong>:(数据集)</strong></h4><ul><li><strong>RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数；</strong></li><li><strong>RDD 也可以缓存起来, 相当于存储具体数据。</strong></li></ul><h4 id="Distributed-："><a href="#Distributed-：" class="headerlink" title="Distributed****："></a><strong>Distributed****：</strong></h4><h4 id="RDD-支持分区-可以运行在集群中。"><a href="#RDD-支持分区-可以运行在集群中。" class="headerlink" title="RDD 支持分区, 可以运行在集群中。"></a><strong>RDD 支持分区, 可以运行在集群中。</strong></h4><h4 id="Resilient-："><a href="#Resilient-：" class="headerlink" title="Resilient****："></a><strong>Resilient****：</strong></h4><ul><li><strong>RDD 支持高效的容错；</strong></li><li><strong>RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中。</strong></li></ul><h3 id="1-RDD的特点："><a href="#1-RDD的特点：" class="headerlink" title="1.RDD的特点："></a><strong>1.RDD的特点：</strong></h3><ul><li><p>弹性</p><ul><li>容错的弹性:数据丢失可以自动恢复;</li><li>存储的弹性:内存与磁盘的自动切换;</li><li>计算的弹性:计算出错重试机制;</li><li>分片的弹性:可根据需要重新分片。</li></ul></li><li><p>分布式:数据存储在集群不同节点上&#x2F;计算分布式。</p></li><li><p>数据集: RDD封装了计算逻辑，并不保存数据。</p></li><li><p>数据抽象: RDD是一个抽象类，需要子类具体实现。</p></li><li><p>不可变: RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑。</p></li><li><p>可分区、并行计算。</p></li></ul>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Spark/">Spark</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Spark/">Spark</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fa63.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark on yarn</title>
      <link>https://bigdata-yx.github.io/posts/fa64.html</link>
      <guid>https://bigdata-yx.github.io/posts/fa64.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:59:03 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;Spark-on-Yarn&quot;&gt;&lt;a href=&quot;#Spark-on-Yarn&quot; class=&quot;headerlink&quot; title=&quot;Spark on Yarn&quot;&gt;&lt;/a&gt;Spark on Yarn&lt;/h1&gt;&lt;h3 id=&quot;SparkOnYarn本质&quot;&gt;&lt;a hre</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="Spark-on-Yarn"><a href="#Spark-on-Yarn" class="headerlink" title="Spark on Yarn"></a>Spark on Yarn</h1><h3 id="SparkOnYarn本质"><a href="#SparkOnYarn本质" class="headerlink" title="SparkOnYarn本质"></a><strong>SparkOnYarn本质</strong></h3><p>master角色由yarn的Resourcemanager担任</p><p>worker角色由yarn的nodemanager担任</p><p>deiver角色运行在Yarn容器内或提交任务的客户端进程中</p><p>真正干活的Executor运行在yarn提供的容器内</p><h3 id="部署："><a href="#部署：" class="headerlink" title="部署："></a><strong>部署：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在spark-env.sh上只要添加这两个即可</span></span><br><span class="line">HADOOP_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line"><span class="comment"># 添加环境变量之后还要添加一个</span></span><br><span class="line"><span class="built_in">which</span> python</span><br><span class="line"><span class="built_in">export</span> SPARK_PYTHON=<span class="built_in">which</span> python</span><br><span class="line"><span class="comment"># 然后启动，bin/pyspark --master yarn</span></span><br></pre></td></tr></table></figure><h3 id="Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"><a href="#Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式" class="headerlink" title="Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"></a><strong>Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式</strong></h3><h3 id="这两种模式的区别就是Driver运行的位置"><a href="#这两种模式的区别就是Driver运行的位置" class="headerlink" title="这两种模式的区别就是Driver运行的位置"></a><strong>这两种模式的区别就是Driver运行的位置</strong></h3><h3 id="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"><a href="#集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内" class="headerlink" title="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"></a><strong>集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内</strong></h3><h3 id="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"><a href="#客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中" class="headerlink" title="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"></a><strong>客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端模式</span></span><br><span class="line">SPARK HOME=/export/server/spark<span class="variable">$&#123;SPARK HOME&#125;</span>/bin/spark-submit\--master yarn</span><br><span class="line">--deploy-mode client \（默认是客户端模式，不加也可以）</span><br><span class="line"><span class="comment"># 下面的参数可加可不加</span></span><br><span class="line">--driver-memory 512m \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line"><span class="variable">$&#123;SPARK HOME&#125;</span>/examples/src/main/python/pi.py 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群模式</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Spark/">Spark</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Spark/">Spark</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fa64.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Spark搭建</title>
      <link>https://bigdata-yx.github.io/posts/fa65.html</link>
      <guid>https://bigdata-yx.github.io/posts/fa65.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:59:03 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;spark安装部署&quot;&gt;&lt;a href=&quot;#spark安装部署&quot; class=&quot;headerlink&quot; title=&quot;spark安装部署&quot;&gt;&lt;/a&gt;spark安装部署&lt;/h1&gt;&lt;h3 id=&quot;先安装anacondea3然后再解压spark&quot;&gt;&lt;a href=&quot;#先安</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="spark安装部署"><a href="#spark安装部署" class="headerlink" title="spark安装部署"></a>spark安装部署</h1><h3 id="先安装anacondea3然后再解压spark"><a href="#先安装anacondea3然后再解压spark" class="headerlink" title="先安装anacondea3然后再解压spark"></a><strong>先安装anacondea3然后再解压spark</strong></h3><h3 id="然后追加以下内容至-x2F-root-x2F-condarc"><a href="#然后追加以下内容至-x2F-root-x2F-condarc" class="headerlink" title="然后追加以下内容至&#x2F;root&#x2F; .condarc"></a><strong>然后追加以下内容至&#x2F;root&#x2F; .condarc</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - defaults</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/fastai/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br></pre></td></tr></table></figure><h3 id="配置pyspark"><a href="#配置pyspark" class="headerlink" title="配置pyspark"></a><strong>配置pyspark</strong></h3><h3 id="conda-create-n-pyspark-python-x3D-版本号然后回车就下载了"><a href="#conda-create-n-pyspark-python-x3D-版本号然后回车就下载了" class="headerlink" title="conda create -n pyspark python&#x3D;版本号然后回车就下载了"></a><strong>conda create -n pyspark python&#x3D;版本号然后回车就下载了</strong></h3><h3 id="安装完之后conda-activate-pyspark切换虚拟环境"><a href="#安装完之后conda-activate-pyspark切换虚拟环境" class="headerlink" title="安装完之后conda activate pyspark切换虚拟环境"></a><strong>安装完之后conda activate pyspark切换虚拟环境</strong></h3><h1 id="Local环境部署"><a href="#Local环境部署" class="headerlink" title="Local环境部署"></a><strong>Local环境部署</strong></h1><h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a><strong>添加环境变量</strong></h3><h2 id="如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath"><a href="#如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath" class="headerlink" title="如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)"></a><strong>如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)</strong></h2><h3 id="然后就可以启动了pysparkspark-shell"><a href="#然后就可以启动了pysparkspark-shell" class="headerlink" title="然后就可以启动了pysparkspark shell"></a><strong>然后就可以启动了pysparkspark shell</strong></h3><h3 id="运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动"><a href="#运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动" class="headerlink" title="运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动"></a><strong>运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动</strong></h3><h3 id="Spark集群搭建"><a href="#Spark集群搭建" class="headerlink" title="Spark集群搭建"></a>Spark集群搭建</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br><span class="line">    <span class="comment"># 指定 Java Home</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/export/servers/jdk1.8.0_221</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定 Spark Master 地址</span></span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_HOST=node01</span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> works.template works</span><br><span class="line">    将Localhost删了换成master slave1 slave2</span><br><span class="line">    </span><br><span class="line">配置 HistoryServer</span><br><span class="line"><span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vi spark-defaults.conf (将这两个前面<span class="comment">#去掉)</span></span><br><span class="line">    spark.eventLog.enabled  <span class="literal">true</span></span><br><span class="line">    spark.eventLog.<span class="built_in">dir</span>      hdfs://node01:8020/spark_log</span><br><span class="line">vi spark-env.sh末尾添加</span><br><span class="line">    <span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:8020/spark_log&quot;</span></span><br><span class="line"> 创建hdfs目录</span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /spark_log</span><br><span class="line"></span><br><span class="line">修改log4j2.properties.template改名为log4j2.properties然后将19行的info改为WARN</span><br><span class="line"></span><br><span class="line">然后分发</span><br><span class="line">启动历史服务器：sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">最后启动</span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">连接，在webui的那个</span><br><span class="line">pyspark --master spark://master:7077</span><br></pre></td></tr></table></figure><h2 id="spark基于zookeeper实现HA"><a href="#spark基于zookeeper实现HA" class="headerlink" title="spark基于zookeeper实现HA"></a><strong>spark基于zookeeper实现HA</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前提确保zookeeper和hdfs均已启动</span><br><span class="line">vi spark-env.sh文件</span><br><span class="line">将<span class="built_in">export</span> SPARK_MASTER_HOST=master和他的那个端口注释了</span><br><span class="line">追加SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181,hadoop03:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span><br><span class="line">然后重新启动spark并且再启动另外一台或者多台机的start-master.sh</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Spark/">Spark</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Spark/">Spark</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fa65.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume Put事务和Take事务</title>
      <link>https://bigdata-yx.github.io/posts/fe67.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe67.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class</description>
        
      
      
      
      <content:encoded><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">在source和chanel的传输中是批量过去的，channels传输到sinks也是批量的</span><br><span class="line"></span><br><span class="line">在source到channel中呢就有一个缓冲区</span><br><span class="line"></span><br><span class="line">1.doPut操作，浆皮数据写入到临时缓冲区 Putlist中</span><br><span class="line"></span><br><span class="line">2.doCommit操作：检查channel队列中是否有足够空间用来存放数据 有的话就会执行doCommit操作</span><br><span class="line"></span><br><span class="line">3.如果channel空间不够就会执行回滚数据的操作（doRollBack）</span><br><span class="line"></span><br><span class="line">在channel到sink中的事务叫take事务，也是批量过去的</span><br><span class="line"></span><br><span class="line">1.doTake操作，拉取一批channel的数据，然后进入缓冲区</span><br><span class="line"></span><br><span class="line">2.takeList操作，临时缓冲</span><br><span class="line"></span><br><span class="line">3.doCommit操作，将这一批数据发送出去，发送失败就会回滚</span><br><span class="line"></span><br><span class="line">4.doRollBack发送失败就进行回滚</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe67.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume source的type</title>
      <link>https://bigdata-yx.github.io/posts/fe55.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe55.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h5 id=&quot;不支持断点续传&quot;&gt;&lt;a href=&quot;#不支持断点续传&quot; class=&quot;headerlink&quot; title=&quot;不支持断点续传&quot;&gt;&lt;/a&gt;不支持断点续传&lt;/h5&gt;&lt;h1 id=&quot;spooling-directory-source&quot;&gt;&lt;a href=&quot;#spooling</description>
        
      
      
      
      <content:encoded><![CDATA[<h5 id="不支持断点续传"><a href="#不支持断点续传" class="headerlink" title="不支持断点续传"></a>不支持断点续传</h5><h1 id="spooling-directory-source"><a href="#spooling-directory-source" class="headerlink" title="spooling directory source"></a>spooling directory source</h1><h2 id="监听某一个目录，只要目录下有文件，文件中的数据就会收集"><a href="#监听某一个目录，只要目录下有文件，文件中的数据就会收集" class="headerlink" title="监听某一个目录，只要目录下有文件，文件中的数据就会收集"></a>监听某一个目录，只要目录下有文件，文件中的数据就会收集</h2><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; spooldir a1.sources.r1.spoolDir &#x3D; &#x2F;root&#x2F;spool</p><p>a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 10000 a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>注意Dir大写</p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe55.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>案例：Flume exec sources监听命令</title>
      <link>https://bigdata-yx.github.io/posts/fe64.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe64.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;exec-sources&quot;&gt;&lt;a href=&quot;#exec-sources&quot; class=&quot;headerlink&quot; title=&quot;exec sources&quot;&gt;&lt;/a&gt;exec sources&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;a1.sources &amp;#x3D;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="exec-sources"><a href="#exec-sources" class="headerlink" title="exec sources"></a>exec sources</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; exec</p><p>a1.sources.r1.command &#x3D; tail -f &#x2F;root&#x2F;exec.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>那个命令就是需要监听的命令</p><p>他是不断的去监听你文件添加了什么</p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe64.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>案例：Flume file channel</title>
      <link>https://bigdata-yx.github.io/posts/fe63.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe63.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;file-channel&quot;&gt;&lt;a href=&quot;#file-channel&quot; class=&quot;headerlink&quot; title=&quot;file channel&quot;&gt;&lt;/a&gt;file channel&lt;/h2&gt;&lt;p&gt;需要的参数 type &amp;#x3D; file&lt;/p&gt;
&lt;p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="file-channel"><a href="#file-channel" class="headerlink" title="file channel"></a>file channel</h2><p>需要的参数 type &#x3D; file</p><p>dataDirs &#x3D; &#x2F;roort</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.channels.c1.type &#x3D; file&#x3D;&#x3D;<br>a1.channels.c1.capaciry&#x3D;10000<br>a1.channels.c1.transactioncapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe63.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>案例：Flume avro sink</title>
      <link>https://bigdata-yx.github.io/posts/fe65.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe65.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;avro-sink&quot;&gt;&lt;a href=&quot;#avro-sink&quot; class=&quot;headerlink&quot; title=&quot;avro sink&quot;&gt;&lt;/a&gt;avro sink&lt;/h2&gt;&lt;p&gt;需要配置的：type &amp;#x3D; avro&lt;/p&gt;
&lt;p&gt;hostname &amp;#x</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="avro-sink"><a href="#avro-sink" class="headerlink" title="avro sink"></a>avro sink</h2><p>需要配置的：type &#x3D; avro</p><p>hostname &#x3D; 主机名</p><p>port  &#x3D; 端口</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; avro<br>a1.sources.r1.bind &#x3D; 192.168.1.122<br>a1.sources.r1.port &#x3D; 55555&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1 c2<br>a1.sinks &#x3D; k1 k2</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100<br>a1.channels.c2.type &#x3D; memory<br>a1.channels.c2.capacity &#x3D; 10000<br>a1.channels.c2.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; avro<br>a1.sinks.k1.hostname &#x3D; 192.168.1.123<br>a1.sinks.k1.port &#x3D; 55555<br>a1.sinks.k2.type &#x3D; avro<br>=&#x3D;a1.sinks.k2.hostname &#x3D; 192.168.1.124<br>a1.sinks.k2.port &#x3D; 55555</p><p>a1.sources.r1.channels &#x3D; c1 c2<br>a1.sinks.k1.channel &#x3D; c1<br>a1.sinks.k2.channel &#x3D; c2</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe65.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume file_roll sink</title>
      <link>https://bigdata-yx.github.io/posts/fe62.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe62.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;File-roll-sink&quot;&gt;&lt;a href=&quot;#File-roll-sink&quot; class=&quot;headerlink&quot; title=&quot;File_roll sink&quot;&gt;&lt;/a&gt;File_roll sink&lt;/h2&gt;&lt;p&gt;必须的：type &amp;#x3D; file_r</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="File-roll-sink"><a href="#File-roll-sink" class="headerlink" title="File_roll sink"></a>File_roll sink</h2><p>必须的：type &#x3D; file_roll</p><p>sink.directory 保存在那个目录&amp;#x20;</p><p>非必须：sink.rollInterval &#x3D; 30 就是每过30s就会生成一个新的文件用来存储数据</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; file_roll</p><p>a1.sinks.k1.sink.directory &#x3D; &#x2F;root&#x2F;file_roll&amp;#x20;</p><p>a1.sinks.k1.sink.rollInterval &#x3D; 10</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe62.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume http source 监听Http</title>
      <link>https://bigdata-yx.github.io/posts/fe59.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe59.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;http-source-监听Http&quot;&gt;&lt;a href=&quot;#http-source-监听Http&quot; class=&quot;headerlink&quot; title=&quot;http source 监听Http&quot;&gt;&lt;/a&gt;http source 监听Http&lt;/h2&gt;&lt;blockquo</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="http-source-监听Http"><a href="#http-source-监听Http" class="headerlink" title="http source 监听Http"></a>http source 监听Http</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; http<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试办法curl -X POST -d ‘[{“headers”:{“key”:”Flume”},”body”:”TestEvent1”}]’ <a href="http://master:4141/">http://master:4141/</a></p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe59.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume Sink Processors sink处理器</title>
      <link>https://bigdata-yx.github.io/posts/fe66.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe66.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;Sink-Processors-sink处理器&quot;&gt;&lt;a href=&quot;#Sink-Processors-sink处理器&quot; class=&quot;headerlink&quot; title=&quot;Sink Processors sink处理器&quot;&gt;&lt;/a&gt;Sink Processors s</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="Sink-Processors-sink处理器"><a href="#Sink-Processors-sink处理器" class="headerlink" title="Sink Processors sink处理器"></a>Sink Processors sink处理器</h2><h3 id="failover-sink-Processor故障转移处理器"><a href="#failover-sink-Processor故障转移处理器" class="headerlink" title="failover sink Processor故障转移处理器"></a><em><strong>failover sink Processor故障转移处</strong>理器</em></h3><h4 id="amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"><a href="#amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力" class="headerlink" title="&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"></a>&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力</h4><p>&amp;#x9;2)需要指定: processor.type  &#x3D; failover</p><p>&amp;#x9;processor.priority.&lt;sinkName&gt; &#x3D; 数字（就是优先级，数字越大优先级越高）</p><p>&amp;#x9;processor.maxpenalty &#x3D; 10000（就是允许你宕机以后给你重连的时间）</p><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>负载平衡处理器提供了在多个sink负载平衡流量的能力。支持两种模式：round<em>robin and random。round</em>_robin可以将数据负载均衡到多个sink上，random支持随机分发到不同的sink上</p><p>第一种就是随机给你发送（random随机）</p><p>第二种就是负载均衡（robin负载均衡）</p><blockquote><p>a1.sources&#x3D;r1 <br>a1.sinks&#x3D;k1 k2 <br>a1.channels&#x3D;c1 <br><br>a1.sources.r1.type&#x3D;netcat <br>a1.sources.r1.bind&#x3D;worker-1 <br>a1.sources.r1.port&#x3D;44444 <br><br>a1.channels.c1.type&#x3D;memory <br>a1.channels.c1.capacity&#x3D;100000 <br>a1.channels.c1.transactionCapacity&#x3D;100 <br><br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-1<br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-2 <br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinkgroups &#x3D; g1 <br>a1.sinkgroups.g1.sinks &#x3D; k1 k2 <br>a1.sinkgroups.g1.processor.type &#x3D; failover(故障转移)   or    load_<em>balance(随机)<br>a1.sinkgroups.g1.processor.selector &#x3D; random or    round</em>_<em>robin# 默认是故障转移的不写就是故障转移，写了就是随机random是随机的意思，而round</em>_robin是轮询的意思<br><br>a1.sources.r1.channels&#x3D;c1 <br>a1.sinks.k1.channel&#x3D;c1 <br>a1.sinks.k2.channel&#x3D;c1</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe66.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume kafka sink</title>
      <link>https://bigdata-yx.github.io/posts/fe57.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe57.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;kafka-sink&quot;&gt;&lt;a href=&quot;#kafka-sink&quot; class=&quot;headerlink&quot; title=&quot;kafka sink&quot;&gt;&lt;/a&gt;kafka sink&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;a1.sources &amp;#x3D; r1&lt;br&gt;a</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="kafka-sink"><a href="#kafka-sink" class="headerlink" title="kafka sink"></a>kafka sink</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; org.apache.flume.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.server &#x3D; master<br>a1.sinks.k1.kafka.topic &#x3D; hkjcpdd&#x3D;&#x3D;</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>flume监听nginx的access.log文件给kafka消费</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; taildir<br>a1.sources.r1.filegroups &#x3D; f1<br>a1.sources.r1.filegroups.f1 &#x3D; &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;access.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.servers &#x3D; master:9092<br>a1.sinks.k1.kafka.topic&#x3D;hkjcpdd</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe57.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume source的type</title>
      <link>https://bigdata-yx.github.io/posts/fe56.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe56.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。&lt;/p&gt;
&lt;p&gt;spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文</description>
        
      
      
      
      <content:encoded><![CDATA[<p>exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。</p><p>spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文件，并在新文件处显示从新文件中解析出来。 与exec source不同，spooling directory source是可靠的， 即使flume重新启动或被kill，也不会丢失数据，同时作为这种可靠的代价，指定目录中的被手机的文件必须是不可变的、唯一命名的。flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常；</p><p>taildir source 监控指定的一些文件，并在检测新的一行数据残生的时候几乎实时的读取他们，如果新的一行数据还没写完，taildir source 等到这行写完后读取</p><p>kafka source 就是一个apache kafka消费者， 他从kafka的topic中读取消息，如果运行了多个Kafka source 则可以把他们配置到同一个消费者组，以便每个source都读取一组唯一的topic分区</p><p>syslog sources 针对系统日志</p><p>http sources 发送get协议请求的</p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe56.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume的netcat 监听端口</title>
      <link>https://bigdata-yx.github.io/posts/fe61.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe61.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;netcat-agent&quot;&gt;&lt;a href=&quot;#netcat-agent&quot; class=&quot;headerlink&quot; title=&quot;netcat agent&quot;&gt;&lt;/a&gt;netcat agent&lt;/h1&gt;&lt;p&gt;配置文件如下&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a1.</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="netcat-agent"><a href="#netcat-agent" class="headerlink" title="netcat agent"></a>netcat agent</h1><p>配置文件如下</p><blockquote><p>a1.sources &#x3D; r1&amp;#x20;</p><p>a1.channels &#x3D; c1&amp;#x20;</p><p>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat&amp;#x20;</p><p>=&#x3D;a1.sources.r1.bind &#x3D; localhost &#x3D;&#x3D;</p><p>=&#x3D;a1.sources.r1.port &#x3D; 44444&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory&amp;#x20;</p><p>a1.channels.c1.capacity &#x3D; 10000&amp;#x20;</p><p>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试方法：telnet localhost 44444</p>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe61.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flume kafka channel</title>
      <link>https://bigdata-yx.github.io/posts/fe58.html</link>
      <guid>https://bigdata-yx.github.io/posts/fe58.html</guid>
      <pubDate>Mon, 13 Jan 2025 07:37:07 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;kafka-channel&quot;&gt;&lt;a href=&quot;#kafka-channel&quot; class=&quot;headerlink&quot; title=&quot;kafka channel&quot;&gt;&lt;/a&gt;kafka channel&lt;/h2&gt;&lt;p&gt;需要指定的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="kafka-channel"><a href="#kafka-channel" class="headerlink" title="kafka channel"></a>kafka channel</h2><p>需要指定的</p><blockquote><p>type &#x3D; org.apache.flume.channel.kafka.KafkaChannel</p><p>kafka.bootstrap.server &#x3D; hostname:port</p><p>kafka.topic &#x3D; hkjcpdd</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; org.apache.flume.channel.kafka.KafkaChannel <br>a1.channels.c1.kafka.bootstrap.servers &#x3D; master:9092</p><p>a1.channels.c1.kafka.topic&#x3D;hkjmjj</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content:encoded>
      
      
      <category domain="https://bigdata-yx.github.io/categories/Flume/">Flume</category>
      
      
      <category domain="https://bigdata-yx.github.io/tags/%E7%A4%BA%E4%BE%8B/">示例</category>
      
      <category domain="https://bigdata-yx.github.io/tags/Flume/">Flume</category>
      
      
      <comments>https://bigdata-yx.github.io/posts/fe58.html#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
