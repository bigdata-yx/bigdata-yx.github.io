<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>mysql运维</title>
      <link href="/posts/a403.html"/>
      <url>/posts/a403.html</url>
      
        <content type="html"><![CDATA[<p>（1）查看当前 MySQL 服务器状态和版本信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> status 或者 \s</span><br></pre></td></tr></table></figure><p>（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE house_market <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8mb4;</span><br></pre></td></tr></table></figure><p>（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">all</span> privileges <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure><p>（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# mkdir mysqlBackup</span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# vi mysqlBackup.sh</span><br><span class="line">#<span class="operator">!</span><span class="operator">/</span>bin<span class="operator">/</span>bash</span><br><span class="line"></span><br><span class="line">DB_NAME<span class="operator">=</span>&quot;house_market&quot;</span><br><span class="line">BACKUP_DIR<span class="operator">=</span>&quot;/root/mysqlBackup&quot;</span><br><span class="line"><span class="type">DATE</span><span class="operator">=</span>$(<span class="type">date</span> <span class="operator">+</span>&quot;%Y%m%d%H%M%S&quot;)</span><br><span class="line">BACKUP_FILE<span class="operator">=</span>&quot;$BACKUP_DIR/$DB_NAME-$DATE.sql&quot;</span><br><span class="line"></span><br><span class="line">mysqldump <span class="operator">-</span>u root <span class="operator">-</span>p<span class="string">&#x27;123456&#x27;</span> $DB_NAME <span class="operator">&gt;</span> $BACKUP_FILE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# chmod <span class="operator">-</span>x mysqlBackup.sh</span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# crontab <span class="operator">-</span>e</span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">/</span>root<span class="operator">/</span>mysqlBackup.sh</span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# crontab <span class="operator">-</span>l</span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">/</span>root<span class="operator">/</span>mysqlBackup.sh</span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> <span class="operator">~</span>]# sh mysqlBackup.sh </span><br><span class="line">mysqldump: [Warning] <span class="keyword">Using</span> a password <span class="keyword">on</span> the command line interface can be insecure.</span><br><span class="line">[root<span class="variable">@master</span> mysqlBackup]# ls</span><br><span class="line">house_market<span class="number">-20250217194213.</span><span class="keyword">sql</span></span><br></pre></td></tr></table></figure><p>（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> max_connections <span class="operator">=</span> <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（7）查看 house_market 数据库中所有表的存储引擎类型</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLE</span> STATUS <span class="keyword">FROM</span> house_market;</span><br></pre></td></tr></table></figure><p>（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> BACKUP_ADMIN,RELOAD <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables <span class="keyword">from</span> mysql;</span><br></pre></td></tr></table></figure><p>（5）创建数据库 education 并设置正确的字符集</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database education <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）授予 eduadmin 用户对学习数据库的查询权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> education.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;eduadmin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（4）创建新的用户 bike_admin</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（5）创建数据库 bike_data，并设置正确的字符集</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database bike_data <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br></pre></td></tr></table></figure><p>（6）授予新用户查询数据和插入数据的权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span> <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（1）配置 MySQL 服务器的最大连接数为 1000</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> max_connections <span class="operator">=</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure><p>（2）创建数据库 tourism 并设置默认字符集为 utf8mb4</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database tourism <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br></pre></td></tr></table></figure><p>（3）创建一个名为 tourism_backup 的定时备份脚本，每天凌晨 2 点自动备份数据库 tourism</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#<span class="operator">!</span><span class="operator">/</span>bin<span class="operator">/</span>bash</span><br><span class="line"></span><br><span class="line">db_name<span class="operator">=</span>&quot;tourism&quot;</span><br><span class="line">backup_dir<span class="operator">=</span>&quot;/opt/tourism_backup&quot;</span><br><span class="line"><span class="type">date</span><span class="operator">=</span>$(<span class="type">date</span> <span class="operator">+</span>&quot;%Y%m%d%H%M%S&quot;)</span><br><span class="line">backup_file<span class="operator">=</span>&quot;$backup_dir/$db_name-$date.sql&quot;</span><br><span class="line"></span><br><span class="line">mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p<span class="string">&#x27;123456&#x27;</span> $db_name <span class="operator">&gt;</span> $backup_file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# chmod <span class="operator">-</span>x tourism_backup.sh </span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# crontab <span class="operator">-</span>e</span><br><span class="line"><span class="keyword">no</span> crontab <span class="keyword">for</span> root <span class="operator">-</span> <span class="keyword">using</span> an <span class="keyword">empty</span> <span class="keyword">one</span></span><br><span class="line">crontab: installing <span class="keyword">new</span> crontab</span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# crontab <span class="operator">-</span>l</span><br><span class="line"><span class="number">0</span> <span class="number">2</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">*</span> <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>tourism_backup.sh</span><br></pre></td></tr></table></figure><p>（4）优化 MySQL 查询性能，开启慢查询日志，设置超过 3 秒的查询被记录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> slow_query_log<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> long_query_time<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><p>（5）配置 MySQL 的 binlog 日志，设置过期时间为 7 天</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> expire_logs_days <span class="operator">=</span> <span class="number">7</span>;</span><br></pre></td></tr></table></figure><p>（2）启用 MySQL 慢查询日志功能，设置慢查询阈值为 2 秒</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> slow_query_log <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> long_query_time <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>（3）创建名为 hadoop 的数据库用户，密码设置为 Hadoop@2025</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;Hadoop@2025&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（4）为 hadoop 用户授予 bigdata 数据库的 SELECT、INSERT 和 UPDATE 权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span>, <span class="keyword">update</span> <span class="keyword">on</span> bigdata.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure><p>（5）创建名为 monitor 的监控专用用户，仅授予 PROCESS 和 SHOW DATABASES 权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> process, <span class="keyword">show</span> databases <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br></pre></td></tr></table></figure><p>（6）使用 mysqldump 工具备份 bigdata 数据库，将备份文件保存为 bigdata_backup.sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p bigdata <span class="operator">&gt;</span> bigdata_backup.sql</span><br><span class="line">Enter password: </span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# ls</span><br><span class="line">bigdata_backup.sql</span><br></pre></td></tr></table></figure><p>（7）配置数据库字符集为 utf8mb4，并将默认排序规则设置为 utf8mb4_unicode_ci</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> character_set_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4&#x27;</span>;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> collation_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4_unicode_ci&#x27;</span>;</span><br></pre></td></tr></table></figure><p>（8）创建一个定时备份脚本，实现每天凌晨 2 点自动备份 bigdata 数据库到 &#x2F;opt&#x2F;backup 目录，并只保留最近 7 天的备份文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#<span class="operator">!</span><span class="operator">/</span>bin<span class="operator">/</span>bash</span><br><span class="line">db_name<span class="operator">=</span>&quot;bigdata&quot;</span><br><span class="line">backup_dir<span class="operator">=</span>&quot;/opt/backup&quot;</span><br><span class="line"><span class="type">date</span><span class="operator">=</span>$(<span class="type">date</span> <span class="operator">+</span>&quot;%Y%m%d%H%M%S&quot;)</span><br><span class="line">backup_file<span class="operator">=</span>&quot;$backup_dir/$db_name-$date.sql&quot;</span><br><span class="line"></span><br><span class="line">mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p<span class="string">&#x27;123456&#x27;</span> $db_name <span class="operator">&gt;</span> $backup_file</span><br><span class="line"></span><br><span class="line">find &quot;$backup_dir&quot; <span class="operator">-</span>type f <span class="operator">-</span>name &quot;$&#123;db_name&#125;-*.sql&quot; <span class="operator">-</span>mtime <span class="operator">+</span><span class="number">7</span> <span class="operator">-</span><span class="keyword">exec</span> rm &#123;&#125; \;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题1</title>
      <link href="/posts/a298.html"/>
      <url>/posts/a298.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>1</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>在大数据技术快速发展的今天，房地产市场正经历着数字化转型。传统的房地产交易和分析模式主要依赖经纪人的个人经验和直觉，这种方式不仅效率低下，而且难以准确把握市场动态。随着大数据技术的应用，房地产行业正在向数据驱动的决策模式转变，这使得市场分析更加精准，服务更加个性化。</p><p>房地产大数据分析平台通过采集和处理海量的交易数据、用户行为数据和市场环境数据，可以全方位地描绘市场格局。这些数据包括但不限于房源基本信息、交易历史、区域配套设施、用户浏览轨迹、市场成交周期等。通过对这些数据的深度分析，可以准确预测房价走势、评估投资价值、识别市场机会，从而为购房者、房产经纪和开发商提供数据支持。为完成二手房销售数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的SSH免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><h4 id="3．子任务三：Zookeeper-集群安装配置"><a href="#3．子任务三：Zookeeper-集群安装配置" class="headerlink" title="3．子任务三：Zookeeper 集群安装配置"></a>3．子任务三：Zookeeper 集群安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）在 master 节点将 &#x2F;opt&#x2F;software 目录下的 apache-zookeeper-3.8.3-bin.tar.gz 包解压到 &#x2F;opt&#x2F;module 路径下， 将解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><p>（2）把解压后的 apache-zookeeper-3.8.3-bin 文件夹更名为 zookeeper-3.8.3，将命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）设置 zookeeper 环境变量，将新增的环境变量内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）创建 zookeeper 配置文件 zoo.cfg 并配置 master、slave1、slave2 三个节点的集群配置，其中 dataDir 参数设置为 &#x2F;opt&#x2F;module&#x2F;zookeeper-3.8.3&#x2F;data ，提交 zoo.cfg 配置内容至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 master 节点上创建文件 myid 用于标识服务器序号，并将文件内容设置为1；</p><p>（6）在 master 节点上将配置的 zookeeper 环境变量文件及 zookeeper 解压包拷贝到 slave1、slave2 节点，提交命令至【提交结果.docx】中对应的任务序号下；</p><p>（7）在 slave1 节点上修改 myid 文件内容修改为 2，在 slave2 节点上修改 myid 文件内容修改为 3，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）在 master 节点、slave1 节点、slave2 节点分别启动 zookeeper，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下； </p><p>（9）在 master 节点、slave1 节点、slave2 节点分别查看 zookeeper 的状态，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（10）在 master 节点查看 Java 进程，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><h4 id="4．子任务四：Kafka-安装配置"><a href="#4．子任务四：Kafka-安装配置" class="headerlink" title="4．子任务四：Kafka 安装配置"></a>4．子任务四：Kafka 安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）从 master 中的 &#x2F;opt&#x2F;software 目录下将文件 kafka_2.12-3.6.1.tgz 解压到 &#x2F;opt&#x2F;module 目录下，把解压后的 kafka_2.12-3.6.1 文件夹更名为 kafka，将 Kafka 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><p>（2）配置好 zookeeper，其中 zookeeper 使用集群模式，分别将 master、slave1、slave2 作为其节点（若 zookpeer 已安装配置好，则无需再次配置）；</p><p>（3）配置 Kafka 环境变量，并使环境变量生效，将新增的环境变量内容截图粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4） 使用 kafka-server-start.sh --version 查看 Kafka 的版本内容， 并将命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><p>（5）修改 server.properties 配置文件，并分发 Kafka 文件到 slave1、slave2 中，并在每个节点启动 Kafka，将启动命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><p>（6）创建 Topic，其中 Topic 名称为 installtopic，分区数为 2，副本数为 2，将创建命令和创建成果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2. 子任务二：MySQL 运维"></a>2. 子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><h4 id="（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3. 子任务三：数据表的创建及维护"></a>3. 子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 house_market 数据库中创建房源信息表（house_info）。房源信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>主键</td></tr><tr><td><strong>community</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>layout</strong></td><td>varchar(50)</td><td>户型</td><td>不能为空</td></tr><tr><td><strong>area</strong></td><td>decimal(10,2)</td><td>建筑面积</td><td>不能为空</td></tr><tr><td><strong>price</strong></td><td>decimal(12,2)</td><td>挂牌价格</td><td>不能为空</td></tr><tr><td><strong>floor_info</strong></td><td>varchar(50)</td><td>楼层信息</td><td></td></tr><tr><td><strong>orientation</strong></td><td>varchar(50)</td><td>朝向</td><td></td></tr><tr><td><strong>status</strong></td><td>enum</td><td>房源状态</td><td>默认”在售”</td></tr></tbody></table><p>（2）根据以下数据字段在 house_market 数据库中创建小区信息表（community_info）。小区信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>community_id</strong></td><td>int</td><td>小区编号</td><td>主键</td></tr><tr><td><strong>community_name</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>build_year</strong></td><td>year</td><td>建成年份</td><td></td></tr><tr><td><strong>property_fee</strong></td><td>decimal(10,2)</td><td>物业费用</td><td></td></tr><tr><td><strong>subway_distance</strong></td><td>int</td><td>地铁距离</td><td>单位：米</td></tr></tbody></table><p>（3）根据以下数据字段在 house_market 数据库中创建交易记录表（transaction_records）。交易记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>record_id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>外键</td></tr><tr><td><strong>transaction_date</strong></td><td>date</td><td>成交日期</td><td>不能为空</td></tr><tr><td><strong>transaction_price</strong></td><td>decimal(12,2)</td><td>成交价格</td><td>不能为空</td></tr><tr><td><strong>price_change</strong></td><td>decimal(12,2)</td><td>价格变动</td><td></td></tr><tr><td><strong>transaction_type</strong></td><td>varchar(50)</td><td>交易类型</td><td>默认”二手房”</td></tr></tbody></table><p>将这三个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（4）将提供的数据文件 house_market_data.sql 导入数据库 house_market中并编写如下 SQL 查询语句：</p><ul><li><p>统计每个区域的在售房源数量和平均价格</p></li><li><p>查询成交价格高于该区域平均成交价格的房源信息</p></li><li><p>查询距离地铁站1000米以内的小区及其房源数量</p></li><li><p>统计每种户型的平均单价（按面积计算）并按降序排列</p></li><li><p>查询最近一个月内成交的房源信息及其所在小区详情</p></li></ul><p>将这五个 SQL 查询语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题1解析</title>
      <link href="/posts/a398.html"/>
      <url>/posts/a398.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>1</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>在大数据技术快速发展的今天，房地产市场正经历着数字化转型。传统的房地产交易和分析模式主要依赖经纪人的个人经验和直觉，这种方式不仅效率低下，而且难以准确把握市场动态。随着大数据技术的应用，房地产行业正在向数据驱动的决策模式转变，这使得市场分析更加精准，服务更加个性化。</p><p>房地产大数据分析平台通过采集和处理海量的交易数据、用户行为数据和市场环境数据，可以全方位地描绘市场格局。这些数据包括但不限于房源基本信息、交易历史、区域配套设施、用户浏览轨迹、市场成交周期等。通过对这些数据的深度分析，可以准确预测房价走势、评估投资价值、识别市场机会，从而为购房者、房产经纪和开发商提供数据支持。为完成二手房销售数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p><img src="https://pic1.imgdb.cn/item/67b2e605d0e0a243d400288a.png"></p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dyn...</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dyna...</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDr...</span><br><span class="line">Feb 17 02:46:40 master systemd[1]: Stopping firewalld - dynamic firewall.....</span><br><span class="line">Feb 17 02:46:40 master systemd[1]: Stopped firewalld - dynamic firewall ...n.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的SSH免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Mon Feb 17 02:49:05 2025 from master</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Mon Feb 17 02:49:10 2025 from master</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo scp -r hadoop/ slave1:/opt/module</span><br><span class="line">sudo scp -r hadoop/ slave2:/opt/module</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br><span class="line">[hadoop@slave2 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-17 03:30:54,631 INFO namenode.FSImage: Allocated new BlockPoolId: BP-515666401-192.168.1.91-1739781054627</span><br><span class="line">2025-02-17 03:30:54,637 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-17 03:30:54,653 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-17 03:30:54,714 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-17 03:30:54,718 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-17 03:30:54,733 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-17 03:30:54,733 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-17 03:30:54,735 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-17 03:30:54,735 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">8672 SecondaryNameNode</span><br><span class="line">9478 WebAppProxyServer</span><br><span class="line">9622 Jps</span><br><span class="line">8491 DataNode</span><br><span class="line">8907 ResourceManager</span><br><span class="line">9228 NodeManager</span><br><span class="line">9565 JobHistoryServer</span><br></pre></td></tr></table></figure><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ jps</span><br><span class="line">2565 DataNode</span><br><span class="line">2664 NodeManager</span><br><span class="line">2780 Jps</span><br></pre></td></tr></table></figure><h4 id="3．子任务三：Zookeeper-集群安装配置"><a href="#3．子任务三：Zookeeper-集群安装配置" class="headerlink" title="3．子任务三：Zookeeper 集群安装配置"></a>3．子任务三：Zookeeper 集群安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）在 master 节点将 &#x2F;opt&#x2F;software 目录下的 apache-zookeeper-3.8.3-bin.tar.gz 包解压到 &#x2F;opt&#x2F;module 路径下， 将解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）把解压后的 apache-zookeeper-3.8.3-bin 文件夹更名为 zookeeper-3.8.3，将命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7</span></span><br></pre></td></tr></table></figure><p>（3）设置 zookeeper 环境变量，将新增的环境变量内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）创建 zookeeper 配置文件 zoo.cfg 并配置 master、slave1、slave2 三个节点的集群配置，其中 dataDir 参数设置为 &#x2F;opt&#x2F;module&#x2F;zookeeper-3.8.3&#x2F;data ，提交 zoo.cfg 配置内容至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper-3.5.7/data</span><br><span class="line"></span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=slave1:2888:3888</span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure><p>（5）在 master 节点上创建文件 myid 用于标识服务器序号，并将文件内容设置为1；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master data]<span class="comment"># cat myid </span></span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>（6）在 master 节点上将配置的 zookeeper 环境变量文件及 zookeeper 解压包拷贝到 slave1、slave2 节点，提交命令至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># scp -r zookeeper-3.5.7/ slave1:`pwd` &amp;&amp; scp -r zookeeper-3.5.7/ slave2:`pwd` &amp;&amp; scp /etc/profile slave1:/etc &amp;&amp; scp /etc/profile slave2:/etc</span></span><br></pre></td></tr></table></figure><p>（7）在 slave1 节点上修改 myid 文件内容修改为 2，在 slave2 节点上修改 myid 文件内容修改为 3，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@slave1 data]<span class="comment"># echo &quot;2&quot; &gt; myid</span></span><br><span class="line">[root@slave2 data]<span class="comment"># echo &quot;3&quot; &gt; myid</span></span><br></pre></td></tr></table></figure><p>（8）在 master 节点、slave1 节点、slave2 节点分别启动 zookeeper，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">[root@slave1 data]<span class="comment"># zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">[root@slave2 data]<span class="comment"># zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><p>（9）在 master 节点、slave1 节点、slave2 节点分别查看 zookeeper 的状态，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@master logs]<span class="comment"># zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line">[root@slave1 data]<span class="comment"># zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br><span class="line">[root@slave2 data]<span class="comment"># zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><p>（10）在 master 节点查看 Java 进程，提交命令和结果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master logs]<span class="comment"># jps</span></span><br><span class="line">8672 SecondaryNameNode</span><br><span class="line">10309 QuorumPeerMain</span><br><span class="line">9478 WebAppProxyServer</span><br><span class="line">10393 Jps</span><br><span class="line">8491 DataNode</span><br><span class="line">8907 ResourceManager</span><br><span class="line">9228 NodeManager</span><br><span class="line">9565 JobHistoryServer</span><br></pre></td></tr></table></figure><h4 id="4．子任务四：Kafka-安装配置"><a href="#4．子任务四：Kafka-安装配置" class="headerlink" title="4．子任务四：Kafka 安装配置"></a>4．子任务四：Kafka 安装配置</h4><p>本任务需要使用 root 用户完成相关配置，已安装 Hadoop 及需要配置前置环境，具体要求如下： </p><p>（1）从 master 中的 &#x2F;opt&#x2F;software 目录下将文件 kafka_2.12-3.6.1.tgz 解压到 &#x2F;opt&#x2F;module 目录下，把解压后的 kafka_2.12-3.6.1 文件夹更名为 kafka，将 Kafka 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf kafka_2.12-3.6.1.tgz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）配置好 zookeeper，其中 zookeeper 使用集群模式，分别将 master、slave1、slave2 作为其节点（若 zookpeer 已安装配置好，则无需再次配置）；</p><p>（3）配置 Kafka 环境变量，并使环境变量生效，将新增的环境变量内容截图粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> KAFKA_HOME=/opt/module/kafka</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4） 使用 kafka-server-start.sh --version 查看 Kafka 的版本内容， 并将命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># kafka-server-start.sh --version</span></span><br><span class="line">[2025-02-17 03:54:04,061] INFO Registered kafka:<span class="built_in">type</span>=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)</span><br><span class="line">3.6.1</span><br></pre></td></tr></table></figure><p>（5）修改 server.properties 配置文件，并分发 Kafka 文件到 slave1、slave2 中，并在每个节点启动 Kafka，将启动命令和结果截图粘贴至【提交结 果.docx】中对应的任务序号下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master kafka]<span class="comment"># bin/kafka-server-start.sh config/server.properties</span></span><br><span class="line"></span><br><span class="line">[root@slave1 kafka]<span class="comment"># bin/kafka-server-start.sh config/server.properties </span></span><br><span class="line"></span><br><span class="line">[root@slave2 kafka]<span class="comment"># bin/kafka-server-start.sh config/server.properties</span></span><br></pre></td></tr></table></figure><p>（6）创建 Topic，其中 Topic 名称为 installtopic，分区数为 2，副本数为 2，将创建命令和创建成果截图粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master kafka]<span class="comment"># kafka-topics.sh --create --bootstrap-server master:9092 --partitions 2 --replication-factor 2 --topic installtopic</span></span><br><span class="line">Created topic installtopic.</span><br></pre></td></tr></table></figure><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpmwarning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">   </span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2. 子任务二：MySQL 运维"></a>2. 子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><h4 id="（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（1）查看当前-MySQL-服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（1）查看当前 MySQL 服务器状态和版本信息，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> version();</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> version() <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5.7</span><span class="number">.44</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> status;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name                                 <span class="operator">|</span> <span class="keyword">Value</span>                                                                                                                                                                                                                                                                                                                                                                                                                                                               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Aborted_clients                               <span class="operator">|</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h4 id="（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（2）创建一个名为-house-market-的数据库，设置其默认字符集为-utf8mb4，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（2）创建一个名为 house_market 的数据库，设置其默认字符集为 utf8mb4，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> DATABASE house_market <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h4 id="（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（3）创建两个用户账号：house-admin（具有所有权限）和-house-viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（3）创建两个用户账号：house_admin（具有所有权限）和 house_viewer（只具有查询权限），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">all</span> privileges <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;house_viewer&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h4 id="（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（4）为-house-market-数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（4）为 house_market 数据库创建一个定时备份计划，每天凌晨2点自动备份数据库，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># mkdir mysqlBackup</span></span><br><span class="line">[root@master ~]<span class="comment"># vi mysqlBackup.sh</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">DB_NAME=<span class="string">&quot;house_market&quot;</span></span><br><span class="line">BACKUP_DIR=<span class="string">&quot;/root/mysqlBackup&quot;</span></span><br><span class="line">DATE=$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M%S&quot;</span>)</span><br><span class="line">BACKUP_FILE=<span class="string">&quot;<span class="variable">$BACKUP_DIR</span>/<span class="variable">$DB_NAME</span>-<span class="variable">$DATE</span>.sql&quot;</span></span><br><span class="line"></span><br><span class="line">mysqldump -u root -p<span class="string">&#x27;123456&#x27;</span> <span class="variable">$DB_NAME</span> &gt; <span class="variable">$BACKUP_FILE</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># chmod -x mysqlBackup.sh</span></span><br><span class="line">[root@master ~]<span class="comment"># crontab -e</span></span><br><span class="line">0 2 * * * /root/mysqlBackup.sh</span><br><span class="line">[root@master ~]<span class="comment"># crontab -l</span></span><br><span class="line">0 2 * * * /root/mysqlBackup.sh</span><br><span class="line"></span><br><span class="line">[root@master ~]<span class="comment"># sh mysqlBackup.sh </span></span><br><span class="line">mysqldump: [Warning] Using a password on the <span class="built_in">command</span> line interface can be insecure.</span><br><span class="line">[root@master mysqlBackup]<span class="comment"># ls</span></span><br><span class="line">house_market-20250217194213.sql</span><br></pre></td></tr></table></figure><h4 id="（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（5）使用命令查看-MySQL-当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（5）使用命令查看 MySQL 当前的最大连接数和缓存大小配置，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line">+-----------------+-------+</span><br><span class="line">| Variable_name   | Value |</span><br><span class="line">+-----------------+-------+</span><br><span class="line">| max_connections | 151   |</span><br><span class="line">+-----------------+-------+</span><br><span class="line">1 row <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; show variables like <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br><span class="line">+------------------+---------+</span><br><span class="line">| Variable_name    | Value   |</span><br><span class="line">+------------------+---------+</span><br><span class="line">| query_cache_size | 1048576 |</span><br><span class="line">+------------------+---------+</span><br><span class="line">1 row <span class="keyword">in</span> <span class="built_in">set</span> (0.00 sec)</span><br></pre></td></tr></table></figure><h4 id="（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（6）修改-MySQL-配置，将最大连接数设置为1000，查询缓存大小设置为-64MB，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（6）修改 MySQL 配置，将最大连接数设置为1000，查询缓存大小设置为 64MB，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> mysqlBackup]# vi <span class="operator">/</span>etc<span class="operator">/</span>my.cnf</span><br><span class="line">max_connections <span class="operator">=</span> <span class="number">1000</span></span><br><span class="line">query_cache_size <span class="operator">=</span> <span class="number">64</span>M</span><br><span class="line"></span><br><span class="line">[root<span class="variable">@master</span> mysqlBackup]# systemctl restart mysqld</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;max_connections&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name   <span class="operator">|</span> <span class="keyword">Value</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-------+</span></span><br><span class="line"><span class="operator">|</span> max_connections <span class="operator">|</span> <span class="number">1000</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+-------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;query_cache_size&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name    <span class="operator">|</span> <span class="keyword">Value</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+</span></span><br><span class="line"><span class="operator">|</span> query_cache_size <span class="operator">|</span> <span class="number">67108864</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h4 id="（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（7）查看-house-market-数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（7）查看 house_market 数据库中所有表的存储引擎类型，并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLE</span> STATUS <span class="keyword">FROM</span> house_market;</span><br></pre></td></tr></table></figure><h4 id="（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；"><a href="#（8）创建一个名为-backup-user-的用户，只授予其备份相关的权限（BACKUP-ADMIN-和-RELOAD），并将命令和结果复制粘贴至【提交结果-docx】中对应的任务序号下；" class="headerlink" title="（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；"></a>（8）创建一个名为 backup_user 的用户，只授予其备份相关的权限（BACKUP_ADMIN 和 RELOAD），并将命令和结果复制粘贴至【提交结果.docx】中对应的任务序号下；</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> BACKUP_ADMIN,RELOAD <span class="keyword">on</span> house_market.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;backup_user&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br></pre></td></tr></table></figure><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3. 子任务三：数据表的创建及维护"></a>3. 子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 house_market 数据库中创建房源信息表（house_info）。房源信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>主键</td></tr><tr><td><strong>community</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>layout</strong></td><td>varchar(50)</td><td>户型</td><td>不能为空</td></tr><tr><td><strong>area</strong></td><td>decimal(10,2)</td><td>建筑面积</td><td>不能为空</td></tr><tr><td><strong>price</strong></td><td>decimal(12,2)</td><td>挂牌价格</td><td>不能为空</td></tr><tr><td><strong>floor_info</strong></td><td>varchar(50)</td><td>楼层信息</td><td></td></tr><tr><td><strong>orientation</strong></td><td>varchar(50)</td><td>朝向</td><td></td></tr><tr><td><strong>status</strong></td><td>enum</td><td>房源状态</td><td>默认”在售”</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> house_info( house_id <span class="type">int</span> <span class="keyword">not</span> <span class="keyword">null</span>, community <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">not</span> <span class="keyword">null</span>, district <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, layout <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, area <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">not</span> <span class="keyword">null</span>, price <span class="type">decimal</span>(<span class="number">12</span>,<span class="number">2</span>) <span class="keyword">not</span> <span class="keyword">null</span>, floor_info <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, orientation <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, status enum(<span class="string">&#x27;在售&#x27;</span>) <span class="keyword">default</span> <span class="string">&#x27;在</span></span><br><span class="line"><span class="string">售&#x27;</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 house_market 数据库中创建小区信息表（community_info）。小区信息表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>community_id</strong></td><td>int</td><td>小区编号</td><td>主键</td></tr><tr><td><strong>community_name</strong></td><td>varchar(100)</td><td>小区名称</td><td>不能为空</td></tr><tr><td><strong>district</strong></td><td>varchar(50)</td><td>所在区域</td><td>不能为空</td></tr><tr><td><strong>build_year</strong></td><td>year</td><td>建成年份</td><td></td></tr><tr><td><strong>property_fee</strong></td><td>decimal(10,2)</td><td>物业费用</td><td></td></tr><tr><td><strong>subway_distance</strong></td><td>int</td><td>地铁距离</td><td>单位：米</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> community_info( community_id <span class="type">int</span> <span class="keyword">primary</span> key, community_naame <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">not</span> <span class="keyword">null</span>, district <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not</span> <span class="keyword">null</span>, build_year <span class="keyword">year</span>, prooperty_fee <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">2</span>), subway_distance <span class="type">int</span> comment &quot;单位:米&quot;);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><p>（3）根据以下数据字段在 house_market 数据库中创建交易记录表（transaction_records）。交易记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>record_id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>house_id</strong></td><td>int</td><td>房源编号</td><td>外键</td></tr><tr><td><strong>transaction_date</strong></td><td>date</td><td>成交日期</td><td>不能为空</td></tr><tr><td><strong>transaction_price</strong></td><td>decimal(12,2)</td><td>成交价格</td><td>不能为空</td></tr><tr><td><strong>price_change</strong></td><td>decimal(12,2)</td><td>价格变动</td><td></td></tr><tr><td><strong>transaction_type</strong></td><td>varchar(50)</td><td>交易类型</td><td>默认”二手房”</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> transaction_records( record_id <span class="type">int</span> <span class="keyword">primary</span> key, house_id</span><br><span class="line"><span class="type">int</span>, transaction_date <span class="type">date</span> <span class="keyword">not</span> <span class="keyword">null</span>, transaction_price <span class="type">decimal</span>(<span class="number">12</span>,<span class="number">2</span>) <span class="keyword">not</span> <span class="keyword">null</span>, price_change <span class="type">decimal</span>(<span class="number">12</span>,<span class="number">2</span>), transaction_type <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">default</span> <span class="string">&#x27;二手房&#x27;</span>,</span><br><span class="line"><span class="keyword">foreign</span> key (house_id) <span class="keyword">references</span> house_info(house_id));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这三个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（4）将提供的数据文件 house_market_data.sql 导入数据库 house_market中并编写如下 SQL 查询语句：</p><ul><li><p>统计每个区域的在售房源数量和平均价格</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> district, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> house_count, <span class="built_in">avg</span>(price) <span class="keyword">as</span> avg_price <span class="keyword">from</span> house_info <span class="keyword">where</span> status<span class="operator">=</span><span class="string">&#x27;在售&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> district;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> district  <span class="operator">|</span> house_count <span class="operator">|</span> avg_price      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> 丰台区    <span class="operator">|</span>           <span class="number">1</span> <span class="operator">|</span> <span class="number">5000000.000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 朝阳区    <span class="operator">|</span>           <span class="number">2</span> <span class="operator">|</span> <span class="number">8000000.000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 海淀区    <span class="operator">|</span>           <span class="number">2</span> <span class="operator">|</span> <span class="number">9000000.000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-------------+----------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询成交价格高于该区域平均成交价格的房源信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> t.record_id, h.community, h.district, h.layout, h.area, t.transaction_price</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">FROM</span> transaction_records t</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> house_info h <span class="keyword">ON</span> t.house_id <span class="operator">=</span> h.house_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">WHERE</span> t.transaction_price <span class="operator">&gt;</span> (</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">SELECT</span> <span class="built_in">AVG</span>(tr.transaction_price)</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">FROM</span> transaction_records tr</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">WHERE</span> tr.house_id <span class="operator">=</span> h.house_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> );</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询距离地铁站1000米以内的小区及其房源数量</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> community_name, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> house_count <span class="keyword">from</span> community_info <span class="keyword">where</span> subway_distance <span class="operator">&lt;</span> <span class="number">1000</span> <span class="keyword">group</span> <span class="keyword">by</span> community_name;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>统计每种户型的平均单价（按面积计算）并按降序排列</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> layout, <span class="built_in">avg</span>(price <span class="operator">/</span> area) <span class="keyword">as</span> avg_price <span class="keyword">from</span> house_info <span class="keyword">group</span> <span class="keyword">by</span> layout <span class="keyword">order</span> <span class="keyword">by</span> avg_price <span class="keyword">desc</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+------------------+</span></span><br><span class="line"><span class="operator">|</span> layout       <span class="operator">|</span> avg_price        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+------------------+</span></span><br><span class="line"><span class="operator">|</span> 四室两厅     <span class="operator">|</span> <span class="number">80000.0000000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 三室两厅     <span class="operator">|</span> <span class="number">70539.4190870000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 三室一厅     <span class="operator">|</span> <span class="number">68181.8181820000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 两室一厅     <span class="operator">|</span> <span class="number">66666.6666670000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 两室两厅     <span class="operator">|</span> <span class="number">50000.0000000000</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查询最近一个月内成交的房源信息及其所在小区详情</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> h.house_id, h.community, h.district, h.layout, h.area, h.price, h.floor_info, h.orientation  <span class="keyword">from</span> house_info h <span class="keyword">join</span> transaction_records t <span class="keyword">on</span> h.house_id<span class="operator">=</span>t.house_id <span class="keyword">where</span> t.transaction_date <span class="operator">&gt;=</span> date_sub(curdate(), <span class="type">interval</span> <span class="number">1</span> <span class="keyword">month</span>);</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这五个 SQL 查询语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题2</title>
      <link href="/posts/a299.html"/>
      <url>/posts/a299.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p>样</p><p>题</p><p>2</p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，教育行业正在经历深刻的变革。在传统教育模式中，教学过程往往依赖教师的经验判断，缺乏对学习者行为的深入理解和精准分析。而在线教育平台的兴起，为教育领域带来了全新的可能。通过收集和分析学习者在平台上的行为数据，如课程选择、学习进度、作业完成情况、互动参与度等，可以更准确地把握学习者的需求和学习特点。平台能够根据用户的学习轨迹、知识掌握程度、学习时长等数据，建立个性化的学习档案，为每位学习者提供更有针对性的课程推荐和学习建议。</p><p>因数据驱动的大数据时代已经到来，在线教育平台需要通过数据分析来提供更优质的教育服务。为完成在线教育平台的大数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p>__（一）任务一：大数据平台搭建 __</p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>HDFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><p>（2）在 master 节点中使用 rpm -ivh 安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）创建新用户 eduadmin 并设置密码，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建数据库 education 并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）授予 eduadmin 用户对学习数据库的查询权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 education 数据库中创建课程表（course）。课程表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>课程编号</td><td>主键</td></tr><tr><td>course_name</td><td>varchar</td><td>课程名称</td><td></td></tr><tr><td>category</td><td>varchar</td><td>课程类别</td><td></td></tr><tr><td>level</td><td>varchar</td><td>难度等级</td><td></td></tr><tr><td>duration</td><td>int</td><td>课程时长(分钟)</td><td></td></tr><tr><td>price</td><td>decimal</td><td>课程价格</td><td></td></tr><tr><td>instructor</td><td>varchar</td><td>讲师姓名</td><td></td></tr><tr><td>avg_rating</td><td>float</td><td>平均评分</td><td></td></tr><tr><td>enrollment_count</td><td>int</td><td>报名人数</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 education 数据库中创建学习记录表（learning_record）。学习记录表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td>user_id</td><td>int</td><td>用户ID</td><td></td></tr><tr><td>course_id</td><td>int</td><td>课程ID</td><td></td></tr><tr><td>watch_duration</td><td>int</td><td>观看时长(分钟)</td><td></td></tr><tr><td>completion_rate</td><td>float</td><td>完成率</td><td></td></tr><tr><td>last_watch_time</td><td>datetime</td><td>最后观看时间</td><td></td></tr><tr><td>quiz_score</td><td>float</td><td>测验得分</td><td></td></tr><tr><td>study_duration</td><td>int</td><td>学习时长(分钟)</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）编写下列 SQL 查询语句：</p><ul><li><p>查询每门课程的平均完成率</p></li><li><p>统计每个课程类别的总报名人数</p></li><li><p>查找观看时长超过课程时长的学习记录</p></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题2解析</title>
      <link href="/posts/a399.html"/>
      <url>/posts/a399.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p>样</p><p>题</p><p>2</p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，教育行业正在经历深刻的变革。在传统教育模式中，教学过程往往依赖教师的经验判断，缺乏对学习者行为的深入理解和精准分析。而在线教育平台的兴起，为教育领域带来了全新的可能。通过收集和分析学习者在平台上的行为数据，如课程选择、学习进度、作业完成情况、互动参与度等，可以更准确地把握学习者的需求和学习特点。平台能够根据用户的学习轨迹、知识掌握程度、学习时长等数据，建立个性化的学习档案，为每位学习者提供更有针对性的课程推荐和学习建议。</p><p>因数据驱动的大数据时代已经到来，在线教育平台需要通过数据分析来提供更优质的教育服务。为完成在线教育平台的大数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p>__（一）任务一：大数据平台搭建 __</p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic <span class="keyword">fi</span>....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting ...</span><br><span class="line">Feb 17 22:09:22 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 17 22:09:23 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Mon Feb 17 22:11:09 2025 from master</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Mon Feb 17 22:03:12 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>HDFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave1:`<span class="built_in">pwd</span>`</span><br><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave2:`<span class="built_in">pwd</span>`</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop /opt/module/hadoop/</span><br><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop /opt/module/hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-17 22:31:36,568 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">2025-02-17 22:31:36,568 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 533.1 KB</span><br><span class="line">2025-02-17 22:31:36,568 INFO util.GSet: capacity      = 2^16 = 65536 entries</span><br><span class="line">2025-02-17 22:31:36,585 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1945592902-192.168.1.91-1739849496581</span><br><span class="line">2025-02-17 22:31:36,592 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-17 22:31:36,608 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-17 22:31:36,668 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-17 22:31:36,672 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-17 22:31:36,687 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-17 22:31:36,687 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-17 22:31:36,690 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-17 22:31:36,690 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">11424 Jps</span><br><span class="line">10273 DataNode</span><br><span class="line">11025 NodeManager</span><br><span class="line">11362 JobHistoryServer</span><br><span class="line">10468 SecondaryNameNode</span><br><span class="line">10024 NameNode</span><br><span class="line">11276 WebAppProxyServer</span><br><span class="line">10703 ResourceManager</span><br></pre></td></tr></table></figure><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 hadoop]$ jps</span><br><span class="line">4064 Jps</span><br><span class="line">3838 DataNode</span><br><span class="line">3951 NodeManager</span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br><span class="line">[root@master module]<span class="comment"># systemctl start mysqld</span></span><br></pre></td></tr></table></figure><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind-address=0.0.0.0</span><br></pre></td></tr></table></figure><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables <span class="keyword">from</span> mysql;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_mysql           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> columns_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> db                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> engine_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> event                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> func                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> general_log               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> gtid_executed             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_category             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_keyword              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_relation             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_topic                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_index_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_table_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ndb_binlog_index          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proc                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> procs_priv                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proxies_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> server_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> servers                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_master_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_relay_log_info      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_worker_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slow_log                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tables_priv               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_leap_second     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_name            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition_type <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="number">31</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（4）创建新用户 eduadmin 并设置密码，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;eduadmin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（5）创建数据库 education 并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database education <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）授予 eduadmin 用户对学习数据库的查询权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span> <span class="keyword">on</span> education.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;eduadmin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 education 数据库中创建课程表（course）。课程表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>课程编号</td><td>主键</td></tr><tr><td>course_name</td><td>varchar</td><td>课程名称</td><td></td></tr><tr><td>category</td><td>varchar</td><td>课程类别</td><td></td></tr><tr><td>level</td><td>varchar</td><td>难度等级</td><td></td></tr><tr><td>duration</td><td>int</td><td>课程时长(分钟)</td><td></td></tr><tr><td>price</td><td>decimal</td><td>课程价格</td><td></td></tr><tr><td>instructor</td><td>varchar</td><td>讲师姓名</td><td></td></tr><tr><td>avg_rating</td><td>float</td><td>平均评分</td><td></td></tr><tr><td>enrollment_count</td><td>int</td><td>报名人数</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> course( id <span class="type">int</span> <span class="keyword">primary</span> key, course_name <span class="type">varchar</span>(<span class="number">255</span>), category vaarchar(<span class="number">255</span>), level <span class="type">varchar</span>(<span class="number">255</span>), duration <span class="type">int</span>, price <span class="type">decimal</span>, instructor <span class="type">varchar</span>(<span class="number">255</span>), avg_rating <span class="type">float</span>, enrollment_count <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 education 数据库中创建学习记录表（learning_record）。学习记录表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td>user_id</td><td>int</td><td>用户ID</td><td></td></tr><tr><td>course_id</td><td>int</td><td>课程ID</td><td></td></tr><tr><td>watch_duration</td><td>int</td><td>观看时长(分钟)</td><td></td></tr><tr><td>completion_rate</td><td>float</td><td>完成率</td><td></td></tr><tr><td>last_watch_time</td><td>datetime</td><td>最后观看时间</td><td></td></tr><tr><td>quiz_score</td><td>float</td><td>测验得分</td><td></td></tr><tr><td>study_duration</td><td>int</td><td>学习时长(分钟)</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> learning_record( id <span class="type">int</span> <span class="keyword">primary</span> key, user_id <span class="type">int</span>, course_id <span class="type">int</span>, watch_duration <span class="type">int</span>, completion_rate <span class="type">float</span>, last_watch_time datetime, quiz_score floaat, study_duration <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）编写下列 SQL 查询语句：</p><ul><li><p>查询每门课程的平均完成率</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> course_id, <span class="built_in">avg</span>(completion_rate) <span class="keyword">from</span> learning_record <span class="keyword">group</span> <span class="keyword">by</span> course_id;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.03</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>统计每个课程类别的总报名人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> category, <span class="built_in">sum</span>(course_name) <span class="keyword">from</span> course <span class="keyword">group</span> <span class="keyword">by</span> category;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>查找观看时长超过课程时长的学习记录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> l.id, l.user_id, l.course_id, l.watch_duration, l.completion_rate, l.ltast_watch_time, l.quiz_score, l.study_duration <span class="keyword">from</span> learning_record l <span class="keyword">join</span> course con l.course_id<span class="operator">=</span>c.id <span class="keyword">where</span> l.watch_duration <span class="operator">&gt;</span> c.duration;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题3</title>
      <link href="/posts/a300.html"/>
      <url>/posts/a300.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>3</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们的出行方式发生了显著变化。共享单车作为一种新型的城市短途出行解决方案，不仅满足了人们”最后一公里”的出行需求，也为城市交通带来了新的活力。在传统的运营模式中，由于缺乏数据支持，共享单车的投放和调度主要依靠运营人员的经验判断，导致供需失衡、车辆分布不均等问题。而在大数据时代，通过对骑行数据的分析，可以更精准地预测用户需求，优化车辆调度，提升运营效率。</p><p>共享单车平台可以收集包括用户骑行轨迹、使用时段、车辆状态等多维度数据。通过对这些数据的分析，可以识别热门区域和时段，预测车辆使用需求，优化调度策略，同时也能够对车辆维护进行预警，提升用户体验。这些数据还可以为城市交通规划提供重要参考，助力智慧城市建设。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供高效的共享单车服务。为完成共享单车使用数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令 java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）创建新的用户 bike_admin，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建数据库 bike_data，并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）授予新用户查询数据和插入数据的权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 bike_data 数据库中创建骑行记录表（ride_records）。骑行记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td></td></tr><tr><td><strong>user_id</strong></td><td>varchar</td><td>用户ID</td><td></td></tr><tr><td><strong>start_time</strong></td><td>datetime</td><td>开始时间</td><td></td></tr><tr><td><strong>end_time</strong></td><td>datetime</td><td>结束时间</td><td></td></tr><tr><td><strong>start_location</strong></td><td>varchar</td><td>起始位置</td><td></td></tr><tr><td><strong>end_location</strong></td><td>varchar</td><td>结束位置</td><td></td></tr><tr><td><strong>ride_distance</strong></td><td>double</td><td>骑行距离(km)</td><td></td></tr><tr><td><strong>ride_duration</strong></td><td>int</td><td>骑行时长(分钟)</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 bike_data 数据库中创建单车状态表（bike_status）。单车状态表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td>主键</td></tr><tr><td><strong>status</strong></td><td>varchar</td><td>车辆状态</td><td></td></tr><tr><td><strong>battery_level</strong></td><td>int</td><td>电量百分比</td><td></td></tr><tr><td><strong>last_maintain_time</strong></td><td>datetime</td><td>最后维护时间</td><td></td></tr><tr><td><strong>current_location</strong></td><td>varchar</td><td>当前位置</td><td></td></tr><tr><td><strong>total_mileage</strong></td><td>double</td><td>总里程(km)</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已提供的 SQL 文件将这两份数据导入 bike_data 数据库中，并对其中的数据进行如下操作：</p><ul><li><p>在 ride_records 表中查询骑行时长超过60分钟的记录数量；</p></li><li><p>在 bike_status 表中统计各个状态的单车数量；</p></li></ul><p>将这两个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题3解析</title>
      <link href="/posts/a400.html"/>
      <url>/posts/a400.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>3</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们的出行方式发生了显著变化。共享单车作为一种新型的城市短途出行解决方案，不仅满足了人们”最后一公里”的出行需求，也为城市交通带来了新的活力。在传统的运营模式中，由于缺乏数据支持，共享单车的投放和调度主要依靠运营人员的经验判断，导致供需失衡、车辆分布不均等问题。而在大数据时代，通过对骑行数据的分析，可以更精准地预测用户需求，优化车辆调度，提升运营效率。</p><p>共享单车平台可以收集包括用户骑行轨迹、使用时段、车辆状态等多维度数据。通过对这些数据的分析，可以识别热门区域和时段，预测车辆使用需求，优化调度策略，同时也能够对车辆维护进行预警，提升用户体验。这些数据还可以为城市交通规划提供重要参考，助力智慧城市建设。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供高效的共享单车服务。为完成共享单车使用数据分析工作，你所在的小组将应用大数据技术，通过Python语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令 java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status firewalld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic ....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting...</span><br><span class="line">Feb 18 06:28:12 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 18 06:28:13 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Tue Feb 18 06:23:09 2025 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Tue Feb 18 06:23:12 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提下，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置服务端 MySQL 数据库的远程连接，将新增的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind-address=0.0.0.0</span><br></pre></td></tr></table></figure><p>（2）配置 root 用户允许任意 IP 连接，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（3）通过 root 用户登录 MySQL 数据库系统，查看 mysql 库下的所有表，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables <span class="keyword">from</span> mysql;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_mysql           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="operator">|</span> columns_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> db                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> engine_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> event                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> func                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> general_log               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> gtid_executed             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_category             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_keyword              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_relation             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> help_topic                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_index_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> innodb_table_stats        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ndb_binlog_index          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> plugin                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proc                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> procs_priv                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> proxies_priv              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> server_cost               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> servers                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_master_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_relay_log_info      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slave_worker_info         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> slow_log                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tables_priv               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_leap_second     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_name            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> time_zone_transition_type <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+</span></span><br><span class="line"><span class="number">31</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（4）创建新的用户 bike_admin，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（5）创建数据库 bike_data，并设置正确的字符集，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database bike_data <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）授予新用户查询数据和插入数据的权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span> <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;bike_admin&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（7）刷新权限，将完整命令及执行命令后的结果复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 bike_data 数据库中创建骑行记录表（ride_records）。骑行记录表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>记录编号</td><td>主键</td></tr><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td></td></tr><tr><td><strong>user_id</strong></td><td>varchar</td><td>用户ID</td><td></td></tr><tr><td><strong>start_time</strong></td><td>datetime</td><td>开始时间</td><td></td></tr><tr><td><strong>end_time</strong></td><td>datetime</td><td>结束时间</td><td></td></tr><tr><td><strong>start_location</strong></td><td>varchar</td><td>起始位置</td><td></td></tr><tr><td><strong>end_location</strong></td><td>varchar</td><td>结束位置</td><td></td></tr><tr><td><strong>ride_distance</strong></td><td>double</td><td>骑行距离(km)</td><td></td></tr><tr><td><strong>ride_duration</strong></td><td>int</td><td>骑行时长(分钟)</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> ride_records( id <span class="type">int</span> <span class="keyword">primary</span> key, bike_id <span class="type">varchar</span>(<span class="number">255</span>), user_id <span class="type">varchar</span>(<span class="number">255</span>), start_time datetime, end_time datetime, start_location <span class="type">varchar</span>(<span class="number">255</span>), end_location <span class="type">varchar</span>(<span class="number">255</span>), ride_distance <span class="keyword">double</span>, ride_duration <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 bike_data 数据库中创建单车状态表（bike_status）。单车状态表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>bike_id</strong></td><td>varchar</td><td>单车编号</td><td>主键</td></tr><tr><td><strong>status</strong></td><td>varchar</td><td>车辆状态</td><td></td></tr><tr><td><strong>battery_level</strong></td><td>int</td><td>电量百分比</td><td></td></tr><tr><td><strong>last_maintain_time</strong></td><td>datetime</td><td>最后维护时间</td><td></td></tr><tr><td><strong>current_location</strong></td><td>varchar</td><td>当前位置</td><td></td></tr><tr><td><strong>total_mileage</strong></td><td>double</td><td>总里程(km)</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> bike_status(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> bike_id <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> status <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> battery_level <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> last_maintain_time datetime,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> current_location <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> total_mileage <span class="keyword">double</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已提供的 SQL 文件将这两份数据导入 bike_data 数据库中，并对其中的数据进行如下操作：</p><ul><li><p>在 ride_records 表中查询骑行时长超过60分钟的记录数量；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> ride_records.<span class="operator">*</span> <span class="keyword">from</span> ride_records <span class="keyword">where</span> ride_duration<span class="operator">&gt;</span><span class="number">60</span>;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>在 bike_status 表中统计各个状态的单车数量；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> bike_status <span class="keyword">group</span> <span class="keyword">by</span> status;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这两个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题4</title>
      <link href="/posts/a301.html"/>
      <url>/posts/a301.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>4</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。旅游行业作为现代服务业的重要组成部分，其数据分析对于提升游客体验、优化资源配置具有重要意义。通过收集和分析游客的行为数据，包括景点偏好、消费习惯、住宿选择等，可以为旅游服务提供者制定更精准的运营策略提供依据。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务。为完成旅游景区客流量预测与分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>DHFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置 MySQL 服务器的最大连接数为 1000，并将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）创建数据库 tourism 并设置默认字符集为 utf8mb4，并将设置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）创建一个名为 tourism_backup 的定时备份脚本，每天凌晨 2 点自动备份数据库 tourism，将脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）优化 MySQL 查询性能，开启慢查询日志，设置超过 3 秒的查询被记录，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）配置 MySQL 的 binlog 日志，设置过期时间为 7 天，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 tourism 数据库中创建景区信息表（scenic_spot）。景区信息表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>主键</td></tr><tr><td>spot_name</td><td>varchar</td><td>景区名称</td><td></td></tr><tr><td>city</td><td>varchar</td><td>所在城市</td><td></td></tr><tr><td>level</td><td>varchar</td><td>景区等级</td><td></td></tr><tr><td>type</td><td>varchar</td><td>景区类型</td><td></td></tr><tr><td>ticket_price</td><td>decimal</td><td>门票价格</td><td></td></tr><tr><td>opening_hours</td><td>varchar</td><td>开放时间</td><td></td></tr><tr><td>max_capacity</td><td>int</td><td>最大承载量</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 tourism 数据库中创建游客流量表（visitor_flow）。游客流量表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>record_id</td><td>int</td><td>记录ID</td><td>主键</td></tr><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>外键</td></tr><tr><td>visit_date</td><td>date</td><td>参观日期</td><td></td></tr><tr><td>visitor_count</td><td>int</td><td>游客数量</td><td></td></tr><tr><td>peak_hour</td><td>time</td><td>高峰时段</td><td></td></tr><tr><td>weather</td><td>varchar</td><td>天气状况</td><td></td></tr><tr><td>is_holiday</td><td>boolean</td><td>是否节假日</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）将提供的数据文件导入数据库 tourism 中并编写以下数据库操作语句：</p><ul><li><p>查询2024年1月每个景区的总游客人数。查询结果需要显示景区名称、所在城市和游客总数，并按游客总数降序排列。</p></li><li><p>统计每个景区在节假日和非节假日的平均游客数量。查询结果需要显示景区名称、景区等级、节假日平均游客数和非节假日平均游客数，按景区编号排序。</p></li><li><p>找出所有景区中单日游客量超过该景区最大承载量的情况。查询结果需要显示景区名称、参观日期、实际游客数量、最大承载量和超出人数，按超出人数降序排列。</p></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题4解析</title>
      <link href="/posts/a401.html"/>
      <url>/posts/a401.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>4</strong></p><p><strong>一、背景描述</strong></p><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。旅游行业作为现代服务业的重要组成部分，其数据分析对于提升游客体验、优化资源配置具有重要意义。通过收集和分析游客的行为数据，包括景点偏好、消费习惯、住宿选择等，可以为旅游服务提供者制定更精准的运营策略提供依据。</p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务。为完成旅游景区客流量预测与分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><p><strong>二、模块一：平台搭建与运维</strong></p><p><strong>（一）任务一：大数据平台搭建</strong></p><p><strong>1．子任务一：基础环境准备</strong></p><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -version 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic <span class="keyword">fi</span>....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting ...</span><br><span class="line">Feb 18 21:04:08 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 18 21:04:09 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last login: Tue Feb 18 21:00:07 2025 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Tue Feb 18 21:00:01 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></p><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th>服务器</th><th>master</th><th>slave1</th><th>slave2</th></tr></thead><tbody><tr><td>DHFS</td><td>NameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td>HDFS</td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td></td><td></td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td>历史日志服务器</td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave1:`<span class="built_in">pwd</span>`</span><br><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave2:`<span class="built_in">pwd</span>`</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br><span class="line">[hadoop@slave2 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7）在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-18 21:22:43,566 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">2025-02-18 21:22:43,566 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 533.1 KB</span><br><span class="line">2025-02-18 21:22:43,566 INFO util.GSet: capacity      = 2^16 = 65536 entries</span><br><span class="line">2025-02-18 21:22:43,585 INFO namenode.FSImage: Allocated new BlockPoolId: BP-464131597-192.168.1.91-1739931763581</span><br><span class="line">2025-02-18 21:22:43,593 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-18 21:22:43,609 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-18 21:22:43,672 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-18 21:22:43,676 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-18 21:22:43,691 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-18 21:22:43,691 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-18 21:22:43,694 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-18 21:22:43,694 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8）在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">4528 NameNode</span><br><span class="line">5202 ResourceManager</span><br><span class="line">5748 WebAppProxyServer</span><br><span class="line">5349 NodeManager</span><br><span class="line">4774 DataNode</span><br><span class="line">5896 Jps</span><br><span class="line">4969 SecondaryNameNode</span><br><span class="line">5834 JobHistoryServer</span><br></pre></td></tr></table></figure><p>（9）在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ jps</span><br><span class="line">2935 Jps</span><br><span class="line">2730 DataNode</span><br><span class="line">2842 NodeManager</span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库服务器的安装与运维</strong></p><p><strong>1. 子任务一：MySQL 安装配置</strong></p><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat、mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><p><strong>2. 子任务二：MySQL 运维</strong></p><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）配置 MySQL 服务器的最大连接数为 1000，并将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> max_connections <span class="operator">=</span> <span class="number">1000</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（2）创建数据库 tourism 并设置默认字符集为 utf8mb4，并将设置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database tourism <span class="type">character</span> <span class="keyword">set</span> utf8mb4;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（3）创建一个名为 tourism_backup 的定时备份脚本，每天凌晨 2 点自动备份数据库 tourism，将脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">db_name=<span class="string">&quot;tourism&quot;</span></span><br><span class="line">backup_dir=<span class="string">&quot;/opt/tourism_backup&quot;</span></span><br><span class="line"><span class="built_in">date</span>=$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M%S&quot;</span>)</span><br><span class="line">backup_file=<span class="string">&quot;<span class="variable">$backup_dir</span>/<span class="variable">$db_name</span>-<span class="variable">$date</span>.sql&quot;</span></span><br><span class="line"></span><br><span class="line">mysqldump -uroot -p<span class="string">&#x27;123456&#x27;</span> <span class="variable">$db_name</span> &gt; <span class="variable">$backup_file</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@master module]<span class="comment"># chmod -x tourism_backup.sh </span></span><br><span class="line">[root@master module]<span class="comment"># crontab -e</span></span><br><span class="line">no crontab <span class="keyword">for</span> root - using an empty one</span><br><span class="line">crontab: installing new crontab</span><br><span class="line">[root@master module]<span class="comment"># crontab -l</span></span><br><span class="line">0 2 * * * /opt/module/tourism_backup.sh</span><br></pre></td></tr></table></figure><p>（4）优化 MySQL 查询性能，开启慢查询日志，设置超过 3 秒的查询被记录，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> slow_query_log<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> long_query_time<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（5）配置 MySQL 的 binlog 日志，设置过期时间为 7 天，将配置命令复制粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> expire_logs_days <span class="operator">=</span> <span class="number">7</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>3. 子任务三：数据表的创建及维护</strong></p><p>（1）根据以下数据字段在 tourism 数据库中创建景区信息表（scenic_spot）。景区信息表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>主键</td></tr><tr><td>spot_name</td><td>varchar</td><td>景区名称</td><td></td></tr><tr><td>city</td><td>varchar</td><td>所在城市</td><td></td></tr><tr><td>level</td><td>varchar</td><td>景区等级</td><td></td></tr><tr><td>type</td><td>varchar</td><td>景区类型</td><td></td></tr><tr><td>ticket_price</td><td>decimal</td><td>门票价格</td><td></td></tr><tr><td>opening_hours</td><td>varchar</td><td>开放时间</td><td></td></tr><tr><td>max_capacity</td><td>int</td><td>最大承载量</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> </span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> scenic_spot(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> spot_id <span class="type">int</span> <span class="keyword">primary</span> key,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> spot_name <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> city <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> level <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> type <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ticket_price <span class="type">decimal</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> opening_hours <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> max_capacity <span class="type">int</span>);</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 tourism 数据库中创建游客流量表（visitor_flow）。游客流量表字段如下：</p><table><thead><tr><th>字段</th><th>类型</th><th>中文含义</th><th>备注</th></tr></thead><tbody><tr><td>record_id</td><td>int</td><td>记录ID</td><td>主键</td></tr><tr><td>spot_id</td><td>int</td><td>景区编号</td><td>外键</td></tr><tr><td>visit_date</td><td>date</td><td>参观日期</td><td></td></tr><tr><td>visitor_count</td><td>int</td><td>游客数量</td><td></td></tr><tr><td>peak_hour</td><td>time</td><td>高峰时段</td><td></td></tr><tr><td>weather</td><td>varchar</td><td>天气状况</td><td></td></tr><tr><td>is_holiday</td><td>boolean</td><td>是否节假日</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> visitor_flow(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> record_id <span class="type">int</span> <span class="keyword">primary</span> key,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> spot_id <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> visit_date <span class="type">date</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> visitor_count <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> peak_hour <span class="type">time</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> weather <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> is_holiday <span class="type">boolean</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">foreign</span> key (spot_id) <span class="keyword">references</span> scenic_spot(spot_id));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）将提供的数据文件导入数据库 tourism 中并编写以下数据库操作语句：</p><ul><li><p>查询2024年1月每个景区的总游客人数。查询结果需要显示景区名称、所在城市和游客总数，并按游客总数降序排列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> spot_name, city, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> scenic_spot s <span class="keyword">join</span> visitor_flow v <span class="keyword">on</span> s.spot_id<span class="operator">=</span>v.spot_id <span class="keyword">where</span> visit_date<span class="operator">&gt;=</span>&quot;2025:01:01&quot; <span class="keyword">and</span> visit_date<span class="operator">&lt;=</span>&quot;2025:01:31&quot; <span class="keyword">group</span></span><br><span class="line"><span class="keyword">by</span> spot_name, city;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>统计每个景区在节假日和非节假日的平均游客数量。查询结果需要显示景区名称、景区等级、节假日平均游客数和非节假日平均游客数，按景区编号排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_name <span class="keyword">AS</span> 景区名称,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.level <span class="keyword">AS</span> 景区等级,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="built_in">AVG</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> v.is_holiday <span class="operator">=</span> <span class="number">1</span> <span class="keyword">THEN</span> v.visitor_count <span class="keyword">ELSE</span> <span class="keyword">NULL</span> <span class="keyword">END</span>) <span class="keyword">AS</span> 节 假日平均游客数,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="built_in">AVG</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> v.is_holiday <span class="operator">=</span> <span class="number">0</span> <span class="keyword">THEN</span> v.visitor_count <span class="keyword">ELSE</span> <span class="keyword">NULL</span> <span class="keyword">END</span>) <span class="keyword">AS</span> 非 节假日平均游客数</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">FROM</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     scenic_spot s</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     visitor_flow v </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">ON</span> s.spot_id <span class="operator">=</span> v.spot_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_name, </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.level, </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> </span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     s.spot_id;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>找出所有景区中单日游客量超过该景区最大承载量的情况。查询结果需要显示景区名称、参观日期、实际游客数量、最大承载量和超出人数，按超出人数降序排列。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> spot_name, visit_date, visitor_count, max_capacity, (max_capacity<span class="operator">-</span>visitor_count) <span class="keyword">as</span> 超出人数 <span class="keyword">from</span> scenic_spot s <span class="keyword">join</span> visitor_flow v <span class="keyword">on</span> s.spot_id<span class="operator">=</span>v.spot_id <span class="keyword">where</span> v.visitor_count <span class="operator">&gt;</span> s.max_capacity <span class="keyword">order</span> <span class="keyword">by</span> 超出人数 <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这三个 SQL 语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题5解析</title>
      <link href="/posts/a302.html"/>
      <url>/posts/a302.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>5</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。平台可以根据用户的浏览，点击，评论等行为信息数据进行收集和整理。通过大量用户的行为可以对某一个产品进行比较准确客观的评分和评价，或者进行相应的用户画像，将产品推荐给喜欢该产品的用户进行相应的消费。 </p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务，为完成互联网酒店的大数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2.子任务二：MySQL 运维"></a>2.子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）修改 MySQL 配置文件启用远程连接，将修改后的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（2）启用 MySQL 慢查询日志功能，设置慢查询阈值为 2 秒，将完整的配置命令和验证命令结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（3）创建名为 hadoop 的数据库用户，密码设置为 Hadoop@2025，将创建用户的完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（4）为 hadoop 用户授予 bigdata 数据库的 SELECT、INSERT 和 UPDATE 权限，将授权命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（5）创建名为 monitor 的监控专用用户，仅授予 PROCESS 和 SHOW DATABASES 权限，将完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（6）使用 mysqldump 工具备份 bigdata 数据库，将备份文件保存为 bigdata_backup.sql，将备份命令及执行过程复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（7）配置数据库字符集为 utf8mb4，并将默认排序规则设置为 utf8mb4_unicode_ci，将配置命令和验证结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><p>（8）创建一个定时备份脚本，实现每天凌晨 2 点自动备份 bigdata 数据库到 &#x2F;opt&#x2F;backup 目录，并只保留最近 7 天的备份文件，将完整脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3.子任务三：数据表的创建及维护"></a>3.子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 bigdata 数据库中创建酒店表 （hotel）。酒店表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>酒店编号</td><td></td></tr><tr><td><strong>hotel_name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>city</strong></td><td>varchar</td><td>城市</td><td></td></tr><tr><td><strong>province</strong></td><td>varchar</td><td>省份</td><td></td></tr><tr><td><strong>level</strong></td><td>varchar</td><td>星级</td><td></td></tr><tr><td><strong>room_num</strong></td><td>int</td><td>房间数</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>commnet_num</strong></td><td>varchar</td><td>评论数</td><td></td></tr></tbody></table><p>（2）根据以下数据字段在 bigdata 数据库中创建评论表 （comment）。评论表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>评论编号</td><td></td></tr><tr><td><strong>name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>commentator</strong></td><td>varchar</td><td>评论人</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>comment_time</strong></td><td>datetime</td><td>评论时间</td><td></td></tr><tr><td><strong>content</strong></td><td>varchar</td><td>评论内容</td><td></td></tr></tbody></table><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已给到的 sql 文件将这两份数据导入 bigdata 数据库中，并对其中的数据进行如下操作：</p><ul><li><p>统计各省份的平均酒店评分，并将评分低于该省份平均分的酒店评分上调0.5分（评分上限为5分）；</p></li><li><p>查找出每个城市评论数最多的酒店及其详细信息；</p></li><li><p>找出所有发表过3条及以上差评（评分小于等于2分）的评论人，并列出这些评论人的评论时间、评论内容及对应的酒店名称，按评论时间降序排序。</p></li></ul><p>将这3个 SQL 语句分别复制粘贴至 【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>省赛样题5解析</title>
      <link href="/posts/a402.html"/>
      <url>/posts/a402.html</url>
      
        <content type="html"><![CDATA[<p><strong>2024-2025学年广东省职业院校技能大赛</strong></p><p><strong>中职组大数据应用与服务赛项</strong></p><p><strong>样</strong></p><p><strong>题</strong></p><p><strong>5</strong></p><h2 id="一、背景描述"><a href="#一、背景描述" class="headerlink" title="一、背景描述"></a>一、背景描述</h2><p>大数据时代背景下，人们生活习惯发生了很多改变。在传统运营模式中，缺乏数据积累，人们在做出一些决策行为过程中，更多是凭借个人经验和直觉，发展路径比较自我封闭。而大数据时代，为人们提供一种全新的思路，通过大量的数据分析得出的结果将更加现实和准确。平台可以根据用户的浏览，点击，评论等行为信息数据进行收集和整理。通过大量用户的行为可以对某一个产品进行比较准确客观的评分和评价，或者进行相应的用户画像，将产品推荐给喜欢该产品的用户进行相应的消费。 </p><p>因数据驱动的大数据时代已经到来，没有大数据，我们无法为用户提供大部分服务，为完成互联网酒店的大数据分析工作，你所在的小组将应用大数据技术，通过 Python 语言以数据采集为基础，将采集的数据进行相应处理，并且进行数据标注、数据分析与可视化、通过大数据业务分析方法实现相应数据分析。运行维护数据库系统保障存储数据的安全性。通过运用相关大数据工具软件解决具体业务问题。你们作为该小组的技术人员，请按照下面任务完成本次工作。</p><h2 id="二、模块一：平台搭建与运维"><a href="#二、模块一：平台搭建与运维" class="headerlink" title="二、模块一：平台搭建与运维"></a>二、模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a>（一）任务一：大数据平台搭建</h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a>1．子任务一：基础环境准备</h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop 需要配置前置环境。命令中要求使用绝对路径，具体要求如下：</p><p>（1）配置三个节点的主机名，分别为 master、slave1、slave2，然后修改三个节点的 hosts 文件，使得三个节点之间可以通过主机名访问，在 master上将执行命令 cat &#x2F;etc&#x2F;hosts 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># cat /etc/hosts</span></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.1.91 master</span><br><span class="line">192.168.1.92 slave1</span><br><span class="line">192.168.1.93 slave2</span><br></pre></td></tr></table></figure><p>（2）将 &#x2F;opt&#x2F;software 目录下将文件 jdk-8u191-linux-x64.tar.gz 安装包（若slave1、slave2节点不存在以上文件则需从master节点复制）解压到 &#x2F;opt&#x2F;module 路径中（若路径不存在，则需新建），将 JDK 解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -zxvf jdk-8u391-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（3）在 &#x2F;etc&#x2F;profile 文件中配置 JDK 环境变量 JAVA_HOME 和 PATH 的值，并让配置文件立即生效，将在 master上 &#x2F;etc&#x2F;profile 中新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_391</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>（4）查看 JDK 版本，检测 JDK 是否安装成功，在 master 上将执行命令java -vserion 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># java -version</span></span><br><span class="line">java version <span class="string">&quot;1.8.0_391&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br></pre></td></tr></table></figure><p>（5）创建 hadoop 用户并设置密码，为 hadoop 用户添加管理员权限。在 master 上将执行命令 grep ‘hadoop’ &#x2F;etc&#x2F;sudoers 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># grep &#x27;hadoop&#x27; /etc/sudoers</span></span><br><span class="line">hadoopALL=(ALL)NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p>（6）关闭防火墙，设置开机不自动启动防火墙，在 master 上将执行命令 systemctl status fireawlld 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># systemctl disable firewalld</span></span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line">[root@master module]<span class="comment"># systemctl status firewalld</span></span><br><span class="line">● firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:firewalld(1)</span><br><span class="line"></span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Starting firewalld - dynamic f....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain systemd[1]: Started firewalld - dynamic <span class="keyword">fi</span>....</span><br><span class="line">Feb 17 02:23:47 localhost.localdomain firewalld[747]: WARNING: AllowZoneDrifting ...</span><br><span class="line">Feb 18 02:09:52 master systemd[1]: Stopping firewalld - dynamic firewall daemon...</span><br><span class="line">Feb 18 02:09:53 master systemd[1]: Stopped firewalld - dynamic firewall daemon.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>（7）配置三个节点的 SSH 免密登录，在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># ssh slave1</span></span><br><span class="line">Last failed login: Tue Feb 18 02:11:10 EST 2025 from slave2 on ssh:notty</span><br><span class="line">There was 1 failed login attempt since the last successful login.</span><br><span class="line">Last login: Tue Feb 18 02:02:49 2025 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]<span class="comment"># hostname</span></span><br><span class="line">slave1</span><br><span class="line">[root@slave1 ~]<span class="comment"># exit</span></span><br><span class="line"><span class="built_in">logout</span></span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@master module]<span class="comment"># ssh slave2</span></span><br><span class="line">Last login: Tue Feb 18 02:02:52 2025 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]<span class="comment"># hostname</span></span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a>2．子任务二：Hadoop 完全分布式安装配置</h4><p>本任务需要使用 root 用户和 hadoop 用户完成相关配置，使用三个节点完成 Hadoop 完全分布式安装配置。命令中要求使用绝对路径，具体要求如下：</p><p>（1）在 master 节点中的 &#x2F;opt&#x2F;software 目录下将文件 hadoop-3.3.6.tar.gz 安装包解压到 &#x2F;opt&#x2F;module 路径中，将 hadoop 安装包解压命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master software]$ sudo tar -zxvf hadoop-3.3.6.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><p>（2）在 master 节点中将解压的 Hadoop 安装目录重命名为 hadoop ，并修改该目录下的所有文件的所属者为 hadoop，所属组为 hadoop，将修改所属者的完整命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（3）在 master 节点中使用 hadoop 用户依次配置 hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、masters 和 workers 配置文件，Hadoop集群部署规划如下表，将 yarn-site.xml 文件内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><table><thead><tr><th><strong>服务器</strong></th><th><strong>master</strong></th><th><strong>slave1</strong></th><th><strong>slave2</strong></th></tr></thead><tbody><tr><td><strong>DHFS</strong></td><td>NameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>SecondaryNameNode</td><td></td><td></td></tr><tr><td><strong>HDFS</strong></td><td>DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td><strong>YARN</strong></td><td>ResourceManager</td><td></td><td></td></tr><tr><td><strong>YARN</strong></td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr><tr><td><strong>历史日志服务器</strong></td><td>JobHistoryServer</td><td></td><td></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/*:/opt/module/hadoop/share/hadoop/common/*:/opt/module/hadoop/share/hadoop/hdfs:/opt/module/hadoop/share/hadoop/hdfs/lib/*:/opt/module/hadoop/share/hadoop/hdfs/*:/opt/module/hadoop/share/hadoop/mapreduce/*:/opt/module/hadoop/share/hadoop/yarn:/opt/module/hadoop/share/hadoop/yarn/lib/*:/opt/module/hadoop/share/hadoop/yarn/*</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）在 master 节点中使用 scp 命令将配置完的 hadoop 安装目录直接拷贝至 slave1 和 slave2 节点，将完整的 scp 命令复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave1:`<span class="built_in">pwd</span>`</span><br><span class="line">[hadoop@master module]$ sudo scp -r jdk1.8.0_391/ hadoop/ slave2:`<span class="built_in">pwd</span>`</span><br></pre></td></tr></table></figure><p>（5）在 slave1 和 slave2 节点中将 hadoop 安装目录的所有文件的所属者为 hadoop，所属组为 hadoop。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br><span class="line">[hadoop@slave2 module]$ sudo <span class="built_in">chown</span> -R hadoop:hadoop hadoop/</span><br></pre></td></tr></table></figure><p>（6）在三个节点的 &#x2F;etc&#x2F;profile 文件中配置 Hadoop 环境变量 HADOOP_HOME 和 PATH 的值，并让配置文件立即生效，将 master 节点中 &#x2F;etc&#x2F;profile 文件新增的内容复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><p>（7） 在 master 节点中初始化 Hadoop 环境 namenode，将初始化命令及初始化结果（截取初始化结果日志最后 20 行即可）粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ hadoop namenode -format</span><br><span class="line"></span><br><span class="line">2025-02-18 02:31:58,444 INFO util.GSet: capacity      = 2^16 = 65536 entries</span><br><span class="line">2025-02-18 02:31:58,461 INFO namenode.FSImage: Allocated new BlockPoolId: BP-610088070-192.168.1.91-1739863918457</span><br><span class="line">2025-02-18 02:31:58,468 INFO common.Storage: Storage directory /data/nn has been successfully formatted.</span><br><span class="line">2025-02-18 02:31:58,484 INFO namenode.FSImageFormatProtobuf: Saving image file /data/nn/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">2025-02-18 02:31:58,544 INFO namenode.FSImageFormatProtobuf: Image file /data/nn/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">2025-02-18 02:31:58,549 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">2025-02-18 02:31:58,564 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> active state</span><br><span class="line">2025-02-18 02:31:58,564 INFO namenode.FSNamesystem: Stopping services started <span class="keyword">for</span> standby state</span><br><span class="line">2025-02-18 02:31:58,566 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.</span><br><span class="line">2025-02-18 02:31:58,566 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at master/192.168.1.91</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure><p>（8） 在 master 节点中依次启动HDFS、YARN集群和历史服务。在 master 上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@master hadoop]$ jps</span><br><span class="line">4372 NameNode</span><br><span class="line">5045 ResourceManager</span><br><span class="line">5253 NodeManager</span><br><span class="line">5591 WebAppProxyServer</span><br><span class="line">4618 DataNode</span><br><span class="line">4812 SecondaryNameNode</span><br><span class="line">5724 Jps</span><br><span class="line">5662 JobHistoryServer</span><br></pre></td></tr></table></figure><p>（9） 在 slave1 查看 Java 进程情况。在 slave1上将执行命令 jps 的结果复制并粘贴至【提交结果.docx】中对应的任务序号下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@slave1 module]$ jps</span><br><span class="line">2838 Jps</span><br><span class="line">2634 DataNode</span><br><span class="line">2746 NodeManager</span><br></pre></td></tr></table></figure><h3 id="（二）任务二：数据库服务器的安装与运维"><a href="#（二）任务二：数据库服务器的安装与运维" class="headerlink" title="（二）任务二：数据库服务器的安装与运维"></a>（二）任务二：数据库服务器的安装与运维</h3><h4 id="1-子任务一：MySQL-安装配置"><a href="#1-子任务一：MySQL-安装配置" class="headerlink" title="1. 子任务一：MySQL 安装配置"></a>1. 子任务一：MySQL 安装配置</h4><p>本任务需要使用 rpm 工具安装 MySQL 并初始化，具体要求如下：</p><p>（1） 在 master 节点中的 &#x2F;opt&#x2F;software 目录下将 MySQL 5.7.44 安装包解压到 &#x2F;opt&#x2F;module 目录下； </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]<span class="comment"># tar -xvf mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /opt/module/</span></span><br></pre></td></tr></table></figure><p>（2）在 master 节点中使用 rpm -ivh 依次安装 mysql-community-common、mysql-community-libs、mysql-community-libs-compat 、 mysql-community-client 和 mysql-community-server 包，将所有命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-common-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-5.7.44-1.el7<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-libs-compat-5.7.4<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-client-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-client-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br><span class="line">[root@master module]<span class="comment"># rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm </span></span><br><span class="line">warning: mysql-community-server-5.7.44-1.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-server-5.7.44-1.e<span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure><p>（3）在 master 节点中启动数据库系统并初始化 MySQL 数据库系统，将完整命令复制粘贴至【提交结果.docx】 中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]<span class="comment"># mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span></span><br></pre></td></tr></table></figure><h4 id="2-子任务二：MySQL-运维"><a href="#2-子任务二：MySQL-运维" class="headerlink" title="2.子任务二：MySQL 运维"></a>2.子任务二：MySQL 运维</h4><p>本任务需要在成功安装 MySQL 的前提，对 MySQL 进行运维操作，具体要求如下：</p><p>（1）修改 MySQL 配置文件启用远程连接，将修改后的配置内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bind-address=0.0.0.0</span><br></pre></td></tr></table></figure><p>（2）启用 MySQL 慢查询日志功能，设置慢查询阈值为 2 秒，将完整的配置命令和验证命令结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; <span class="built_in">set</span> global slow_query_log = 1;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="built_in">set</span> global long_query_time = 2;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure><p>（3）创建名为 hadoop 的数据库用户，密码设置为 Hadoop@2025，将创建用户的完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;Hadoop@2025&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（4）为 hadoop 用户授予 bigdata 数据库的 SELECT、INSERT 和 UPDATE 权限，将授权命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">insert</span>, <span class="keyword">update</span> <span class="keyword">on</span> bigdata.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;hadoop&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（5）创建名为 monitor 的监控专用用户，仅授予 PROCESS 和 SHOW DATABASES 权限，将完整命令及执行结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> process, <span class="keyword">show</span> databases <span class="keyword">on</span> <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;monitor&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（6）使用 mysqldump 工具备份 bigdata 数据库，将备份文件保存为 bigdata_backup.sql，将备份命令及执行过程复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# mysqldump <span class="operator">-</span>uroot <span class="operator">-</span>p bigdata <span class="operator">&gt;</span> bigdata_backup.sql</span><br><span class="line">Enter password: </span><br><span class="line">[root<span class="variable">@master</span> <span class="keyword">module</span>]# ls</span><br><span class="line">bigdata_backup.sql</span><br></pre></td></tr></table></figure><p>（7）配置数据库字符集为 utf8mb4，并将默认排序规则设置为 utf8mb4_unicode_ci，将配置命令和验证结果复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> character_set_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> <span class="keyword">global</span> collation_server <span class="operator">=</span> <span class="string">&#x27;utf8mb4_unicode_ci&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>（8）创建一个定时备份脚本，实现每天凌晨 2 点自动备份 bigdata 数据库到 &#x2F;opt&#x2F;backup 目录，并只保留最近 7 天的备份文件，将完整脚本内容复制粘贴至【提交结果.docx】中对应的任务序号下；</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">db_name=<span class="string">&quot;bigdata&quot;</span></span><br><span class="line">backup_dir=<span class="string">&quot;/opt/backup&quot;</span></span><br><span class="line"><span class="built_in">date</span>=$(<span class="built_in">date</span> +<span class="string">&quot;%Y%m%d%H%M%S&quot;</span>)</span><br><span class="line">backup_file=<span class="string">&quot;<span class="variable">$backup_dir</span>/<span class="variable">$db_name</span>-<span class="variable">$date</span>.sql&quot;</span></span><br><span class="line"></span><br><span class="line">mysqldump -uroot -p<span class="string">&#x27;123456&#x27;</span> <span class="variable">$db_name</span> &gt; <span class="variable">$backup_file</span></span><br><span class="line"></span><br><span class="line">find <span class="string">&quot;<span class="variable">$backup_dir</span>&quot;</span> -<span class="built_in">type</span> f -name <span class="string">&quot;<span class="variable">$&#123;db_name&#125;</span>-*.sql&quot;</span> -mtime +7 -<span class="built_in">exec</span> <span class="built_in">rm</span> &#123;&#125; \;</span><br></pre></td></tr></table></figure><h4 id="3-子任务三：数据表的创建及维护"><a href="#3-子任务三：数据表的创建及维护" class="headerlink" title="3.子任务三：数据表的创建及维护"></a>3.子任务三：数据表的创建及维护</h4><p>（1）根据以下数据字段在 bigdata 数据库中创建酒店表 （hotel）。酒店表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>酒店编号</td><td></td></tr><tr><td><strong>hotel_name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>city</strong></td><td>varchar</td><td>城市</td><td></td></tr><tr><td><strong>province</strong></td><td>varchar</td><td>省份</td><td></td></tr><tr><td><strong>level</strong></td><td>varchar</td><td>星级</td><td></td></tr><tr><td><strong>room_num</strong></td><td>int</td><td>房间数</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>commnet_num</strong></td><td>varchar</td><td>评论数</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> hotel(</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> id <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> hotel_name <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> city <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> province <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> level <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> room_num <span class="type">int</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> score <span class="keyword">double</span>,</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> commnet_num <span class="type">varchar</span>(<span class="number">255</span>));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）根据以下数据字段在 bigdata 数据库中创建评论表 （comment）。评论表字段如下：</p><table><thead><tr><th><strong>字段</strong></th><th><strong>类型</strong></th><th><strong>中文含义</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>id</strong></td><td>int</td><td>评论编号</td><td></td></tr><tr><td><strong>name</strong></td><td>varchar</td><td>酒店名称</td><td></td></tr><tr><td><strong>commentator</strong></td><td>varchar</td><td>评论人</td><td></td></tr><tr><td><strong>score</strong></td><td>double</td><td>评分</td><td></td></tr><tr><td><strong>comment_time</strong></td><td>datetime</td><td>评论时间</td><td></td></tr><tr><td><strong>content</strong></td><td>varchar</td><td>评论内容</td><td></td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> comment( id <span class="type">int</span>, name <span class="type">varchar</span>(<span class="number">255</span>), commentator <span class="type">varchar</span>(<span class="number">255</span>), score <span class="keyword">double</span>, comment_time datetime, content <span class="type">varchar</span>(<span class="number">255</span>));</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>将这两个 SQL 建表语句分别复制粘贴至【提交结果.docx】中对应的任务序号下。</p><p>（3）根据已给到的 sql 文件将这两份数据导入 bigdata 数据库中，并对其中的数据进行如下操作：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> source <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>hotel_all_data.sql</span><br><span class="line">mysql<span class="operator">&gt;</span> source <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>comment_all_data.sql</span><br></pre></td></tr></table></figure><ul><li><p>统计各省份的平均酒店评分，并将评分低于该省份平均分的酒店评分上调0.5分（评分上限为5分）；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> province, <span class="built_in">avg</span>(score) <span class="keyword">from</span> hotel_all <span class="keyword">group</span> <span class="keyword">by</span> province;</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">UPDATE</span> hotels h</span><br><span class="line"><span class="keyword">SET</span> h.score <span class="operator">=</span> h.score <span class="operator">+</span> <span class="number">0.5</span></span><br><span class="line"><span class="keyword">WHERE</span> h.score <span class="operator">&lt;</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> <span class="built_in">AVG</span>(score)</span><br><span class="line">    <span class="keyword">FROM</span> hotels</span><br><span class="line">    <span class="keyword">WHERE</span> province <span class="operator">=</span> h.province</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> province, <span class="built_in">AVG</span>(score) <span class="keyword">AS</span> avg_score</span><br><span class="line"><span class="keyword">FROM</span> hotels</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> province;</span><br></pre></td></tr></table></figure></li><li><p>查找出每个城市评论数最多的酒店及其详细信息；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> h.<span class="operator">*</span></span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">SELECT</span> id, <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> comment_count</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">FROM</span> comment</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">GROUP</span> <span class="keyword">BY</span> id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ) <span class="keyword">AS</span> comment_counts</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> hotel <span class="keyword">AS</span> h <span class="keyword">ON</span> comment_counts.id <span class="operator">=</span> h.id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> <span class="keyword">JOIN</span> (</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">SELECT</span> name, <span class="built_in">MAX</span>(comment_count) <span class="keyword">AS</span> max_comment_count</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>         <span class="keyword">SELECT</span> name, id, <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> comment_count</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>         <span class="keyword">FROM</span> comment</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>         <span class="keyword">GROUP</span> <span class="keyword">BY</span> name, id</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     ) <span class="keyword">AS</span> subquery</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span>     <span class="keyword">GROUP</span> <span class="keyword">BY</span> name</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ) <span class="keyword">AS</span> max_comments <span class="keyword">ON</span> h.hotel_name <span class="operator">=</span> max_comments.name <span class="keyword">AND</span> comment_counts.comment_count <span class="operator">=</span> max_comments.max_comment_count;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li><li><p>找出所有发表过3条及以上差评（评分小于等于2分）的评论人，并列出这些评论人的评论时间、评论内容及对应的酒店名称，按评论时间降序排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> com.commentator, com.comment_time, com.content, com.name <span class="keyword">from</span> ( <span class="keyword">select</span> commentator, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> count_c <span class="keyword">from</span> comment <span class="keyword">group</span> <span class="keyword">by</span> commentator <span class="keyword">having</span> <span class="built_in">count</span>(<span class="operator">*</span>)<span class="operator">&gt;=</span><span class="number">3</span>) <span class="keyword">as</span> c <span class="keyword">join</span> comment com <span class="keyword">on</span> c.commentator<span class="operator">=</span>com.commentator <span class="keyword">order</span> <span class="keyword">by</span> comment_time <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure></li></ul><p>将这3个 SQL 语句分别复制粘贴至 【提交结果.docx】中对应的任务序号下。</p>]]></content>
      
      
      <categories>
          
          <category> 省赛样题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> 省赛样题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop Ha</title>
      <link href="/posts/yx1.html"/>
      <url>/posts/yx1.html</url>
      
        <content type="html"><![CDATA[<h1 id="配置Hadoop-Ha"><a href="#配置Hadoop-Ha" class="headerlink" title="配置Hadoop Ha"></a>配置Hadoop Ha</h1><blockquote><p>[!NOTE]</p><p>在hadoop集群的基础上</p></blockquote><h3 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hadoop-env.sh</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/root/software/jdk</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/root/software/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_LOG_DIR=<span class="variable">$HADOOP_HOME</span>/logs</span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_uSER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_PROXYUSER_USER=root</span><br><span class="line"><span class="comment"># 加上了以下两个</span></span><br><span class="line"><span class="built_in">export</span> HDFS_JOURNALNODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_ZKFC_USER=root</span><br></pre></td></tr></table></figure><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  # 这里就不能指定 主机名:端口了</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>131072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> # 加上了zookeeper的配置</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/nn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/dn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir.perm<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>700<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>268435456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"># 以上是原来的配置，但是不用hosts了，因为默认是全部</span><br><span class="line"># 以下则是高可用的配置</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://master:8485;slave1:8485;slave2:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"># 这个得添加，配置hdfs-site.xml隔离机制方法(栅栏方法）</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">          sshfence</span><br><span class="line">          shell(/bin/true)</span><br><span class="line">      <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="mapred-site-xml-保持原有配置即可"><a href="#mapred-site-xml-保持原有配置即可" class="headerlink" title="mapred-site.xml(保持原有配置即可)"></a>mapred-site.xml(保持原有配置即可)</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19899<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$HADOOP_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># 不能有resourcemanager指定hostname了因为是高可用</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/software/hadoop/etc/hadoop:/root/software/hadoop/share/hadoop/common/lib/*:/root/software/hadoop/share/hadoop/common/*:/root/software/hadoop/share/hadoop/hdfs:/root/software/hadoop/share/hadoop/hdfs/lib/*:/root/software/hadoop/share/hadoop/hdfs/*:/root/software/hadoop/share/hadoop/mapreduce/*:/root/software/hadoop/share/hadoop/yarn:/root/software/hadoop/share/hadoop/yarn/lib/*:/root/software/hadoop/share/hadoop/yarn/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.proxy-web.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.zk.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="可能看不到datanode之类的那就手动启动"><a href="#可能看不到datanode之类的那就手动启动" class="headerlink" title="可能看不到datanode之类的那就手动启动"></a>可能看不到datanode之类的那就手动启动</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@slave1 ~]<span class="comment"># hadoop-daemon.sh start datanode</span></span><br></pre></td></tr></table></figure><h3 id="然后就可以去查看两个节点的hdfs-webui了"><a href="#然后就可以去查看两个节点的hdfs-webui了" class="headerlink" title="然后就可以去查看两个节点的hdfs webui了"></a>然后就可以去查看两个节点的hdfs webui了</h3><p><img src="https://pic1.imgdb.cn/item/67886febd0e0a243d4f4ae34.png"></p><p><img src="https://pic1.imgdb.cn/item/67886ff3d0e0a243d4f4ae37.png"></p><h3 id="会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs-ha成功了"><a href="#会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs-ha成功了" class="headerlink" title="会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs ha成功了"></a>会看到一个standby一个是active，standby是备用节点，而active是活跃节点，那说明hdfs ha成功了</h3><h4 id="查看NameNode的状态"><a href="#查看NameNode的状态" class="headerlink" title="查看NameNode的状态"></a>查看NameNode的状态</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs haadmin -getServiceState nn1</span><br><span class="line">hdfs haadmin -getServiceState nn2</span><br></pre></td></tr></table></figure><h4 id="查看ResourceManager的状态"><a href="#查看ResourceManager的状态" class="headerlink" title="查看ResourceManager的状态"></a>查看ResourceManager的状态</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yarn rmadmin -getServiceState rm1</span><br><span class="line">yarn rmadmin -getServiceState rm2</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis</title>
      <link href="/posts/nb11.html"/>
      <url>/posts/nb11.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis常见命令</title>
      <link href="/posts/nb13.html"/>
      <url>/posts/nb13.html</url>
      
        <content type="html"><![CDATA[<h2 id="Redis的常见命令"><a href="#Redis的常见命令" class="headerlink" title="Redis的常见命令"></a>Redis的常见命令</h2><h3 id="1-1Redis数据结构介绍，数据类型"><a href="#1-1Redis数据结构介绍，数据类型" class="headerlink" title="1.1Redis数据结构介绍，数据类型"></a>1.1Redis数据结构介绍，数据类型</h3><p><img src="https://pic1.imgdb.cn/item/67773d23d0e0a243d4ee11a6.png"></p><h3 id="1-2Redis通用命令"><a href="#1-2Redis通用命令" class="headerlink" title="1.2Redis通用命令"></a>1.2Redis通用命令</h3><ul><li>keys pattern(查看符合模版的所有key)例如：keys *keys ?keys *n?*是多个字符 ?是单个字符</li><li>del key [key …]    (删除一个或多个指定的key)    例如：del k1 k2 k3</li><li>exists key [key …] (判断一个k是否存在)              例如：exists name</li><li>expire key seconds (给一个key设置有效期)        例如: expire name 1</li><li>ttl key                       (查看一个key的生育有效期) 例如：ttl name</li></ul><h3 id="1-3String类型"><a href="#1-3String类型" class="headerlink" title="1.3String类型"></a>1.3String类型</h3><p><img src="https://pic1.imgdb.cn/item/677743f4d0e0a243d4ee2616.png"></p><h3 id="1-4key层级结构"><a href="#1-4key层级结构" class="headerlink" title="1.4key层级结构"></a>1.4key层级结构</h3><p><img src="https://pic1.imgdb.cn/item/67774629d0e0a243d4ee2ac2.png"></p><h3 id="1-5Hash类型"><a href="#1-5Hash类型" class="headerlink" title="1.5Hash类型"></a>1.5Hash类型</h3><p><img src="https://pic1.imgdb.cn/item/677746c1d0e0a243d4ee2c52.png"></p><p><img src="https://pic1.imgdb.cn/item/677746d2d0e0a243d4ee2c83.png"></p><h3 id="1-6List类型"><a href="#1-6List类型" class="headerlink" title="1.6List类型"></a>1.6List类型</h3><p><img src="https://pic1.imgdb.cn/item/67774f9ad0e0a243d4ee350d.png"></p><p><img src="https://pic1.imgdb.cn/item/67774fadd0e0a243d4ee3510.png"></p><h3 id="1-7Set类型"><a href="#1-7Set类型" class="headerlink" title="1.7Set类型"></a>1.7Set类型</h3><p><img src="https://pic1.imgdb.cn/item/67775270d0e0a243d4ee3572.png"></p><p><img src="https://pic1.imgdb.cn/item/6777527dd0e0a243d4ee3573.png"></p><h3 id="1-8SortedSet类型"><a href="#1-8SortedSet类型" class="headerlink" title="1.8SortedSet类型"></a>1.8SortedSet类型</h3><p><img src="https://pic1.imgdb.cn/item/67775933d0e0a243d4ee4227.png"></p><p><img src="https://pic1.imgdb.cn/item/6777594ad0e0a243d4ee422b.png"></p>]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis常见客户端</title>
      <link href="/posts/nb14.html"/>
      <url>/posts/nb14.html</url>
      
        <content type="html"><![CDATA[<h2 id="Redis客户端"><a href="#Redis客户端" class="headerlink" title="Redis客户端"></a>Redis客户端</h2><ul><li>命令行客户端</li><li>图形化桌面客户端</li><li>编程客户端</li></ul><h3 id="1-1Redis命令行客户端"><a href="#1-1Redis命令行客户端" class="headerlink" title="1.1Redis命令行客户端"></a>1.1Redis命令行客户端</h3><p>Redis安装完成后就自带了命令行客户端：redis-cli，使用方式如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli [options] [commonds] <span class="comment"># 一般就是redis-cli -h master -a 123456即可</span></span><br></pre></td></tr></table></figure><p>其中常见的options有：</p><ul><li><code>-h 127.0.0.1</code>：指定要连接的redis节点的IP地址，默认是127.0.0.1</li><li><code>-p 6379</code>：指定要连接的redis节点的端口，默认是6379</li><li><code>-a 123321</code>：指定redis的访问密码</li></ul><p>其中的commonds就是Redis的操作命令，例如：</p><ul><li><code>ping</code>：与redis服务端做心跳测试，服务端正常会返回<code>pong</code></li></ul><h3 id="1-2图形化桌面客户端"><a href="#1-2图形化桌面客户端" class="headerlink" title="1.2图形化桌面客户端"></a>1.2图形化桌面客户端</h3><p><a href="https://github.com/lework/RedisDesktopManager-Windows/releases">https://github.com/lework/RedisDesktopManager-Windows/releases</a></p>]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis搭建</title>
      <link href="/posts/nb12.html"/>
      <url>/posts/nb12.html</url>
      
        <content type="html"><![CDATA[<h2 id="Redis搭建"><a href="#Redis搭建" class="headerlink" title="Redis搭建"></a>Redis搭建</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压 tar -xvzf redis-6.2.6.tar.gz</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入解压后的目录 make &amp;&amp; make install</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">默认的安装路径是在 `/usr/local/bin`目录下</span></span><br></pre></td></tr></table></figure><h3 id="1-1默认启动"><a href="#1-1默认启动" class="headerlink" title="1.1默认启动"></a>1.1默认启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server</span><br></pre></td></tr></table></figure><h3 id="1-2指定配置启动"><a href="#1-2指定配置启动" class="headerlink" title="1.2指定配置启动"></a>1.2指定配置启动</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conf文件在解压后的目录下，然后进行配置</span></span><br><span class="line"><span class="comment"># 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0</span></span><br><span class="line"><span class="attr">bind</span> <span class="string">0.0.0.0</span></span><br><span class="line"><span class="comment"># 守护进程，修改为yes后即可后台运行</span></span><br><span class="line"><span class="attr">daemonize</span> <span class="string">yes </span></span><br><span class="line"><span class="comment"># 密码，设置后访问Redis必须输入密码</span></span><br><span class="line"><span class="attr">requirepass</span> <span class="string">123321</span></span><br></pre></td></tr></table></figure><h4 id="Redis的其它常见配置："><a href="#Redis的其它常见配置：" class="headerlink" title="Redis的其它常见配置："></a>Redis的其它常见配置：</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 监听的端口</span></span><br><span class="line"><span class="attr">port</span> <span class="string">6379</span></span><br><span class="line"><span class="comment"># 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录</span></span><br><span class="line"><span class="attr">dir</span> <span class="string">.</span></span><br><span class="line"><span class="comment"># 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15</span></span><br><span class="line"><span class="attr">databases</span> <span class="string">1</span></span><br><span class="line"><span class="comment"># 设置redis能够使用的最大内存</span></span><br><span class="line"><span class="attr">maxmemory</span> <span class="string">512mb</span></span><br><span class="line"><span class="comment"># 日志文件，默认为空，不记录日志，可以指定日志文件名</span></span><br><span class="line"><span class="attr">logfile</span> <span class="string">&quot;redis.log&quot;</span></span><br></pre></td></tr></table></figure><h3 id="启动Redis"><a href="#启动Redis" class="headerlink" title="启动Redis:"></a>启动Redis:</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入redis安装目录 </span></span><br><span class="line"><span class="built_in">cd</span> /usr/local/src/redis-6.2.6</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">redis-server redis.conf</span><br></pre></td></tr></table></figure><h3 id="停止服务："><a href="#停止服务：" class="headerlink" title="停止服务："></a>停止服务：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务，</span></span><br><span class="line"><span class="comment"># 因为之前配置了密码，因此需要通过 -u 来指定密码</span></span><br><span class="line">redis-cli -u 123321 shutdown</span><br></pre></td></tr></table></figure><h3 id="1-3开机自启："><a href="#1-3开机自启：" class="headerlink" title="1.3开机自启："></a>1.3开机自启：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先创建一个系统服务文件</span></span><br><span class="line">vi /etc/systemd/system/redis.service</span><br></pre></td></tr></table></figure><h3 id="内容如下："><a href="#内容如下：" class="headerlink" title="内容如下："></a>内容如下：</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">[Unit]</span></span><br><span class="line"><span class="attr">Description</span>=<span class="string">redis-server</span></span><br><span class="line"><span class="attr">After</span>=<span class="string">network.target</span></span><br><span class="line"></span><br><span class="line"><span class="attr">[Service]</span></span><br><span class="line"><span class="attr">Type</span>=<span class="string">forking</span></span><br><span class="line"><span class="attr">ExecStart</span>=<span class="string">/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf</span></span><br><span class="line"><span class="attr">PrivateTmp</span>=<span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">[Install]</span></span><br><span class="line"><span class="attr">WantedBy</span>=<span class="string">multi-user.target</span></span><br></pre></td></tr></table></figure><h3 id="然后重载系统服务："><a href="#然后重载系统服务：" class="headerlink" title="然后重载系统服务："></a>然后重载系统服务：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br></pre></td></tr></table></figure><h3 id="现在，我们可以用下面这组命令来操作redis了："><a href="#现在，我们可以用下面这组命令来操作redis了：" class="headerlink" title="现在，我们可以用下面这组命令来操作redis了："></a>现在，我们可以用下面这组命令来操作redis了：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动</span></span><br><span class="line">systemctl start redis</span><br><span class="line"><span class="comment"># 停止</span></span><br><span class="line">systemctl stop redis</span><br><span class="line"><span class="comment"># 重启</span></span><br><span class="line">systemctl restart redis</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">systemctl status redis</span><br></pre></td></tr></table></figure><h3 id="执行下面的命令，可以让redis开机自启："><a href="#执行下面的命令，可以让redis开机自启：" class="headerlink" title="执行下面的命令，可以让redis开机自启："></a>执行下面的命令，可以让redis开机自启：</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> redis</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink</title>
      <link href="/posts/pd11.html"/>
      <url>/posts/pd11.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink快速上手</title>
      <link href="/posts/pd13.html"/>
      <url>/posts/pd13.html</url>
      
        <content type="html"><![CDATA[<h1 id="Flink快速上手"><a href="#Flink快速上手" class="headerlink" title="Flink快速上手"></a>Flink快速上手</h1><blockquote><p>[!NOTE]</p><p>前提准备好相关maven环境和依赖</p></blockquote><h2 id="1-WordCount代码编写"><a href="#1-WordCount代码编写" class="headerlink" title="1.WordCount代码编写"></a>1.WordCount代码编写</h2><h5 id="需求：统计一段文字中，每个单词出现的频次。"><a href="#需求：统计一段文字中，每个单词出现的频次。" class="headerlink" title="需求：统计一段文字中，每个单词出现的频次。"></a>需求：统计一段文字中，每个单词出现的频次。</h5><h5 id="环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。"><a href="#环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。" class="headerlink" title="环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。"></a>环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。</h5><h3 id="1-1-批处理"><a href="#1-1-批处理" class="headerlink" title="1.1  批处理"></a>1.1  批处理</h3><h5 id="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"><a href="#批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。" class="headerlink" title="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"></a>批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。</h5><h4 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1)数据准备"></a>1)数据准备</h4><ul><li>（1）在工程根目录下新建一个input文件夹，并在下面创建文本文件words.txt</li><li>（2）在words.txt中输入一些文字，例如：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello flink</span><br><span class="line">hello world</span><br><span class="line">hello java</span><br></pre></td></tr></table></figure><h4 id="2-代码编写"><a href="#2-代码编写" class="headerlink" title="2)代码编写"></a>2)代码编写</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建一个执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineData = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据集进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineData.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneGroup = wordAndOne.groupBy(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合统计</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）输出"><a href="#（2）输出" class="headerlink" title="（2）输出"></a>（2）输出</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(flink,1)</span><br><span class="line">(world,1)</span><br><span class="line">(hello,3)</span><br><span class="line">(java,1)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>需要注意的是，这种代码的实现方式，是基于DataSet API的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上Flink本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的API来实现。所以从Flink 1.12开始，官方推荐的做法是直接使用DataStream API，在提交任务时通过将执行模式设为BATCH来进行批处理：</p><p>$ bin&#x2F;flink run -Dexecution.runtime-mode&#x3D;BATCH BatchWordCount.jar</p><p>这样，DataSet API就没什么用了，在实际应用中我们只要维护一套DataStream API就可以。这里只是为了方便大家理解，我们依然用DataSet API做了批处理的实现。</p></blockquote><h3 id="1-2流处理"><a href="#1-2流处理" class="headerlink" title="1.2流处理"></a>1.2流处理</h3><blockquote><p>[!CAUTION]</p><p>对于Flink而言，流才是整个处理逻辑的底层核心，所以流批统一之后的DataStream API更加强大，可以直接处理批处理和流处理的所有场景。</p></blockquote><h5 id="我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"><a href="#我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致" class="headerlink" title="我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"></a>我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BoundedStreamWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineDataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndGroup = wordAndOne.keyBy(_._1)</span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合操作</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行任务</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3&gt; (java,1)</span><br><span class="line">5&gt; (hello,1)</span><br><span class="line">5&gt; (hello,2)</span><br><span class="line">5&gt; (hello,3)</span><br><span class="line">13&gt; (flink,1)</span><br><span class="line">9&gt; (world,1)</span><br></pre></td></tr></table></figure><h3 id="主要观察与批处理程序BatchWordCount的不同："><a href="#主要观察与批处理程序BatchWordCount的不同：" class="headerlink" title="主要观察与批处理程序BatchWordCount的不同："></a>主要观察与批处理程序BatchWordCount的不同：</h3><ul><li>创建执行环境的不同，流处理程序使用的是StreamExecutionEnvironment</li><li>转换处理之后，得到的数据对象类型不同</li><li>分组操作调用的是keyBy方法，可以传入一个匿名函数作为键选择器（KeySelector），指定当前分组的key是什么</li><li>代码末尾需要调用env的execute方法，开始执行任务</li></ul><h4 id="2）读取socket文本流"><a href="#2）读取socket文本流" class="headerlink" title="2）读取socket文本流"></a>2）读取socket文本流</h4><blockquote><p>[!NOTE]</p><p>在实际的生产环境中，真正的数据流其实是无界的，有开始却没有结束，这就要求我们需要持续地处理捕获的数据。为了模拟这种场景，可以监听socket端口，然后向该端口不断的发送数据。</p></blockquote><h4 id="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："><a href="#（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：" class="headerlink" title="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："></a>（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.<span class="type">ParameterTool</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCOunt</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> parameterTool = <span class="type">ParameterTool</span>.fromArgs(args)</span><br><span class="line">    <span class="keyword">val</span> hostname = parameterTool.get(<span class="string">&quot;host&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> port = parameterTool.getInt(<span class="string">&quot;port&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineDataStream = env.socketTextStream(hostname, port)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineAndOneGroup = wordAndOne.keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sum = lineAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"><a href="#（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试" class="headerlink" title="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"></a>（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ nc -lk 7777</span><br></pre></td></tr></table></figure><blockquote><p>[!IMPORTANT]</p><p>注意：要先启动端口，后启动StreamWordCount程序，否则会报超时连接异常。</p></blockquote><h4 id="（3）启动StreamWordCount程序"><a href="#（3）启动StreamWordCount程序" class="headerlink" title="（3）启动StreamWordCount程序"></a>（3）启动StreamWordCount程序</h4><blockquote><p>[!WARNING]</p><p>我们会发现程序启动之后没有任何输出、也不会退出。这是正常的，因为Flink的流处理是事件驱动的，当前程序会一直处于监听状态，只有接收到数据才会执行任务、输出统计结果。</p></blockquote><h5 id="（4）从hadoop102发送数据"><a href="#（4）从hadoop102发送数据" class="headerlink" title="（4）从hadoop102发送数据"></a>（4）从hadoop102发送数据</h5><h5 id="①在hadoop102主机中，输入“hello-flink”，输出如下内容"><a href="#①在hadoop102主机中，输入“hello-flink”，输出如下内容" class="headerlink" title="①在hadoop102主机中，输入“hello flink”，输出如下内容"></a>①在hadoop102主机中，输入“hello flink”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13&gt; (flink,1)</span><br><span class="line">5&gt; (hello,1)</span><br></pre></td></tr></table></figure><h5 id="②再输入“hello-world”，输出如下内容"><a href="#②再输入“hello-world”，输出如下内容" class="headerlink" title="②再输入“hello world”，输出如下内容"></a>②再输入“hello world”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2&gt; (world,1)</span><br><span class="line">5&gt; (hello,2)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p><p>因为对于flatMap里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink中的时间和窗口、水位线、</title>
      <link href="/posts/pd15.html"/>
      <url>/posts/pd15.html</url>
      
        <content type="html"><![CDATA[<h5 id="在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。"><a href="#在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。" class="headerlink" title="在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。"></a>在批处理统计中，我们可以等待一批数据都到齐后，统一处理。但是在实时处理统计中，我们是来一条就得处理一条，那么我们怎么统计最近一段时间内的数据呢？引入“窗口”。</h5><h5 id="所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。"><a href="#所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。" class="headerlink" title="所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。"></a>所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下Flink中的时间语义和窗口的应用。</h5><h2 id="1-1-窗口（Window）"><a href="#1-1-窗口（Window）" class="headerlink" title="1.1 窗口（Window）"></a><strong>1.1</strong> 窗口（Window）</h2><h3 id="1-1-窗口的概念"><a href="#1-1-窗口的概念" class="headerlink" title="1.1 窗口的概念"></a><strong>1.1</strong> <strong>窗口的概念</strong></h3><h5 id="Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。"><a href="#Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。" class="headerlink" title="Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。"></a>Flink是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。</h5><p><img src="https://pic1.imgdb.cn/item/67871bf5d0e0a243d4f45ac6.png"></p><p><strong>注意：</strong>Flink中窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。另外，这里我们认为到达窗口结束时间时，窗口就触发计算并关闭，事实上“触发计算”和“窗口关闭”两个行为也可以分开，这部分内容我们会在后面详述。</p><h3 id="6-1-2-窗口的分类"><a href="#6-1-2-窗口的分类" class="headerlink" title="6.1.2 窗口的分类"></a><strong>6.1.2</strong> <strong>窗口的分类</strong></h3><p>我们在上一节举的例子，其实是最为简单的一种时间窗口。在Flink中，窗口的应用非常灵活，我们可以使用各种不同类型的窗口来实现需求。接下来我们就从不同的角度，对Flink中内置的窗口做一个分类说明。</p><p>1）按照驱动类型分</p><p><img src="https://pic1.imgdb.cn/item/67871bf5d0e0a243d4f45ac6.png"></p><h4 id="2）按照窗口分配数据的规则分类"><a href="#2）按照窗口分配数据的规则分类" class="headerlink" title="2）按照窗口分配数据的规则分类"></a>2）按照窗口分配数据的规则分类</h4><h5 id="根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling-Window）、滑动窗口（Sliding-Window）、会话窗口（Session-Window），以及全局窗口（Global-Window）。"><a href="#根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling-Window）、滑动窗口（Sliding-Window）、会话窗口（Session-Window），以及全局窗口（Global-Window）。" class="headerlink" title="根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）、会话窗口（Session Window），以及全局窗口（Global Window）。"></a>根据分配数据的规则，窗口的具体实现可以分为4类：滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）、会话窗口（Session Window），以及全局窗口（Global Window）。</h5><p><img src="https://pic1.imgdb.cn/item/67871d1ed0e0a243d4f45b16.png"></p><p><img src="https://pic1.imgdb.cn/item/67871d3fd0e0a243d4f45b18.png"></p><p><img src="https://pic1.imgdb.cn/item/67871d6ed0e0a243d4f45b3d.png"></p><p><img src="C:/Users/LHX/AppData/Roaming/Typora/typora-user-images/image-20250115103117635.png" alt="image-20250115103117635"></p><h3 id="1-1-3-窗口API概览"><a href="#1-1-3-窗口API概览" class="headerlink" title="1..1.3 窗口API概览"></a>1..1.3 窗口API概览</h3><h3 id="1）按键分区（Keyed）和非按键分区（Non-Keyed）"><a href="#1）按键分区（Keyed）和非按键分区（Non-Keyed）" class="headerlink" title="1）按键分区（Keyed）和非按键分区（Non-Keyed）"></a>1）按键分区（Keyed）和非按键分区（Non-Keyed）</h3><h4 id="在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。"><a href="#在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。" class="headerlink" title="在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。"></a>在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流KeyedStream来开窗，还是直接在没有按键分区的DataStream上开窗。也就是说，在调用窗口算子之前，是否有keyBy操作。</h4><h5 id="（1）按键分区窗口（Keyed-Windows）"><a href="#（1）按键分区窗口（Keyed-Windows）" class="headerlink" title="（1）按键分区窗口（Keyed Windows）"></a>（1）按键分区窗口（Keyed Windows）</h5><h5 id="经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical-streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。"><a href="#经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical-streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。" class="headerlink" title="经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。"></a>经过按键分区keyBy操作后，数据流会按照key被分为多条逻辑流（logical streams），这就是KeyedStream。基于KeyedStream进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同key的数据会被发送到同一个并行子任务，而窗口操作会基于每个key进行单独的处理。所以可以认为，每个key上都定义了一组窗口，各自独立地进行统计计算。</h5><h5 id="在代码实现上，我们需要先对DataStream调用-keyBy-进行按键分区，然后再调用-window-定义窗口。"><a href="#在代码实现上，我们需要先对DataStream调用-keyBy-进行按键分区，然后再调用-window-定义窗口。" class="headerlink" title="在代码实现上，我们需要先对DataStream调用.keyBy()进行按键分区，然后再调用.window()定义窗口。"></a>在代码实现上，我们需要先对DataStream调用.keyBy()进行按键分区，然后再调用.window()定义窗口。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br></pre></td></tr></table></figure><h5 id="（2）非按键分区（Non-Keyed-Windows）"><a href="#（2）非按键分区（Non-Keyed-Windows）" class="headerlink" title="（2）非按键分区（Non-Keyed Windows）"></a>（2）非按键分区（Non-Keyed Windows）</h5><h5 id="如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。"><a href="#如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。" class="headerlink" title="如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。"></a>如果没有进行keyBy，那么原始的DataStream就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了1。</h5><h5 id="在代码中，直接基于DataStream调用-windowAll-定义窗口。"><a href="#在代码中，直接基于DataStream调用-windowAll-定义窗口。" class="headerlink" title="在代码中，直接基于DataStream调用.windowAll()定义窗口。"></a>在代码中，直接基于DataStream调用.windowAll()定义窗口。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.windowAll(...)</span><br></pre></td></tr></table></figure><h5 id="注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。"><a href="#注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。" class="headerlink" title="注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。"></a>注意：对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，windowAll本身就是一个非并行的操作。</h5><h5 id="2）代码中窗口API的调用"><a href="#2）代码中窗口API的调用" class="headerlink" title="2）代码中窗口API的调用"></a>2）代码中窗口API的调用</h5><h5 id="窗口操作主要有两个部分：窗口分配器（Window-Assigners）和窗口函数（Window-Functions）。"><a href="#窗口操作主要有两个部分：窗口分配器（Window-Assigners）和窗口函数（Window-Functions）。" class="headerlink" title="窗口操作主要有两个部分：窗口分配器（Window Assigners）和窗口函数（Window Functions）。"></a>窗口操作主要有两个部分：窗口分配器（Window Assigners）和窗口函数（Window Functions）。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(&lt;key selector&gt;)</span><br><span class="line">       .window(&lt;window assigner&gt;)</span><br><span class="line">       .aggregate(&lt;window function&gt;)</span><br></pre></td></tr></table></figure><h5 id="其中-window-方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的-aggregate-方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只-aggregate-一种，我们接下来就详细展开讲解。"><a href="#其中-window-方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的-aggregate-方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只-aggregate-一种，我们接下来就详细展开讲解。" class="headerlink" title="其中.window()方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的.aggregate()方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只.aggregate()一种，我们接下来就详细展开讲解。"></a>其中.window()方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的.aggregate()方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式，而窗口函数的调用方法也不只.aggregate()一种，我们接下来就详细展开讲解。</h5><h3 id="1-1-4-窗口分配器"><a href="#1-1-4-窗口分配器" class="headerlink" title="1.1.4 窗口分配器"></a><strong>1.1.4</strong> <strong>窗口分配器</strong></h3><h5 id="定义窗口分配器（Window-Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。"><a href="#定义窗口分配器（Window-Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。" class="headerlink" title="定义窗口分配器（Window Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。"></a>定义窗口分配器（Window Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。所以可以说，窗口分配器其实就是在指定窗口的类型。</h5><h5 id="窗口分配器最通用的定义方式，就是调用-window-方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用-windowAll-方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。"><a href="#窗口分配器最通用的定义方式，就是调用-window-方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用-windowAll-方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。" class="headerlink" title="窗口分配器最通用的定义方式，就是调用.window()方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用.windowAll()方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。"></a>窗口分配器最通用的定义方式，就是调用.window()方法。这个方法需要传入一个WindowAssigner作为参数，返回WindowedStream。如果是非按键分区窗口，那么直接调用.windowAll()方法，同样传入一个WindowAssigner，返回的是AllWindowedStream。</h5><h5 id="窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。"><a href="#窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。" class="headerlink" title="窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。"></a>窗口按照驱动类型可以分成时间窗口和计数窗口，而按照具体的分配规则，又有滚动窗口、滑动窗口、会话窗口、全局窗口四种。除去需要自定义的全局窗口外，其他常用的类型Flink中都给出了内置的分配器实现，我们可以方便地调用实现各种需求。</h5><h4 id="1-1-4-1-时间窗口"><a href="#1-1-4-1-时间窗口" class="headerlink" title="1.1.4.1 时间窗口"></a>1.1.4.1 时间窗口</h4><h5 id="时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。"><a href="#时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。" class="headerlink" title="时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。"></a>时间窗口是最常用的窗口类型，又可以细分为滚动、滑动和会话三种。</h5><h5 id="（1）滚动处理时间窗口"><a href="#（1）滚动处理时间窗口" class="headerlink" title="（1）滚动处理时间窗口"></a>（1）滚动处理时间窗口</h5><h5 id="窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法-of-。"><a href="#窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法-of-。" class="headerlink" title="窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法.of()。"></a>窗口分配器由类TumblingProcessingTimeWindows提供，需要调用它的静态方法.of()。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">TumblingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h5 id="这里-of-方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。"><a href="#这里-of-方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。" class="headerlink" title="这里.of()方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。"></a>这里.of()方法需要传入一个Time类型的参数size，表示滚动窗口的大小，我们这里创建了一个长度为5秒的滚动窗口。</h5><h5 id="另外，-of-还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。"><a href="#另外，-of-还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。" class="headerlink" title="另外，.of()还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。"></a>另外，.of()还有一个重载方法，可以传入两个Time类型的参数：size和offset。第一个参数当然还是窗口大小，第二个参数则表示窗口起始点的偏移量。</h5><h4 id="（2）滑动处理时间窗口"><a href="#（2）滑动处理时间窗口" class="headerlink" title="（2）滑动处理时间窗口"></a>（2）滑动处理时间窗口</h4><h5 id="窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法-of-。"><a href="#窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法-of-。" class="headerlink" title="窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法.of()。"></a>窗口分配器由类SlidingProcessingTimeWindows提供，同样需要调用它的静态方法.of()。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">SlidingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)，<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h5 id="这里-of-方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。"><a href="#这里-of-方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。" class="headerlink" title="这里.of()方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。"></a>这里.of()方法需要传入两个Time类型的参数：size和slide，前者表示滑动窗口的大小，后者表示滑动窗口的滑动步长。我们这里创建了一个长度为10秒、滑动步长为5秒的滑动窗口。</h5><h5 id="滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。"><a href="#滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。" class="headerlink" title="滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。"></a>滑动窗口同样可以追加第三个参数，用于指定窗口起始点的偏移量，用法与滚动窗口完全一致。</h5><h4 id="（3）处理时间会话窗口"><a href="#（3）处理时间会话窗口" class="headerlink" title="（3）处理时间会话窗口"></a>（3）处理时间会话窗口</h4><h5 id="窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法-withGap-或者-withDynamicGap-。"><a href="#窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法-withGap-或者-withDynamicGap-。" class="headerlink" title="窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法.withGap()或者.withDynamicGap()。"></a>窗口分配器由类ProcessingTimeSessionWindows提供，需要调用它的静态方法.withGap()或者.withDynamicGap()。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">ProcessingTimeSessionWindows</span>.withGap(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h5 id="这里-withGap-方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session-gap。我们这里创建了静态会话超时时间为10秒的会话窗口。"><a href="#这里-withGap-方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session-gap。我们这里创建了静态会话超时时间为10秒的会话窗口。" class="headerlink" title="这里.withGap()方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session gap。我们这里创建了静态会话超时时间为10秒的会话窗口。"></a>这里.withGap()方法需要传入一个Time类型的参数size，表示会话的超时时间，也就是最小间隔session gap。我们这里创建了静态会话超时时间为10秒的会话窗口。</h5><h5 id="另外，还可以调用withDynamicGap-方法定义session-gap的动态提取逻辑。"><a href="#另外，还可以调用withDynamicGap-方法定义session-gap的动态提取逻辑。" class="headerlink" title="另外，还可以调用withDynamicGap()方法定义session gap的动态提取逻辑。"></a>另外，还可以调用withDynamicGap()方法定义session gap的动态提取逻辑。</h5><h4 id="（4）滚动事件时间窗口"><a href="#（4）滚动事件时间窗口" class="headerlink" title="（4）滚动事件时间窗口"></a>（4）滚动事件时间窗口</h4><h5 id="窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。"><a href="#窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。" class="headerlink" title="窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。"></a>窗口分配器由类TumblingEventTimeWindows提供，用法与滚动处理事件窗口完全一致。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h4 id="（5）滑动事件时间窗口"><a href="#（5）滑动事件时间窗口" class="headerlink" title="（5）滑动事件时间窗口"></a>（5）滑动事件时间窗口</h4><h5 id="窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。"><a href="#窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。" class="headerlink" title="窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。"></a>窗口分配器由类SlidingEventTimeWindows提供，用法与滑动处理事件窗口完全一致。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)，<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h4 id="（6）事件时间会话窗口"><a href="#（6）事件时间会话窗口" class="headerlink" title="（6）事件时间会话窗口"></a>（6）事件时间会话窗口</h4><h5 id="窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。"><a href="#窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。" class="headerlink" title="窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。"></a>窗口分配器由类EventTimeSessionWindows提供，用法与处理事件会话窗口完全一致。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">EventTimeSessionWindows</span>.withGap(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">       .aggregate(...)</span><br></pre></td></tr></table></figure><h3 id="1-1-4-2-计数窗口"><a href="#1-1-4-2-计数窗口" class="headerlink" title="1.1.4.2 计数窗口"></a>1.1.4.2 计数窗口</h3><h5 id="计数窗口概念非常简单，本身底层是基于全局窗口（Global-Window）实现的。Flink为我们提供了非常方便的接口：直接调用-countWindow-方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。"><a href="#计数窗口概念非常简单，本身底层是基于全局窗口（Global-Window）实现的。Flink为我们提供了非常方便的接口：直接调用-countWindow-方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。" class="headerlink" title="计数窗口概念非常简单，本身底层是基于全局窗口（Global Window）实现的。Flink为我们提供了非常方便的接口：直接调用.countWindow()方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。"></a>计数窗口概念非常简单，本身底层是基于全局窗口（Global Window）实现的。Flink为我们提供了非常方便的接口：直接调用.countWindow()方法。根据分配规则的不同，又可以分为滚动计数窗口和滑动计数窗口两类，下面我们就来看它们的具体实现。</h5><h4 id="（1）滚动计数窗口"><a href="#（1）滚动计数窗口" class="headerlink" title="（1）滚动计数窗口"></a>（1）滚动计数窗口</h4><h4 id="滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。"><a href="#滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。" class="headerlink" title="滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。"></a>滚动计数窗口只需要传入一个长整型的参数size，表示窗口的大小。</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .countWindow(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h4 id="我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。"><a href="#我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。" class="headerlink" title="我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。"></a>我们定义了一个长度为10的滚动计数窗口，当窗口中元素数量达到10的时候，就会触发计算执行并关闭窗口。</h4><h4 id="（2）滑动计数窗口"><a href="#（2）滑动计数窗口" class="headerlink" title="（2）滑动计数窗口"></a>（2）滑动计数窗口</h4><h5 id="与滚动计数窗口类似，不过需要在-countWindow-调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。"><a href="#与滚动计数窗口类似，不过需要在-countWindow-调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。" class="headerlink" title="与滚动计数窗口类似，不过需要在.countWindow()调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。"></a>与滚动计数窗口类似，不过需要在.countWindow()调用时传入两个参数：size和slide，前者表示窗口大小，后者表示滑动步长。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .countWindow(<span class="number">10</span>，<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h5 id="我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。"><a href="#我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。" class="headerlink" title="我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。"></a>我们定义了一个长度为10、滑动步长为3的滑动计数窗口。每个窗口统计10个数据，每隔3个数据就统计输出一次结果。</h5><h4 id="3）全局窗口"><a href="#3）全局窗口" class="headerlink" title="3）全局窗口"></a>3）全局窗口</h4><h5 id="全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用-window-，分配器由GlobalWindows类提供。"><a href="#全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用-window-，分配器由GlobalWindows类提供。" class="headerlink" title="全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用.window()，分配器由GlobalWindows类提供。"></a>全局窗口是计数窗口的底层实现，一般在需要自定义窗口时使用。它的定义同样是直接调用.window()，分配器由GlobalWindows类提供。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(<span class="type">GlobalWindows</span>.create());</span><br></pre></td></tr></table></figure><h5 id="需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。"><a href="#需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。" class="headerlink" title="需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。"></a>需要注意使用全局窗口，必须自行定义触发器才能实现窗口计算，否则起不到任何作用。</h5><h3 id="1-1-5-窗口函数"><a href="#1-1-5-窗口函数" class="headerlink" title="1.1.5 窗口函数"></a><strong>1.1.5</strong> <strong>窗口函数</strong></h3><p><img src="https://pic1.imgdb.cn/item/6787210fd0e0a243d4f45c59.png"></p><h5 id="窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。"><a href="#窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。" class="headerlink" title="窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。"></a>窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。下面我们来进行分别讲解。</h5><h4 id="1-1-5-1-增量聚合函数（ReduceFunction-x2F-AggregateFunction）"><a href="#1-1-5-1-增量聚合函数（ReduceFunction-x2F-AggregateFunction）" class="headerlink" title="1.1.5.1 增量聚合函数（ReduceFunction &#x2F; AggregateFunction）"></a>1.1.5.1 增量聚合函数（ReduceFunction &#x2F; AggregateFunction）</h4><h5 id="窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。"><a href="#窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。" class="headerlink" title="窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。"></a>窗口将数据收集起来，最基本的处理操作当然就是进行聚合。我们可以每来一个数据就在之前结果上聚合一次，这就是“增量聚合”。</h5><h5 id="典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。"><a href="#典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。" class="headerlink" title="典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。"></a>典型的增量聚合函数有两个：ReduceFunction和AggregateFunction。</h5><h5 id="1）归约函数（ReduceFunction）"><a href="#1）归约函数（ReduceFunction）" class="headerlink" title="1）归约函数（ReduceFunction）"></a>1）归约函数（ReduceFunction）</h5><h5 id="代码示例："><a href="#代码示例：" class="headerlink" title="代码示例："></a>代码示例：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WindowReduceDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(<span class="keyword">new</span> <span class="title class_">WaterSensorMapFunction</span>())</span><br><span class="line">                .keyBy(r -&gt; r.getId())</span><br><span class="line">                <span class="comment">// 设置滚动事件时间窗口</span></span><br><span class="line">                .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">                .reduce(<span class="keyword">new</span> <span class="title class_">ReduceFunction</span>&lt;WaterSensor&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> WaterSensor <span class="title function_">reduce</span><span class="params">(WaterSensor value1, WaterSensor value2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;调用reduce方法，之前的结果:&quot;</span>+value1 + <span class="string">&quot;,现在来的数据:&quot;</span>+value2);</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WaterSensor</span>(value1.getId(), System.currentTimeMillis(),value1.getVc()+value2.getVc());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2）聚合函数（AggregateFunction）"><a href="#2）聚合函数（AggregateFunction）" class="headerlink" title="2）聚合函数（AggregateFunction）"></a>2）聚合函数（AggregateFunction）</h4><h5 id="ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。"><a href="#ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。" class="headerlink" title="ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。"></a>ReduceFunction可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。</h5><h5 id="Flink-Window-API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。"><a href="#Flink-Window-API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。" class="headerlink" title="Flink Window API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。"></a>Flink Window API中的aggregate就突破了这个限制，可以定义更加灵活的窗口聚合操作。这个方法需要传入一个AggregateFunction的实现类作为参数。</h5><h5 id="AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。"><a href="#AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。" class="headerlink" title="AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。"></a>AggregateFunction可以看作是ReduceFunction的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型IN就是输入流中元素的数据类型；累加器类型ACC则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。</h5><h5 id="接口中有四个方法："><a href="#接口中有四个方法：" class="headerlink" title="接口中有四个方法："></a>接口中有四个方法：</h5><ul><li>createAccumulator()：创建一个累加器，这就是为聚合创建了一个初始状态，每个聚合任务只会调用一次。</li><li>add()：将输入的元素添加到累加器中。</li><li>getResult()：从累加器中提取聚合的输出结果。</li><li>merge()：合并两个累加器，并将合并后的状态作为一个累加器返回。</li></ul><h5 id="所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator-为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add-方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult-方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。"><a href="#所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator-为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add-方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult-方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。" class="headerlink" title="所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator()为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add()方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult()方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。"></a>所以可以看到，AggregateFunction的工作原理是：首先调用createAccumulator()为任务初始化一个状态（累加器）；而后每来一个数据就调用一次add()方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用getResult()方法得到计算结果。很明显，与ReduceFunction相同，AggregateFunction也是增量式的聚合；而由于输入、中间状态、输出的类型可以不同，使得应用更加灵活方便。</h5><h5 id="代码实现如下："><a href="#代码实现如下：" class="headerlink" title="代码实现如下："></a>代码实现如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">AggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AggregateFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">ClickSource</span>)</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计pv 和 uv 输出pv/uv的值</span></span><br><span class="line">    stream.keyBy(data =&gt; <span class="literal">true</span>)  <span class="comment">// 这里就是让所有数据进入同一个分组</span></span><br><span class="line">      .window( <span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">2</span>)))</span><br><span class="line">      .aggregate( <span class="keyword">new</span> <span class="type">PvUv</span> )</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义聚合函数, 用二元组(LOng, Set)表示中间聚合的(pv, uv)状态</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">PvUv</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">Event</span>, (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]), <span class="title">Double</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]) = (<span class="number">0</span>L, <span class="type">Set</span>[<span class="type">String</span>]())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每来一条数据,都回进行add叠加聚合</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(value: <span class="type">Event</span>, accumulator: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>])): (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]) = (accumulator._1 + <span class="number">1</span>, accumulator._2 + value.user)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回最终的计算结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResult</span></span>(accumulator: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>])): <span class="type">Double</span> = accumulator._1.toDouble /accumulator._2.size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(a: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]), b: (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>])): (<span class="type">Long</span>, <span class="type">Set</span>[<span class="type">String</span>]) = ???</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括-sum-x2F-max-x2F-maxBy-x2F-min-x2F-minBy-，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。"><a href="#另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括-sum-x2F-max-x2F-maxBy-x2F-min-x2F-minBy-，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。" class="headerlink" title="另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括.sum()&#x2F;max()&#x2F;maxBy()&#x2F;min()&#x2F;minBy()，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。"></a>另外，Flink也为窗口的聚合提供了一系列预定义的简单聚合方法，可以直接基于WindowedStream调用。主要包括.sum()&#x2F;max()&#x2F;maxBy()&#x2F;min()&#x2F;minBy()，与KeyedStream的简单聚合非常相似。它们的底层，其实都是通过AggregateFunction来实现的。</h5><h4 id="1-1-5-2-全窗口函数（full-window-functions）"><a href="#1-1-5-2-全窗口函数（full-window-functions）" class="headerlink" title="1.1.5.2 全窗口函数（full window functions）"></a>1.1.5.2 全窗口函数（full window functions）</h4><h5 id="有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。"><a href="#有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。" class="headerlink" title="有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。"></a>有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息（比如窗口的起始时间），这是增量聚合函数做不到的。</h5><h4 id="所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。"><a href="#所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。" class="headerlink" title="所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。"></a>所以，我们还需要有更丰富的窗口计算方式。窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。</h4><h4 id="在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。"><a href="#在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。" class="headerlink" title="在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。"></a>在Flink中，全窗口函数也有两种：WindowFunction和ProcessWindowFunction。</h4><h5 id="1）窗口函数（WindowFunction）"><a href="#1）窗口函数（WindowFunction）" class="headerlink" title="1）窗口函数（WindowFunction）"></a>1）窗口函数（WindowFunction）</h5><h5 id="WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用-apply-方法，传入一个WindowFunction的实现类。"><a href="#WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用-apply-方法，传入一个WindowFunction的实现类。" class="headerlink" title="WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用.apply()方法，传入一个WindowFunction的实现类。"></a>WindowFunction字面上就是“窗口函数”，它其实是老版本的通用窗口函数接口。我们可以基于WindowedStream调用.apply()方法，传入一个WindowFunction的实现类。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(&lt;window assigner&gt;)</span><br><span class="line">    .apply(<span class="keyword">new</span> <span class="type">MyWindowFunction</span>());</span><br></pre></td></tr></table></figure><h5 id="这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。"><a href="#这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。" class="headerlink" title="这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。"></a>这个类中可以获取到包含窗口所有数据的可迭代集合（Iterable），还可以拿到窗口（Window）本身的信息。</h5><h5 id="不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。"><a href="#不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。" class="headerlink" title="不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。"></a>不过WindowFunction能提供的上下文信息较少，也没有更高级的功能。事实上，它的作用可以被ProcessWindowFunction全覆盖，所以之后可能会逐渐弃用。</h5><h4 id="2）处理窗口函数（ProcessWindowFunction）"><a href="#2）处理窗口函数（ProcessWindowFunction）" class="headerlink" title="2）处理窗口函数（ProcessWindowFunction）"></a>2）处理窗口函数（ProcessWindowFunction）</h4><h5 id="ProcessWindowFunction是Window-API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing-time）和事件时间水位线（event-time-watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。"><a href="#ProcessWindowFunction是Window-API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing-time）和事件时间水位线（event-time-watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。" class="headerlink" title="ProcessWindowFunction是Window API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing time）和事件时间水位线（event time watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。"></a>ProcessWindowFunction是Window API中最底层的通用窗口函数接口。之所以说它“最底层”，是因为除了可以拿到窗口中的所有数据之外，ProcessWindowFunction还可以获取到一个“上下文对象”（Context）。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的时间和状态信息。这里的时间就包括了处理时间（processing time）和事件时间水位线（event time watermark）。这就使得ProcessWindowFunction更加灵活、功能更加丰富，其实就是一个增强版的WindowFunction。</h5><h5 id="事实上，ProcessWindowFunction是Flink底层API——处理函数（process-function）中的一员，关于处理函数我们会在后续章节展开讲解。"><a href="#事实上，ProcessWindowFunction是Flink底层API——处理函数（process-function）中的一员，关于处理函数我们会在后续章节展开讲解。" class="headerlink" title="事实上，ProcessWindowFunction是Flink底层API——处理函数（process function）中的一员，关于处理函数我们会在后续章节展开讲解。"></a>事实上，ProcessWindowFunction是Flink底层API——处理函数（process function）中的一员，关于处理函数我们会在后续章节展开讲解。</h5><h5 id="代码实现如下：-1"><a href="#代码实现如下：-1" class="headerlink" title="代码实现如下："></a>代码实现如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> com.day03.<span class="type">AggregateFunctionTest</span>.<span class="type">PvUv</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.&#123;<span class="type">SlidingEventTimeWindows</span>, <span class="type">TumblingEventTimeWindows</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FullWindowFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource( <span class="keyword">new</span> <span class="type">ClickSource</span> )</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// pv是指页面被浏览的总次数，每次用户打开或刷新一个页面，都会增加一次Pv</span></span><br><span class="line">    <span class="comment">// uv是指一定时间内访问网站独立用户数，同一用户多次访问记为一次uv</span></span><br><span class="line">    <span class="comment">// 测试全窗口函数， 统计uv</span></span><br><span class="line">    stream.keyBy(data =&gt; <span class="string">&quot;key&quot;</span>)</span><br><span class="line">      .window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>)))</span><br><span class="line">      .process(<span class="keyword">new</span> <span class="type">UvCountByWindow</span> )</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义实现ProcessWindowFunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UvCountByWindow</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Event</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(key: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">Event</span>], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 使用过一个Set进行去重操作</span></span><br><span class="line">      <span class="keyword">var</span> userSet = <span class="type">Set</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 从elements中提取所有数据，一次放入set中去重</span></span><br><span class="line">      elements.foreach(userSet += _.user)</span><br><span class="line">      <span class="keyword">val</span> uv = userSet.size</span><br><span class="line">      <span class="comment">// 提取窗口信息包装String输出</span></span><br><span class="line">      <span class="keyword">val</span> windowEnd = context.window.getEnd</span><br><span class="line">      <span class="keyword">val</span> windowsStart = context.window.getStart</span><br><span class="line"></span><br><span class="line">      out.collect(<span class="string">s&quot;窗口 <span class="subst">$windowsStart</span>~<span class="subst">$windowEnd</span> 的uv值为： <span class="subst">$uv</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-1-5-3-增量聚合和全窗口函数的结合使用"><a href="#1-1-5-3-增量聚合和全窗口函数的结合使用" class="headerlink" title="1.1.5.3 增量聚合和全窗口函数的结合使用"></a>1.1.5.3 增量聚合和全窗口函数的结合使用</h4><h6 id="在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window-API就给我们实现了这样的用法。"><a href="#在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window-API就给我们实现了这样的用法。" class="headerlink" title="在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window API就给我们实现了这样的用法。"></a>在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。Flink的Window API就给我们实现了这样的用法。</h6><h6 id="我们之前在调用WindowedStream的-reduce-和-aggregate-方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。"><a href="#我们之前在调用WindowedStream的-reduce-和-aggregate-方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。" class="headerlink" title="我们之前在调用WindowedStream的.reduce()和.aggregate()方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。"></a>我们之前在调用WindowedStream的.reduce()和.aggregate()方法时，只是简单地直接传入了一个ReduceFunction或AggregateFunction进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是WindowFunction或者ProcessWindowFunction。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">AggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UrlViewCountExample</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义统计输出结果的数据结构</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCount</span>(<span class="params">url: <span class="type">String</span>, count: <span class="type">Long</span>, windowStart: <span class="type">Long</span>, windowEnd: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource( <span class="keyword">new</span> <span class="type">ClickSource</span> )</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 结合使用增量聚合函数和全窗口函数，包装统计信息</span></span><br><span class="line">    stream.keyBy(_.url)</span><br><span class="line">      .window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">      .aggregate(<span class="keyword">new</span> <span class="type">UrlViewCountAgg</span>, <span class="keyword">new</span> <span class="type">UrlViewCountResult</span>)</span><br><span class="line">      .print</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现增强聚合函数</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountAgg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">Event</span>, <span class="type">Long</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">Long</span> = <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(value: <span class="type">Event</span>, accumulator: <span class="type">Long</span>): <span class="type">Long</span> = accumulator + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResult</span></span>(accumulator: <span class="type">Long</span>): <span class="type">Long</span> = accumulator</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(a: <span class="type">Long</span>, b: <span class="type">Long</span>): <span class="type">Long</span> = ???</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现全窗口函数</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Long</span>, <span class="type">UrlViewCount</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(url: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">Long</span>], out: <span class="type">Collector</span>[<span class="type">UrlViewCount</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 提取需要的数据</span></span><br><span class="line">      <span class="keyword">val</span> count = elements.iterator.next()</span><br><span class="line">      <span class="keyword">val</span> start = context.window.getStart</span><br><span class="line">      <span class="keyword">val</span> end = context.window.getEnd</span><br><span class="line"></span><br><span class="line">      out.collect(<span class="type">UrlViewCount</span>(url, count, start, end))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-1-6-其他API"><a href="#1-1-6-其他API" class="headerlink" title="1.1.6 其他API"></a><strong>1.1.6</strong> <strong>其他API</strong></h3><h5 id="对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。"><a href="#对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。" class="headerlink" title="对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。"></a>对于一个窗口算子而言，窗口分配器和窗口函数是必不可少的。除此之外，Flink还提供了其他一些可选的API，让我们可以更加灵活地控制窗口行为。</h5><h4 id="1-1-6-1-触发器（Trigger）"><a href="#1-1-6-1-触发器（Trigger）" class="headerlink" title="1.1.6.1 触发器（Trigger）"></a>1.1.6.1 触发器（Trigger）</h4><h5 id="触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。"><a href="#触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。" class="headerlink" title="触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。"></a>触发器主要是用来控制窗口什么时候触发计算。所谓的“触发计算”，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。</h5><h5 id="基于WindowedStream调用-trigger-方法，就可以传入一个自定义的窗口触发器（Trigger）。"><a href="#基于WindowedStream调用-trigger-方法，就可以传入一个自定义的窗口触发器（Trigger）。" class="headerlink" title="基于WindowedStream调用.trigger()方法，就可以传入一个自定义的窗口触发器（Trigger）。"></a>基于WindowedStream调用.trigger()方法，就可以传入一个自定义的窗口触发器（Trigger）。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br><span class="line">       .trigger(<span class="keyword">new</span> <span class="type">MyTrigger</span>())</span><br></pre></td></tr></table></figure><h4 id="1-1-6-2-移除器（Evictor）"><a href="#1-1-6-2-移除器（Evictor）" class="headerlink" title="1.1.6.2 移除器（Evictor）"></a>1.1.6.2 移除器（Evictor）</h4><h5 id="移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用-evictor-方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。"><a href="#移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用-evictor-方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。" class="headerlink" title="移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用.evictor()方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。"></a>移除器主要用来定义移除某些数据的逻辑。基于WindowedStream调用.evictor()方法，就可以传入一个自定义的移除器（Evictor）。Evictor是一个接口，不同的窗口类型都有各自预实现的移除器。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">       .window(...)</span><br><span class="line">       .evictor(<span class="keyword">new</span> <span class="type">MyEvictor</span>())</span><br></pre></td></tr></table></figure><h2 id="1-2-时间语义"><a href="#1-2-时间语义" class="headerlink" title="1.2 时间语义"></a><strong>1.2</strong> <strong>时间语义</strong></h2><h3 id="1-2-1-Flink中的时间语义"><a href="#1-2-1-Flink中的时间语义" class="headerlink" title="1**.2.1 Flink中的时间语义**"></a>1**.2.1 Flink中的时间语义**</h3><p><img src="https://pic1.imgdb.cn/item/678728cbd0e0a243d4f45ec2.png"></p><h3 id="1-2-2-哪种时间语义更重要"><a href="#1-2-2-哪种时间语义更重要" class="headerlink" title="1.2.2 哪种时间语义更重要"></a><strong>1.2.2</strong> <strong>哪种时间语义更重要</strong></h3><p><strong>1）从《星球大战》说起</strong></p><h4 id="为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。"><a href="#为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。" class="headerlink" title="为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。"></a>为了更加清晰地说明两种语义的区别，我们来举一个非常经典的例子：电影《星球大战》。</h4><p><img src="https://pic1.imgdb.cn/item/67872901d0e0a243d4f45ed1.png"></p><h5 id="如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。"><a href="#如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。" class="headerlink" title="如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。"></a>如上图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“处理时间”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“事件时间”。两种时间语义都有各自的用途，适用于不同的场景。</h5><h5 id="2）数据处理系统中的时间语义"><a href="#2）数据处理系统中的时间语义" class="headerlink" title="2）数据处理系统中的时间语义"></a><strong>2）数据处理系统中的时间语义</strong></h5><h5 id="在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。"><a href="#在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。" class="headerlink" title="在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。"></a>在实际应用中，事件时间语义会更为常见。一般情况下，业务日志数据中都会记录数据生成的时间戳（timestamp），它就可以作为事件时间的判断基础。</h5><h5 id="在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1-12版本开始，Flink已经将事件时间作为默认的时间语义了。"><a href="#在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1-12版本开始，Flink已经将事件时间作为默认的时间语义了。" class="headerlink" title="在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1.12版本开始，Flink已经将事件时间作为默认的时间语义了。"></a>在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从Flink1.12版本开始，Flink已经将事件时间作为默认的时间语义了。</h5><h2 id="1-3-水位线（Watermark）"><a href="#1-3-水位线（Watermark）" class="headerlink" title="1.3 水位线（Watermark）"></a><strong>1.3</strong> <strong>水位线（</strong>Watermark<strong>）</strong></h2><h3 id="1-3-1-事件时间和窗口"><a href="#1-3-1-事件时间和窗口" class="headerlink" title="1.3.1 事件时间和窗口"></a><strong>1.3.1</strong> <strong>事件时间和窗口</strong></h3><p><img src="https://pic1.imgdb.cn/item/6787295ad0e0a243d4f45f29.png"></p><h3 id="1-3-2-什么是水位线"><a href="#1-3-2-什么是水位线" class="headerlink" title="1.3.2 什么是水位线"></a><strong>1.3.2</strong> <strong>什么是水位线</strong></h3><p><img src="https://pic1.imgdb.cn/item/67872997d0e0a243d4f45f52.png"></p><p><img src="https://pic1.imgdb.cn/item/678729add0e0a243d4f45f58.png"></p><p><img src="https://pic1.imgdb.cn/item/678729ccd0e0a243d4f45f67.png"></p><p><img src="https://pic1.imgdb.cn/item/678729efd0e0a243d4f45f6f.png"></p><p><strong>1.3.3 水位线和窗口的工作原理</strong></p><p><img src="https://pic1.imgdb.cn/item/67872a10d0e0a243d4f45f76.png"></p><p><img src="https://pic1.imgdb.cn/item/67872a41d0e0a243d4f45f7a.png"></p><blockquote><p>[!CAUTION]</p><p><strong>注意：</strong>Flink中窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口。另外，这里我们认为到达窗口结束时间时，窗口就触发计算并关闭，事实上“触发计算”和“窗口关闭”两个行为也可以分开，这部分内容我们会在后面详述。</p></blockquote><h3 id="1-3-4-生成水位线"><a href="#1-3-4-生成水位线" class="headerlink" title="1.3.4 生成水位线"></a><strong>1.3.4</strong> <strong>生成水位线</strong></h3><h4 id="1-3-4-1-生成水位线的总体原则"><a href="#1-3-4-1-生成水位线的总体原则" class="headerlink" title="1.3.4.1 生成水位线的总体原则"></a>1.3.4.1 生成水位线的总体原则</h4><h5 id="完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。"><a href="#完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。" class="headerlink" title="完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。"></a>完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。不过如果要保证绝对正确，就必须等足够长的时间，这会带来更高的延迟。</h5><h5 id="如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。"><a href="#如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。" class="headerlink" title="如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。"></a>如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。</h5><h5 id="所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。"><a href="#所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。" class="headerlink" title="所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。"></a>所以Flink中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。</h5><h4 id="1-3-4-2-水位线生成策略"><a href="#1-3-4-2-水位线生成策略" class="headerlink" title="1.3.4.2 水位线生成策略"></a>1.3.4.2 水位线生成策略</h4><h5 id="在Flink的DataStream-API中，有一个单独用于生成水位线的方法：-assignTimestampsAndWatermarks-，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下："><a href="#在Flink的DataStream-API中，有一个单独用于生成水位线的方法：-assignTimestampsAndWatermarks-，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下：" class="headerlink" title="在Flink的DataStream API中，有一个单独用于生成水位线的方法：.assignTimestampsAndWatermarks()，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下："></a>在Flink的DataStream API中，有一个单独用于生成水位线的方法：.assignTimestampsAndWatermarks()，它主要用来为流中的数据分配时间戳，并生成水位线来指示事件时间。具体使用如下：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> <span class="title class_">ClickSource</span>());</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; withTimestampsAndWatermarks = </span><br><span class="line">stream.assignTimestampsAndWatermarks(&lt;watermark strategy&gt;);</span><br></pre></td></tr></table></figure><h5 id="说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。"><a href="#说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。" class="headerlink" title="说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。"></a>说明：WatermarkStrategy作为参数，这就是所谓的“水位线生成策略”。WatermarkStrategy是一个接口，该接口中包含了一个“时间戳分配器”TimestampAssigner和一个“水位线生成器”WatermarkGenerator。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.有序流的水位线生成策略 forMonotonousTimestamps</span></span><br><span class="line">stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forMonotonousTimestamps[<span class="type">Event</span>]()</span><br><span class="line">  .withTimestampAssigner(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="comment">// 提取时间戳的方法</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">    &#125;</span><br><span class="line">  ))</span><br></pre></td></tr></table></figure><h4 id="1-3-4-3-Flink内置水位线"><a href="#1-3-4-3-Flink内置水位线" class="headerlink" title="1.3.4.3 Flink内置水位线"></a>1.3.4.3 Flink内置水位线</h4><p><strong>1）有序流中内置水位线设置</strong></p><h5 id="对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy-forMonotonousTimestamps-方法就可以实现。"><a href="#对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy-forMonotonousTimestamps-方法就可以实现。" class="headerlink" title="对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy.forMonotonousTimestamps()方法就可以实现。"></a>对于有序流，主要特点就是时间戳单调增长，所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用WatermarkStrategy.forMonotonousTimestamps()方法就可以实现。</h5><p><strong>2）乱序流中内置水位线设置</strong></p><h5 id="由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy-forBoundedOutOfOrderness-方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。"><a href="#由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy-forBoundedOutOfOrderness-方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。" class="headerlink" title="由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy. forBoundedOutOfOrderness()方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。"></a>由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用WatermarkStrategy. forBoundedOutOfOrderness()方法就可以实现。这个方法需要传入一个maxOutOfOrderness参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。</h5><h4 id="3-4-4-自定义水位线生成器"><a href="#3-4-4-自定义水位线生成器" class="headerlink" title=".3.4.4 自定义水位线生成器"></a>.3.4.4 自定义水位线生成器</h4><p><strong>1）周期性水位线生成器（Periodic Generator）</strong></p><h5 id="周期性生成器一般是通过onEvent-观察判断输入的事件，而在onPeriodicEmit-里发出水位线。"><a href="#周期性生成器一般是通过onEvent-观察判断输入的事件，而在onPeriodicEmit-里发出水位线。" class="headerlink" title="周期性生成器一般是通过onEvent()观察判断输入的事件，而在onPeriodicEmit()里发出水位线。"></a>周期性生成器一般是通过onEvent()观察判断输入的事件，而在onPeriodicEmit()里发出水位线。</h5><h5 id="下面是一段自定义、有序流、无序流周期性生成水位线的代码："><a href="#下面是一段自定义、有序流、无序流周期性生成水位线的代码：" class="headerlink" title="下面是一段自定义、有序流、无序流周期性生成水位线的代码："></a>下面是一段自定义、有序流、无序流周期性生成水位线的代码：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.&#123;<span class="type">SerializableTimestampAssigner</span>, <span class="type">TimestampAssigner</span>, <span class="type">TimestampAssignerSupplier</span>, <span class="type">Watermark</span>, <span class="type">WatermarkGenerator</span>, <span class="type">WatermarkGeneratorSupplier</span>, <span class="type">WatermarkOutput</span>, <span class="type">WatermarkStrategy</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 自动生成水位线的周期时间间隔</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">500</span>L)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.有序流的水位线生成策略 forMonotonousTimestamps</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forMonotonousTimestamps[<span class="type">Event</span>]()</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 提取时间戳的方法</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.乱序流的水位线生成策略  forBoundedOutOfOrderness</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness[<span class="type">Event</span>](<span class="type">Duration</span>.ofSeconds(<span class="number">2</span>))</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 水位线的设置有周期性和事件性，周期性就是隔一段时间检查一次，事件性就是来一个事件就检测一次</span></span><br><span class="line">    <span class="comment">// 自定义水位线 双new</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">WatermarkStrategy</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createTimestampAssigner</span></span>(context: <span class="type">TimestampAssignerSupplier</span>.<span class="type">Context</span>): <span class="type">TimestampAssigner</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 如果event中没有时间戳字段那就是，得替换掉后面的逻辑，换成自己的逻辑比如：System.currentTimeMillis() 使用当前系统时间</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createWatermarkGenerator</span></span>(context: <span class="type">WatermarkGeneratorSupplier</span>.<span class="type">Context</span>): <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 定义一个延迟时间</span></span><br><span class="line">          <span class="keyword">val</span> delay = <span class="number">5000</span>L</span><br><span class="line">          <span class="comment">// 定义属性保存最大时间戳 就算是0的时间来了也能正常处理</span></span><br><span class="line">          <span class="keyword">var</span> maxTs = <span class="type">Long</span>.<span class="type">MinValue</span> + delay + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">// 每来一条数据进行调用，有了这个下面那个周期性的就没用了</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEvent</span></span>(event: <span class="type">Event</span>, eventTimestamp: <span class="type">Long</span>, output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="comment">// 更新最大时间戳</span></span><br><span class="line">            maxTs = math.max(maxTs, event.timestamp)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 按照周期性进行调用</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onPeriodicEmit</span></span>(output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> watermark = <span class="keyword">new</span> <span class="type">Watermark</span>(maxTs - delay - <span class="number">1</span>)</span><br><span class="line">            output.emitWatermark(watermark)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="我们在onPeriodicEmit-里调用output-emitWatermark-，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。"><a href="#我们在onPeriodicEmit-里调用output-emitWatermark-，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。" class="headerlink" title="我们在onPeriodicEmit()里调用output.emitWatermark()，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。"></a>我们在onPeriodicEmit()里调用output.emitWatermark()，就可以发出水位线了；这个方法由系统框架周期性地调用，默认200ms一次。</h5><h5 id="如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms"><a href="#如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms" class="headerlink" title="如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms"></a>如果想修改默认周期时间，可以通过下面方法修改。例如：修改为400ms</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.getConfig().setAutoWatermarkInterval(<span class="number">400</span>L);</span><br></pre></td></tr></table></figure><p><strong>2）断点式水位线生成器（</strong>Punctuated Generator<strong>）</strong></p><h5 id="断点式生成器会不停地检测onEvent-中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。"><a href="#断点式生成器会不停地检测onEvent-中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。" class="headerlink" title="断点式生成器会不停地检测onEvent()中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。"></a>断点式生成器会不停地检测onEvent()中的事件，当发现带有水位线信息的事件时，就立即发出水位线。我们把发射水位线的逻辑写在onEvent方法当中即可。</h5><p><strong>3）在数据源中发送水位线</strong></p><h5 id="我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下："><a href="#我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下：" class="headerlink" title="我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下："></a>我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用assignTimestampsAndWatermarks方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用assignTimestampsAndWatermarks方法生成水位线二者只能取其一。示例程序如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.fromSource(</span><br><span class="line">kafkaSource, <span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">3</span>)), <span class="string">&quot;kafkasource&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="1-3-5-水位线的传递"><a href="#1-3-5-水位线的传递" class="headerlink" title="1.3.5 水位线的传递"></a><strong>1.3.5</strong> <strong>水位线的传递</strong></h3><p><img src="https://pic1.imgdb.cn/item/67872c85d0e0a243d4f4601f.png"></p><h5 id="在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。"><a href="#在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。" class="headerlink" title="在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。"></a>在流处理中，上游任务处理完水位线、时钟改变之后，要把当前的水位线再次发出，广播给所有的下游子任务。而当一个任务接收到多个上游并行任务传递来的水位线时，应该以最小的那个作为当前任务的事件时钟。</h5><h5 id="水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。"><a href="#水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。" class="headerlink" title="水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。"></a>水位线在上下游任务之间的传递，非常巧妙地避免了分布式系统中没有统一时钟的问题，每个任务都以“处理完之前所有数据”为标准来确定自己的时钟。</h5><h5 id="案例：1-3-4-3-中乱序流的watermark，将并行度设为2，观察现象。"><a href="#案例：1-3-4-3-中乱序流的watermark，将并行度设为2，观察现象。" class="headerlink" title="案例：1.3.4.3 中乱序流的watermark，将并行度设为2，观察现象。"></a>案例：1.3.4.3 中乱序流的watermark，将并行度设为2，观察现象。</h5><h5 id="在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。"><a href="#在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。" class="headerlink" title="在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。"></a>在多个上游并行任务中，如果有其中一个没有数据，由于当前Task是以最小的那个作为当前任务的事件时钟，就会导致当前Task的水位线无法推进，就可能导致窗口无法触发。这时候可以设置空闲等待。</h5><h5 id="用1-3-4-6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下："><a href="#用1-3-4-6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下：" class="headerlink" title="用1.3.4.6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下："></a>用1.3.4.6中的自定义分区器，只输入奇数来模拟部分subtask无数据，代码如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.function.<span class="type">ProcessWindowFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingProcessingTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.<span class="type">TimeWindow</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WatermarkTest1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 自动生成水位线的周期时间间隔</span></span><br><span class="line">    env.getConfig.setAutoWatermarkInterval(<span class="number">500</span>L)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.socketTextStream(<span class="string">&quot;master&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.有序流的水位线生成策略 forMonotonousTimestamps</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forMonotonousTimestamps[<span class="type">Event</span>]()</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 提取时间戳的方法</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.乱序流的水位线生成策略  forBoundedOutOfOrderness</span></span><br><span class="line">    <span class="comment">// 这个5指的是最大延迟时间</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness[<span class="type">Event</span>](<span class="type">Duration</span>.ofSeconds(<span class="number">5</span>))</span><br><span class="line">      .withTimestampAssigner(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      ))</span><br><span class="line">      .keyBy(_.user)</span><br><span class="line">      .window(<span class="type">SlidingProcessingTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">      .process( <span class="keyword">new</span> <span class="type">WatermarkWindowResult</span> )</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 水位线的设置有周期性和事件性，周期性就是隔一段时间检查一次，事件性就是来一个事件就检测一次</span></span><br><span class="line">    <span class="comment">// 自定义水位线 双new</span></span><br><span class="line">    stream.assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="type">WatermarkStrategy</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createTimestampAssigner</span></span>(context: <span class="type">TimestampAssignerSupplier</span>.<span class="type">Context</span>): <span class="type">TimestampAssigner</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 如果event中没有时间戳字段那就是，得替换掉后面的逻辑，换成自己的逻辑比如：System.currentTimeMillis() 使用当前系统时间</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: <span class="type">Event</span>, recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element.timestamp</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createWatermarkGenerator</span></span>(context: <span class="type">WatermarkGeneratorSupplier</span>.<span class="type">Context</span>): <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">WatermarkGenerator</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">          <span class="comment">// 定义一个延迟时间</span></span><br><span class="line">          <span class="keyword">val</span> delay = <span class="number">5000</span>L</span><br><span class="line">          <span class="comment">// 定义属性保存最大时间戳 就算是0的时间来了也能正常处理</span></span><br><span class="line">          <span class="keyword">var</span> maxTs = <span class="type">Long</span>.<span class="type">MinValue</span> + delay + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">          <span class="comment">// 每来一条数据进行调用，有了这个下面那个周期性的就没用了</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onEvent</span></span>(event: <span class="type">Event</span>, eventTimestamp: <span class="type">Long</span>, output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="comment">// 更新最大时间戳</span></span><br><span class="line">            maxTs = math.max(maxTs, event.timestamp)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 按照周期性进行调用</span></span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onPeriodicEmit</span></span>(output: <span class="type">WatermarkOutput</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> watermark = <span class="keyword">new</span> <span class="type">Watermark</span>(maxTs - delay - <span class="number">1</span>)</span><br><span class="line">            output.emitWatermark(watermark)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义全窗口函数</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">WatermarkWindowResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>[<span class="type">Event</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">TimeWindow</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(user: <span class="type">String</span>, context: <span class="type">Context</span>, elements: <span class="type">Iterable</span>[<span class="type">Event</span>], out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 提取信息</span></span><br><span class="line">      <span class="keyword">val</span> start = context.window.getStart</span><br><span class="line">      <span class="keyword">val</span> end = context.window.getEnd</span><br><span class="line">      <span class="keyword">val</span> count = elements.size</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 增加水位线信息</span></span><br><span class="line">      <span class="keyword">val</span> currentWatermark = context.currentWatermark</span><br><span class="line"></span><br><span class="line">      out.collect(<span class="string">s&quot;窗口 <span class="subst">$start</span>~<span class="subst">$end</span>, 用户 <span class="subst">$user</span> 的活跃度为：<span class="subst">$count</span>, 水位线现在位于: <span class="subst">$currentWatermark</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>1.3.6 迟到数据的处理</strong></p><h4 id="1-3-6-1-推迟水印推进"><a href="#1-3-6-1-推迟水印推进" class="headerlink" title="1.3.6.1 推迟水印推进"></a>1.3.6.1 推迟水印推进</h4><h5 id="在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。"><a href="#在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。" class="headerlink" title="在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。"></a>在水印产生时，设置一个乱序容忍度，推迟系统时间的推进，保证窗口计算被延迟执行，为乱序的数据争取更多的时间进入窗口。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness(<span class="type">Duration</span>.ofSeconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><h5 id="1-3-6-2-设置窗口延迟关闭"><a href="#1-3-6-2-设置窗口延迟关闭" class="headerlink" title="1.3.6.2 设置窗口延迟关闭"></a>1.3.6.2 设置窗口延迟关闭</h5><h5 id="Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。"><a href="#Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。" class="headerlink" title="Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。"></a>Flink的窗口，也允许迟到数据。当触发了窗口计算后，会先计算当前的结果，但是此时并不会关闭窗口。</h5><h6 id="以后每来一条迟到数据，就触发一次这条数据所在窗口计算-增量计算-。直到wartermark-超过了窗口结束时间-推迟时间，此时窗口会真正关闭。"><a href="#以后每来一条迟到数据，就触发一次这条数据所在窗口计算-增量计算-。直到wartermark-超过了窗口结束时间-推迟时间，此时窗口会真正关闭。" class="headerlink" title="以后每来一条迟到数据，就触发一次这条数据所在窗口计算(增量计算)。直到wartermark 超过了窗口结束时间+推迟时间，此时窗口会真正关闭。"></a>以后每来一条迟到数据，就触发一次这条数据所在窗口计算(增量计算)。直到wartermark 超过了窗口结束时间+推迟时间，此时窗口会真正关闭。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.window(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">.allowedLateness(<span class="type">Time</span>.seconds(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h5 id="注意"><a href="#注意" class="headerlink" title="注意:"></a>注意:</h5><h5 id="允许迟到只能运用在event-time上"><a href="#允许迟到只能运用在event-time上" class="headerlink" title="允许迟到只能运用在event time上"></a>允许迟到只能运用在event time上</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">.windowAll(<span class="type">TumblingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">.allowedLateness(<span class="type">Time</span>.seconds(<span class="number">3</span>))</span><br><span class="line">.sideOutputLateData(lateWS)</span><br></pre></td></tr></table></figure><h5 id="完整案例代码如下："><a href="#完整案例代码如下：" class="headerlink" title="完整案例代码如下："></a>完整案例代码如下：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day03</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">ClickSource</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> com.day03.<span class="type">UrlViewCountExample</span>.&#123;<span class="type">UrlViewCountAgg</span>, <span class="type">UrlViewCountResult</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime.<span class="type">WatermarkStrategy</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.<span class="type">SlidingEventTimeWindows</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.<span class="type">Duration</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ProcessLateDataExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream = env.socketTextStream(<span class="string">&quot;master&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong)</span><br><span class="line">      &#125;)</span><br><span class="line">      .assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forBoundedOutOfOrderness[<span class="type">Event</span>](<span class="type">Duration</span>.ofSeconds(<span class="number">5</span>)))</span><br><span class="line">      .assignAscendingTimestamps(_.timestamp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个侧输出流的标签,泛型就是你要放什么数据你就要改成什么</span></span><br><span class="line">    <span class="keyword">val</span> outputTag = <span class="type">OutputTag</span>[<span class="type">Event</span>](<span class="string">&quot;late-data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = stream.keyBy(_.url)</span><br><span class="line">      .window(<span class="type">SlidingEventTimeWindows</span>.of(<span class="type">Time</span>.seconds(<span class="number">10</span>), <span class="type">Time</span>.seconds(<span class="number">5</span>)))</span><br><span class="line">      <span class="comment">// 指定窗口允许等待的时间</span></span><br><span class="line">      .allowedLateness(<span class="type">Time</span>.minutes(<span class="number">1</span>))</span><br><span class="line">      <span class="comment">// 将迟到数据输出到侧输出流</span></span><br><span class="line">      .sideOutputLateData(outputTag)</span><br><span class="line">      .aggregate(<span class="keyword">new</span> <span class="type">UrlViewCountAgg</span>, <span class="keyword">new</span> <span class="type">UrlViewCountResult</span>)</span><br><span class="line">    result.print(<span class="string">&quot;result&quot;</span>)</span><br><span class="line"></span><br><span class="line">    stream.print(<span class="string">&quot;input&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将侧输出流的数据进行打印输出</span></span><br><span class="line">    result.getSideOutput(outputTag).print(<span class="string">&quot;late data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink集群部署搭建</title>
      <link href="/posts/pd12.html"/>
      <url>/posts/pd12.html</url>
      
        <content type="html"><![CDATA[<h1 id="Flink1-14-0集群部署"><a href="#Flink1-14-0集群部署" class="headerlink" title="Flink1.14.0集群部署"></a><strong>Flink1.14.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line"><span class="comment">#     将jobmanager.rpc.address的主机名改成master就好了</span></span><br><span class="line"><span class="comment"># 分发到其他主机，然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-17-0集群部署"><a href="#Flink1-17-0集群部署" class="headerlink" title="Flink1.17.0集群部署"></a><strong>Flink1.17.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line">    <span class="comment"># JobManager节点地址.</span></span><br><span class="line">    jobmanager.rpc.address: master</span><br><span class="line">    jobmanager.bind-host: 0.0.0.0</span><br><span class="line">    rest.address: master</span><br><span class="line">    rest.bind-address: 0.0.0.0</span><br><span class="line">    <span class="comment"># TaskManager节点地址.需要配置为当前机器名</span></span><br><span class="line">    taskmanager.bind-host: 0.0.0.0</span><br><span class="line">    taskmanager.host: master</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发到其他主机</span></span><br><span class="line"><span class="comment"># 修改slave1的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave1</span><br><span class="line"><span class="comment"># 修改slave2的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave2</span><br><span class="line"><span class="comment"># 然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-14-0高可用部署"><a href="#Flink1-14-0高可用部署" class="headerlink" title="Flink1.14.0高可用部署"></a><strong>Flink1.14.0高可用部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前面正常部署，后面在high中新增</span></span><br><span class="line"><span class="comment">#开启HA，使用文件系统作为快照存储</span></span><br><span class="line">state.backend: filesystem （选填）</span><br><span class="line"> </span><br><span class="line"><span class="comment">#启用检查点，可以将快照保存到HDFS （选填）</span></span><br><span class="line">state.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints</span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用zookeeper搭建高可用 （必填）</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储JobManager的元数据到HDFS （必填）</span></span><br><span class="line">high-availability.storageDir: hdfs://node1:8020/flink/ha/</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 配置ZK集群地址（必填）</span></span><br><span class="line">high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定flink在zokkeper的位置（选填）</span></span><br><span class="line"> high-availability.zookeeper.path.root: /flink</span><br><span class="line"> </span><br><span class="line">  然后将flink-shaded-hadoop-2-uber-2.8.3-10.0.jar复制到/root/software/flink/lib中</span><br><span class="line"><span class="comment">#  然后scp最后就可以启动了</span></span><br></pre></td></tr></table></figure><h1 id="Flink-On-Yarn"><a href="#Flink-On-Yarn" class="headerlink" title="Flink On Yarn"></a>Flink On Yarn</h1><h3 id="1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode"><a href="#1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode" class="headerlink" title="1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode"></a><strong>1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">配置环境变量，增加环境变量配置如下：</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure><h3 id="2-会话模式部署"><a href="#2-会话模式部署" class="headerlink" title="2 会话模式部署"></a>2 会话模式部署</h3><h3 id="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下："><a href="#YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下：" class="headerlink" title="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下："></a>YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下：</h3><h3 id="（1）启动集群"><a href="#（1）启动集群" class="headerlink" title="（1）启动集群"></a>（1）启动集群</h3><ul><li><h3 id="（1）启动Hadoop集群（HDFS、YARN）。"><a href="#（1）启动Hadoop集群（HDFS、YARN）。" class="headerlink" title="（1）启动Hadoop集群（HDFS、YARN）。"></a>（1）启动Hadoop集群（HDFS、YARN）。</h3></li><li><h3 id="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"><a href="#（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。" class="headerlink" title="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"></a>（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。</h3></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh -nm <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">-d：分离模式，如果你不想让Flink YARN客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，YARN session也可以后台运行。</span><br><span class="line">-jm（–jobManagerMemory）：配置JobManager所需内存，默认单位MB。</span><br><span class="line">-nm（–name）：配置在YARN UI界面上显示的任务名。</span><br><span class="line">-qu（–queue）：指定YARN队列名。</span><br><span class="line">-tm（–taskManager）：配置每个TaskManager所使用内存。</span><br></pre></td></tr></table></figure><h5 id="YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。"><a href="#YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。" class="headerlink" title="YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。"></a>YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2022-11-17 15:20:52,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface hadoop104:40825 of application <span class="string">&#x27;application_1668668287070_0005&#x27;</span>.</span><br><span class="line">JobManager Web Interface: http://hadoop104:40825</span><br></pre></td></tr></table></figure><h3 id="（2）通过命令行提交作业"><a href="#（2）通过命令行提交作业" class="headerlink" title="（2）通过命令行提交作业"></a><strong>（2）通过命令行提交作业</strong></h3><ul><li>① 将FlinkTutorial-1.0-SNAPSHOT.jar任务上传至集群。</li><li>② 执行以下命令将该任务提交到已经开启的Yarn-Session中运行。</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h3 id="3-单作业模式部署"><a href="#3-单作业模式部署" class="headerlink" title="3 单作业模式部署"></a><strong>3 单作业模式部署</strong></h3><h6 id="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"><a href="#在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群" class="headerlink" title="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"></a>在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群</h6><h6 id="（1）执行命令提交作业"><a href="#（1）执行命令提交作业" class="headerlink" title="（1）执行命令提交作业"></a>（1）执行命令提交作业</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -d -t yarn-per-job -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h6 id="注意：如果启动过程中报如下异常"><a href="#注意：如果启动过程中报如下异常" class="headerlink" title="注意：如果启动过程中报如下异常"></a>注意：如果启动过程中报如下异常</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread “Thread-5” java.lang.IllegalStateException: Trying to access closed classloader. Please check <span class="keyword">if</span> you store classloaders directly or indirectly <span class="keyword">in</span> static fields. If the stacktrace suggests that the leak occurs <span class="keyword">in</span> a third party library and cannot be fixed immediately, you can <span class="built_in">disable</span> this check with the configuration ‘classloader.check-leaked-classloader’.</span><br><span class="line">at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders</span><br></pre></td></tr></table></figure><h6 id="解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置"><a href="#解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置" class="headerlink" title="解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置"></a>解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim flink-conf.yaml</span><br><span class="line"></span><br><span class="line">classloader.check-leaked-classloader: <span class="literal">false</span></span><br></pre></td></tr></table></figure><h6 id="（2）可以使用命令行查看或取消作业，命令如下。"><a href="#（2）可以使用命令行查看或取消作业，命令如下。" class="headerlink" title="（2）可以使用命令行查看或取消作业，命令如下。"></a>（2）可以使用命令行查看或取消作业，命令如下。</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h3 id="4-应用模式部署"><a href="#4-应用模式部署" class="headerlink" title="4 应用模式部署"></a><strong>4 应用模式部署</strong></h3><h4 id="1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。"><a href="#1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。" class="headerlink" title="(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。"></a>(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar </span><br></pre></td></tr></table></figure><h5 id="2-在命令行中查看或取消作业。"><a href="#2-在命令行中查看或取消作业。" class="headerlink" title="(2)在命令行中查看或取消作业。"></a>(2)在命令行中查看或取消作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h4 id="2）上传HDFS提交"><a href="#2）上传HDFS提交" class="headerlink" title="2）上传HDFS提交"></a><strong>2）上传HDFS提交</strong></h4><h5 id="可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。"><a href="#可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。" class="headerlink" title="可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。"></a>可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。</h5><h6 id="（1）上传flink的lib和plugins到HDFS上"><a href="#（1）上传flink的lib和plugins到HDFS上" class="headerlink" title="（1）上传flink的lib和plugins到HDFS上"></a>（1）上传flink的lib和plugins到HDFS上</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -<span class="built_in">mkdir</span> /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put lib/ /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put plugins/ /flink-dist</span><br></pre></td></tr></table></figure><h5 id="（2）提交作业"><a href="#（2）提交作业" class="headerlink" title="（2）提交作业"></a>（2）提交作业</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application    -Dyarn.provided.lib.dirs=<span class="string">&quot;hdfs://hadoop102:8020/flink-dist&quot;</span>    -c com.atguigu.wc.SocketStreamWordCount  hdfs://hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的DataStreamAPI</title>
      <link href="/posts/pd14.html"/>
      <url>/posts/pd14.html</url>
      
        <content type="html"><![CDATA[<h5 id="DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："><a href="#DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：" class="headerlink" title="DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："></a>DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：</h5><p><img src="https://pic1.imgdb.cn/item/67861fbed0e0a243d4f42964.png"></p><h5 id="在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法："><a href="#在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法：" class="headerlink" title="在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法："></a>在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.addSource(...);</span><br></pre></td></tr></table></figure><h5 id="方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。"><a href="#方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。" class="headerlink" title="方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。"></a>方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。</h5><h5 id="从Flink1-12开始，主要使用流批统一的新Source架构："><a href="#从Flink1-12开始，主要使用流批统一的新Source架构：" class="headerlink" title="从Flink1.12开始，主要使用流批统一的新Source架构："></a>从Flink1.12开始，主要使用流批统一的新Source架构：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStreamSource</span>&lt;<span class="type">String</span>&gt; stream = env.fromSource(…)</span><br></pre></td></tr></table></figure><h5 id="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"><a href="#Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。" class="headerlink" title="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"></a>Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。</h5><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作:"></a>准备工作:</h2><h5 id="为了方便练习，这里使用WaterSensor作为数据模型。"><a href="#为了方便练习，这里使用WaterSensor作为数据模型。" class="headerlink" title="为了方便练习，这里使用WaterSensor作为数据模型。"></a>为了方便练习，这里使用WaterSensor作为数据模型。</h5><table><thead><tr><th><strong>id</strong></th><th><strong>String</strong></th><th><strong>水位传感器类型</strong></th></tr></thead><tbody><tr><td><strong>ts</strong></td><td><strong>Long</strong></td><td><strong>传感器记录时间戳</strong></td></tr><tr><td><strong>vc</strong></td><td><strong>Integer</strong></td><td><strong>水位记录</strong></td></tr></tbody></table><h4 id="具体代码如下："><a href="#具体代码如下：" class="headerlink" title="具体代码如下："></a>具体代码如下：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensor</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String id;</span><br><span class="line">    <span class="keyword">public</span> Long ts;</span><br><span class="line">    <span class="keyword">public</span> Integer vc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">(String id, Long ts, Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getId</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setId</span><span class="params">(String id)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getTs</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setTs</span><span class="params">(Long ts)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">getVc</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setVc</span><span class="params">(Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;WaterSensor&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;id=&#x27;&quot;</span> + id + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, ts=&quot;</span> + ts +</span><br><span class="line">                <span class="string">&quot;, vc=&quot;</span> + vc +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span> == o) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="literal">null</span> || getClass() != o.getClass()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">WaterSensor</span> <span class="variable">that</span> <span class="operator">=</span> (WaterSensor) o;</span><br><span class="line">        <span class="keyword">return</span> Objects.equals(id, that.id) &amp;&amp;</span><br><span class="line">                Objects.equals(ts, that.ts) &amp;&amp;</span><br><span class="line">                Objects.equals(vc, that.vc);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Objects.hash(id, ts, vc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="这里需要注意，我们定义的WaterSensor，有这样几个特点："><a href="#这里需要注意，我们定义的WaterSensor，有这样几个特点：" class="headerlink" title="这里需要注意，我们定义的WaterSensor，有这样几个特点："></a>这里需要注意，我们定义的WaterSensor，有这样几个特点：</h4><ul><li>类是公有（public）的</li><li>有一个无参的构造方法</li><li>所有属性都是公有（public）的</li><li>所有属性的类型都是可以序列化的</li></ul><h6 id="Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"><a href="#Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。" class="headerlink" title="Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"></a>Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。</h6><h6 id="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"><a href="#我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。" class="headerlink" title="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"></a>我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。</h6><h3 id="1-从集合、文件、元素中读取数据"><a href="#1-从集合、文件、元素中读取数据" class="headerlink" title="1.从集合、文件、元素中读取数据"></a>1.从集合、文件、元素中读取数据</h3><h5 id="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"><a href="#最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。" class="headerlink" title="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"></a>最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceBoundedTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">user: <span class="type">String</span>, url: <span class="type">String</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从元素中读取数据</span></span><br><span class="line"><span class="comment">//    val stream: DataStream[Int] = env.fromElements(1, 2, 3, 4, 5, 6)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从集合中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> clicks = <span class="type">List</span>(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> stream2: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromCollection(clicks)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从文件中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> stream3: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\click.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印输出</span></span><br><span class="line">    stream.print(<span class="string">&quot;从元素中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream2.print(<span class="string">&quot;从集合中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream3.print(<span class="string">&quot;从文件中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2从Socket读取数据"><a href="#1-2从Socket读取数据" class="headerlink" title="1.2从Socket读取数据"></a>1.2从Socket读取数据</h3><ul><li><h6 id="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"><a href="#不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。" class="headerlink" title="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"></a>不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。</h6></li><li><h6 id="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"><a href="#我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。" class="headerlink" title="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"></a>我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。</h6></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>);</span><br></pre></td></tr></table></figure><h3 id="1-3从Kafka中读取数据"><a href="#1-3从Kafka中读取数据" class="headerlink" title="1.3从Kafka中读取数据"></a>1.3从Kafka中读取数据</h3><ul><li><h6 id="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"><a href="#Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。" class="headerlink" title="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"></a>Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。</h6></li><li><h6 id="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。"><a href="#所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。" class="headerlink" title="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。"></a>所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。</h6></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下："><a href="#代码如下：" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用Properties保存kafka连接的配置</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"></span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-从数据生成器读取数据"><a href="#1-4-从数据生成器读取数据" class="headerlink" title="1.4 从数据生成器读取数据"></a>1.4 从数据生成器读取数据</h3><h6 id="Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖："><a href="#Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖：" class="headerlink" title="Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖："></a>Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖：</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-datagen<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下：-1"><a href="#代码如下：-1" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DataGeneratorDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataGeneratorSource&lt;String&gt; dataGeneratorSource =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">DataGeneratorSource</span>&lt;&gt;(</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="keyword">public</span> String <span class="title function_">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                                <span class="keyword">return</span> <span class="string">&quot;Number:&quot;</span>+value;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        Long.MAX_VALUE,</span><br><span class="line">                        RateLimiterStrategy.perSecond(<span class="number">10</span>),</span><br><span class="line">                        Types.STRING</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class="string">&quot;datagenerator&quot;</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-5Flink支持的数据类型"><a href="#1-5Flink支持的数据类型" class="headerlink" title="1.5Flink支持的数据类型"></a>1.5Flink支持的数据类型</h3><ol><li><h5 id="Flink的类型系统"><a href="#Flink的类型系统" class="headerlink" title="Flink的类型系统"></a>Flink的类型系统</h5><p>Flink使用“类型信息”（TypeInformation）来统一表示数据类型。TypeInformation类是Flink中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。</p></li><li><h5 id="Flink支持的数据类型"><a href="#Flink支持的数据类型" class="headerlink" title="Flink支持的数据类型"></a>Flink支持的数据类型</h5><p>对于常见的Java和Scala数据类型，Flink都是支持的。Flink在内部，Flink对支持不同的类型进行了划分，这些类型可以在Types工具类中找到：</p></li></ol><ul><li><h5 id="（1）基本类型"><a href="#（1）基本类型" class="headerlink" title="（1）基本类型"></a>（1）基本类型</h5><p>所有Java基本类型及其包装类，再加上Void、String、Date、BigDecimal和BigInteger。</p></li><li><h5 id="（2）数组类型"><a href="#（2）数组类型" class="headerlink" title="（2）数组类型"></a>（2）数组类型</h5><p>包括基本类型数组（PRIMITIVE_ARRAY）和对象数组（OBJECT_ARRAY）。</p></li><li><h5 id="（3）复合数据类型"><a href="#（3）复合数据类型" class="headerlink" title="（3）复合数据类型"></a>（3）复合数据类型</h5><ol><li><p>Java元组类型（TUPLE）：这是Flink内置的元组类型，是Java API的一部分。最多25个字段，也就是从Tuple0~Tuple25，不支持空字段。</p></li><li><h5 id="Scala-样例类及Scala元组：不支持空字段。"><a href="#Scala-样例类及Scala元组：不支持空字段。" class="headerlink" title="Scala 样例类及Scala元组：不支持空字段。"></a>Scala 样例类及Scala元组：不支持空字段。</h5></li><li><h5 id="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"><a href="#行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。" class="headerlink" title="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"></a>行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。</h5></li><li><h5 id="POJO：Flink自定义的类似于Java-bean模式的类。"><a href="#POJO：Flink自定义的类似于Java-bean模式的类。" class="headerlink" title="POJO：Flink自定义的类似于Java bean模式的类。"></a>POJO：Flink自定义的类似于Java bean模式的类。</h5></li></ol></li></ul><h4 id="（4）辅助类型"><a href="#（4）辅助类型" class="headerlink" title="（4）辅助类型"></a>（4）辅助类型</h4><ul><li><h5 id="Option、Either、List、Map等。"><a href="#Option、Either、List、Map等。" class="headerlink" title="Option、Either、List、Map等。"></a>Option、Either、List、Map等。</h5></li></ul><h4 id="（5）泛型类型（GENERIC）"><a href="#（5）泛型类型（GENERIC）" class="headerlink" title="（5）泛型类型（GENERIC）"></a>（5）泛型类型（GENERIC）</h4><ul><li><h6 id="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"><a href="#Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。" class="headerlink" title="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"></a>Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。</h6></li><li><h6 id="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"><a href="#在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。" class="headerlink" title="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"></a>在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。</h6></li><li><h6 id="Flink对POJO类型的要求如下："><a href="#Flink对POJO类型的要求如下：" class="headerlink" title="Flink对POJO类型的要求如下："></a>Flink对POJO类型的要求如下：</h6><ol><li><h6 id="类是公有（public）的"><a href="#类是公有（public）的" class="headerlink" title="类是公有（public）的"></a>类是公有（public）的</h6></li><li><h6 id="有一个无参的构造方法"><a href="#有一个无参的构造方法" class="headerlink" title="有一个无参的构造方法"></a>有一个无参的构造方法</h6></li><li><h6 id="所有属性都是公有（public）的"><a href="#所有属性都是公有（public）的" class="headerlink" title="所有属性都是公有（public）的"></a>所有属性都是公有（public）的</h6></li><li><h6 id="所有属性的类型都是可以序列化的"><a href="#所有属性的类型都是可以序列化的" class="headerlink" title="所有属性的类型都是可以序列化的"></a>所有属性的类型都是可以序列化的</h6></li></ol></li></ul><h4 id="3）类型提示（Type-Hints）"><a href="#3）类型提示（Type-Hints）" class="headerlink" title="3）类型提示（Type Hints）"></a>3）类型提示（Type Hints）</h4><h6 id="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"><a href="#Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。" class="headerlink" title="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"></a>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</h6><h6 id="为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。"><a href="#为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。" class="headerlink" title="为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。"></a>为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。</h6><h6 id="回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"><a href="#回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。" class="headerlink" title="回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"></a>回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.map(word -&gt; <span class="type">Tuple2</span>.of(word, <span class="number">1</span>L))</span><br><span class="line">.returns(<span class="type">Types</span>.<span class="type">TUPLE</span>(<span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>));</span><br></pre></td></tr></table></figure><h6 id="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。"><a href="#Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。" class="headerlink" title="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。"></a>Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">returns(<span class="keyword">new</span> <span class="type">TypeHint</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">SomeType</span>&gt;&gt;()&#123;&#125;)</span><br></pre></td></tr></table></figure><h3 id="转换算子（Transformation）"><a href="#转换算子（Transformation）" class="headerlink" title="转换算子（Transformation）"></a>转换算子（Transformation）</h3><h5 id="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"><a href="#数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。" class="headerlink" title="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"></a>数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。</h5><p><img src="https://pic1.imgdb.cn/item/678624a7d0e0a243d4f42b7c.png"></p><h3 id="1-基本转换算子（map-x2F-filter-x2F-flatMap）"><a href="#1-基本转换算子（map-x2F-filter-x2F-flatMap）" class="headerlink" title="1.基本转换算子（map&#x2F; filter&#x2F; flatMap）"></a>1.基本转换算子（map&#x2F; filter&#x2F; flatMap）</h3><h4 id="1-1-1映射（map）"><a href="#1-1-1映射（map）" class="headerlink" title="1.1.1映射（map）"></a>1.1.1映射（map）</h4><h6 id="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"><a href="#map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。" class="headerlink" title="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"></a>map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。</h6><p><img src="https://pic1.imgdb.cn/item/6786250bd0e0a243d4f42bac.png"></p><h6 id="我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"><a href="#我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。" class="headerlink" title="我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"></a>我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。</h6><h6 id="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"><a href="#下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。" class="headerlink" title="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"></a>下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">MapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提取每次点击事件的用户名</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.map( _.user ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现mapFunction接口</span></span><br><span class="line">    stream.map(<span class="keyword">new</span> <span class="type">UserExtractor</span>).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserExtractor</span> <span class="keyword">extends</span> <span class="title">MapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"><a href="#上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。" class="headerlink" title="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"></a>上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。</h6><h3 id="1-1-2过滤（filter）"><a href="#1-1-2过滤（filter）" class="headerlink" title="1.1.2过滤（filter）"></a>1.1.2过滤（filter）</h3><h5 id="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"><a href="#filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。" class="headerlink" title="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"></a>filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。</h5><p><img src="https://pic1.imgdb.cn/item/6786255dd0e0a243d4f42bd8.png"></p><h6 id="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。"><a href="#进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。" class="headerlink" title="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。"></a>进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。</h6><h6 id="案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。"><a href="#案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。" class="headerlink" title="案例需求：下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。"></a><strong>案例需求：</strong>下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFilterTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过滤出用户为Marry的所有点击事件</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.filter( _.user == <span class="string">&quot;hkj&quot;</span> ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现FilterFunction</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">UserFilter</span> ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserFilter</span> <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.user == <span class="string">&quot;Bob&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-1-3-扁平映射（flatMap）"><a href="#1-1-3-扁平映射（flatMap）" class="headerlink" title="1.1.3 扁平映射（flatMap）"></a>1.1.3 扁平映射（flatMap）</h4><h6 id="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"><a href="#flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。" class="headerlink" title="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"></a>flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。</h6><p><img src="https://pic1.imgdb.cn/item/6786259fd0e0a243d4f42bf4.png"></p><h6 id="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"><a href="#同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。" class="headerlink" title="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"></a>同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。</h6><h6 id="案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。"><a href="#案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。" class="headerlink" title="案例需求：如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。"></a><strong>案例需求：</strong>如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。</h6><h6 id="实现代码如下："><a href="#实现代码如下：" class="headerlink" title="实现代码如下："></a>实现代码如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FlatMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFlatMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试灵活输出形式</span></span><br><span class="line">    stream.flatMap(<span class="keyword">new</span> <span class="type">MyFlatMap</span>).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// lam表达式</span></span><br><span class="line">    <span class="keyword">val</span> stream1 = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line">    stream1.flatMap( value =&gt; value.split(<span class="string">&quot;,&quot;</span>) ).map(value =&gt; (value, <span class="number">1</span>)).print(<span class="string">&quot;stream1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义实现flatMapFunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">FlatMapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: <span class="type">Event</span>, out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 如果当前数据是hkj的点击事件那就直接输出user</span></span><br><span class="line">      <span class="keyword">if</span> (value.user == <span class="string">&quot;hkj&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 如果当前数据是Bob的点击事件，那么就输出user和url</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (value.user == <span class="string">&quot;Bob&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">        out.collect(value.url)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2聚合算子（Aggregation）"><a href="#1-2聚合算子（Aggregation）" class="headerlink" title="1.2聚合算子（Aggregation）"></a>1.2聚合算子（Aggregation）</h3><h5 id="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"><a href="#计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。" class="headerlink" title="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"></a>计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。</h5><h4 id="1-2-1-按键分区（keyBy）"><a href="#1-2-1-按键分区（keyBy）" class="headerlink" title="1.2.1 按键分区（keyBy）"></a>1.2.1 按键分区（keyBy）</h4><ul><li><h6 id="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"><a href="#对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。" class="headerlink" title="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"></a>对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。</h6></li><li><h6 id="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"><a href="#keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。" class="headerlink" title="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"></a>keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。</h6></li><li><h6 id="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"><a href="#基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。" class="headerlink" title="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"></a>基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。</h6></li></ul><p><img src="https://pic1.imgdb.cn/item/67862611d0e0a243d4f42c2f.png"></p><h6 id="在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。"><a href="#在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。" class="headerlink" title="在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。"></a>在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。</h6><h6 id="keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"><a href="#keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。" class="headerlink" title="keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"></a>keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。</h6><h6 id="我们可以以id作为key做一个分区操作，代码实现如下："><a href="#我们可以以id作为key做一个分区操作，代码实现如下：" class="headerlink" title="我们可以以id作为key做一个分区操作，代码实现如下："></a>我们可以以id作为key做一个分区操作，代码实现如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.<span class="type">KeySelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformKeyByTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.keyBy( <span class="keyword">new</span> <span class="type">MyKeySelector</span> )</span><br><span class="line">      .maxBy(<span class="string">&quot;timestamp&quot;</span>)</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//    stream.keyBy( _.user).print(&quot;lam&quot;)</span></span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyKeySelector</span> <span class="keyword">extends</span> <span class="title">KeySelector</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKey</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"><a href="#需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。" class="headerlink" title="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"></a>需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。</h6><h6 id="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"><a href="#KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。" class="headerlink" title="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"></a>KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。</h6><h4 id="1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）"><a href="#1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）" class="headerlink" title="1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）"></a>1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）</h4><h6 id="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："><a href="#有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：" class="headerlink" title="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："></a>有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：</h6><ol><li><h6 id="sum-：在输入流上，对指定的字段做叠加求和的操作。"><a href="#sum-：在输入流上，对指定的字段做叠加求和的操作。" class="headerlink" title="sum()：在输入流上，对指定的字段做叠加求和的操作。"></a>sum()：在输入流上，对指定的字段做叠加求和的操作。</h6></li><li><h6 id="min-：在输入流上，对指定的字段求最小值。"><a href="#min-：在输入流上，对指定的字段求最小值。" class="headerlink" title="min()：在输入流上，对指定的字段求最小值。"></a>min()：在输入流上，对指定的字段求最小值。</h6></li><li><h6 id="max-：在输入流上，对指定的字段求最大值。"><a href="#max-：在输入流上，对指定的字段求最大值。" class="headerlink" title="max()：在输入流上，对指定的字段求最大值。"></a>max()：在输入流上，对指定的字段求最大值。</h6></li><li><h6 id="minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。"><a href="#minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。" class="headerlink" title="minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。"></a>minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。</h6></li><li><h6 id="maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。"><a href="#maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。" class="headerlink" title="maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。"></a>maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。</h6></li></ol><h5 id="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"><a href="#简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。" class="headerlink" title="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"></a>简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。</h5><h6 id="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"><a href="#对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。" class="headerlink" title="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"></a>对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。</h6><h6 id="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"><a href="#如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。" class="headerlink" title="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"></a>如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransAggregation</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_2&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_3&quot;</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        stream.keyBy(e -&gt; e.id).max(<span class="string">&quot;vc&quot;</span>);    <span class="comment">// 指定字段名称</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"><a href="#简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。" class="headerlink" title="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"></a>简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。</h5><h5 id="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"><a href="#一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。" class="headerlink" title="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"></a>一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。</h5><h4 id="1-2-3-归约聚合（reduce）"><a href="#1-2-3-归约聚合（reduce）" class="headerlink" title="1.2.3 归约聚合（reduce）"></a>1.2.3 归约聚合（reduce）</h4><h5 id="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"><a href="#reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。" class="headerlink" title="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"></a>reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。</h5><h5 id="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"><a href="#reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。" class="headerlink" title="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"></a>reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。</h5><h5 id="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："><a href="#调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：" class="headerlink" title="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："></a>调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ReduceFunction</span>&lt;T&gt; <span class="keyword">extends</span> <span class="title class_">Function</span>, Serializable &#123;</span><br><span class="line">    T <span class="title function_">reduce</span><span class="params">(T value1, T value2)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"><a href="#ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。" class="headerlink" title="ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"></a>ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。</h5><h5 id="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"><a href="#我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。" class="headerlink" title="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"></a>我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。</h5><h5 id="为了方便后续使用，定义一个WaterSensorMapFunction："><a href="#为了方便后续使用，定义一个WaterSensorMapFunction：" class="headerlink" title="为了方便后续使用，定义一个WaterSensorMapFunction："></a>为了方便后续使用，定义一个WaterSensorMapFunction：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensorMapFunction</span> <span class="keyword">implements</span> <span class="title class_">MapFunction</span>&lt;String,WaterSensor&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> WaterSensor <span class="title function_">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] datas = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WaterSensor</span>(datas[<span class="number">0</span>],Long.valueOf(datas[<span class="number">1</span>]) ,Integer.valueOf(datas[<span class="number">2</span>]) );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="案例：使用reduce实现max和maxBy的功能。"><a href="#案例：使用reduce实现max和maxBy的功能。" class="headerlink" title="案例：使用reduce实现max和maxBy的功能。"></a>案例：使用reduce实现max和maxBy的功能。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">ReduceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformReduceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduce归约聚合,提取当前最活跃用户</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    data =&gt; true 的作用</span></span><br><span class="line"><span class="comment">//    1.keyBy 的作用：keyBy 是 Flink 中的一个算子，用于将数据流按照指定的键进行分组。</span></span><br><span class="line"><span class="comment">//                  分组后，相同键的数据会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    2.data =&gt; true 的含义：</span></span><br><span class="line"><span class="comment">//              这里的 data =&gt; true 是一个匿名函数，它对每条数据返回固定的 true 值。</span></span><br><span class="line"><span class="comment">//              由于所有数据的键都是 true，因此所有数据都会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    3.为什么需要 data =&gt; true：</span></span><br><span class="line"><span class="comment">//          在统计最活跃用户时，需要将所有用户的活跃度数据集中到一个分区中，才能进行比较和筛选。</span></span><br><span class="line"><span class="comment">//          如果不使用 keyBy(data =&gt; true)，数据会分散在多个分区中，无法直接进行比较。</span></span><br><span class="line"></span><br><span class="line">    stream.map( data =&gt; (data.user, <span class="number">1</span>L) )</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .reduce( <span class="keyword">new</span> <span class="type">MySum</span> )  <span class="comment">// 统计每个用户的活跃度</span></span><br><span class="line">      .keyBy(data =&gt; <span class="literal">true</span>)  <span class="comment">// 将所有数据按照同样的key分到同一个组中</span></span><br><span class="line">      .reduce( (state, data) =&gt; <span class="keyword">if</span> (data._2 &gt;= state._2) data <span class="keyword">else</span> state ) <span class="comment">// 选取当前最活跃的用户</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MySum</span> <span class="keyword">extends</span> <span class="title">ReduceFunction</span>[(<span class="type">String</span>, <span class="type">Long</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(value1: (<span class="type">String</span>, <span class="type">Long</span>), value2: (<span class="type">String</span>, <span class="type">Long</span>)): (<span class="type">String</span>, <span class="type">Long</span>) = (value1._1, value2._2 + value1._2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"><a href="#reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。" class="headerlink" title="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"></a>reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。</h5><h3 id="1-3-3-用户自定义函数（UDF）"><a href="#1-3-3-用户自定义函数（UDF）" class="headerlink" title="1.3.3 用户自定义函数（UDF）"></a>1.3.3 用户自定义函数（UDF）</h3><h5 id="用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"><a href="#用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。" class="headerlink" title="用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"></a>用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。</h5><h5 id="用户自定义函数分为：函数类、匿名函数、富函数类。"><a href="#用户自定义函数分为：函数类、匿名函数、富函数类。" class="headerlink" title="用户自定义函数分为：函数类、匿名函数、富函数类。"></a>用户自定义函数分为：函数类、匿名函数、富函数类。</h5><h4 id="需求：用来从用户的点击数据中筛选包含“sensor-1”的内容："><a href="#需求：用来从用户的点击数据中筛选包含“sensor-1”的内容：" class="headerlink" title="需求：用来从用户的点击数据中筛选包含“sensor_1”的内容："></a><strong>需求：</strong>用来从用户的点击数据中筛选包含“sensor_1”的内容：</h4><h4 id="方式一：实现FilterFunction接口"><a href="#方式一：实现FilterFunction接口" class="headerlink" title="方式一：实现FilterFunction接口"></a><strong>方式一：</strong>实现FilterFunction接口</h4><h4 id="方式二：通过匿名类来实现FilterFunction接口："><a href="#方式二：通过匿名类来实现FilterFunction接口：" class="headerlink" title="方式二：通过匿名类来实现FilterFunction接口："></a><strong>方式二：</strong>通过匿名类来实现FilterFunction接口：</h4><h5 id="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"><a href="#方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。" class="headerlink" title="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"></a><strong>方式二的优化：</strong>为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。</h5><h5 id="方式三：采用匿名函数（Lambda）"><a href="#方式三：采用匿名函数（Lambda）" class="headerlink" title="方式三：采用匿名函数（Lambda）"></a><strong>方式三：</strong>采用匿名函数（Lambda）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformUDFTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试UDF的用法,筛选url中包含某个关键字hkjcpdd的Event事件</span></span><br><span class="line">    <span class="comment">// 1.实现一个自定义的函数类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">MyFilterFunction</span>(<span class="string">&quot;hkjcpdd&quot;</span>) ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 使用匿名类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">FilterFunction</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(<span class="string">&quot;hkjmjj&quot;</span>)</span><br><span class="line">    &#125; ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 使用lambda表达式</span></span><br><span class="line">    stream.filter( _.url.contains(<span class="string">&quot;hkjmjj&quot;</span>) ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义个filterfunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFilterFunction</span>(<span class="params">keyWord: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// contains是指是否包含某些字段</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(keyWord)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-2-富函数类（Rich-Function-Classes）"><a href="#1-3-2-富函数类（Rich-Function-Classes）" class="headerlink" title="1.3.2 富函数类（Rich Function Classes）"></a>1.3.2 富函数类（Rich Function Classes）</h4><h6 id="“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"><a href="#“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。" class="headerlink" title="“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"></a>“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。</h6><h6 id="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"><a href="#与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。" class="headerlink" title="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"></a>与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</h6><h6 id="Rich-Function有生命周期的概念。典型的生命周期方法有："><a href="#Rich-Function有生命周期的概念。典型的生命周期方法有：" class="headerlink" title="Rich Function有生命周期的概念。典型的生命周期方法有："></a>Rich Function有生命周期的概念。典型的生命周期方法有：</h6><ul><li><h6 id="open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。"><a href="#open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。" class="headerlink" title="open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。"></a>open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。</h6></li><li><h6 id="close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"><a href="#close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。" class="headerlink" title="close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"></a>close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。</h6></li></ul><h6 id="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。"><a href="#需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。" class="headerlink" title="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。"></a>需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。</h6><h6 id="来看一个例子说明："><a href="#来看一个例子说明：" class="headerlink" title="来看一个例子说明："></a>来看一个例子说明：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformRichFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义一个RichMapFunction，测试复函数类的功能</span></span><br><span class="line">    stream.map( <span class="keyword">new</span> <span class="type">MyRichMap</span>() ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRichMap</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichMapFunction</span>[<span class="type">Event</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务开始&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">Long</span> = value.timestamp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务结束&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-3-4-物理分区算子（Physical-Partitioning）"><a href="#5-3-4-物理分区算子（Physical-Partitioning）" class="headerlink" title="5.3.4 物理分区算子（Physical Partitioning）"></a>5.3.4 物理分区算子（Physical Partitioning）</h3><h5 id="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"><a href="#常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。" class="headerlink" title="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"></a>常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。</h5><h3 id="1-4-1-随机分区（shuffle）"><a href="#1-4-1-随机分区（shuffle）" class="headerlink" title="1.4.1 随机分区（shuffle）"></a>1.4.1 随机分区（shuffle）</h3><h5 id="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。"><a href="#最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。" class="headerlink" title="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。"></a>最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。</h5><h5 id="随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。"><a href="#随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。" class="headerlink" title="随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。"></a>随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。</h5><p><img src="https://pic1.imgdb.cn/item/67862896d0e0a243d4f42d1b.png"></p><h5 id="经过随机分区之后，得到的依然是一个DataStream。"><a href="#经过随机分区之后，得到的依然是一个DataStream。" class="headerlink" title="经过随机分区之后，得到的依然是一个DataStream。"></a>经过随机分区之后，得到的依然是一个DataStream。</h5><h5 id="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"><a href="#我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。" class="headerlink" title="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"></a>我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShuffleExample</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"> env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; stream = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>);;</span><br><span class="line"></span><br><span class="line">        stream.shuffle().print()</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-4-2-轮询分区（Round-Robin）"><a href="#1-4-2-轮询分区（Round-Robin）" class="headerlink" title="1.4.2 轮询分区（Round-Robin）"></a>1.4.2 轮询分区（Round-Robin）</h4><h5 id="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"><a href="#轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。" class="headerlink" title="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"></a>轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。</h5><p><img src="https://pic1.imgdb.cn/item/678628c7d0e0a243d4f42d3e.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rebalance()</span><br></pre></td></tr></table></figure><h4 id="1-4-3-重缩放分区（rescale）"><a href="#1-4-3-重缩放分区（rescale）" class="headerlink" title="1.4.3 重缩放分区（rescale）"></a>1.4.3 重缩放分区（rescale）</h4><h5 id="重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"><a href="#重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。" class="headerlink" title="重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"></a>重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。</h5><p><img src="https://pic1.imgdb.cn/item/678628e9d0e0a243d4f42d59.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rescale()</span><br></pre></td></tr></table></figure><h4 id="1-4-4-广播（broadcast）"><a href="#1-4-4-广播（broadcast）" class="headerlink" title="1.4.4 广播（broadcast）"></a>1.4.4 广播（broadcast）</h4><h5 id="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。"><a href="#这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。" class="headerlink" title="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。"></a>这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.broadcast()</span><br></pre></td></tr></table></figure><h4 id="1-4-5-全局分区（global）"><a href="#1-4-5-全局分区（global）" class="headerlink" title="1.4.5 全局分区（global）"></a>1.4.5 全局分区（global）</h4><h5 id="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"><a href="#全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。" class="headerlink" title="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"></a>全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.global()</span><br></pre></td></tr></table></figure><h4 id="1-4-6-自定义分区（Custom）"><a href="#1-4-6-自定义分区（Custom）" class="headerlink" title="1.4.6 自定义分区（Custom）"></a>1.4.6 自定义分区（Custom）</h4><h5 id="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。"><a href="#当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。" class="headerlink" title="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。"></a>当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。</h5><h4 id="1-自定义分区器"><a href="#1-自定义分区器" class="headerlink" title="1)自定义分区器"></a>1)自定义分区器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">ParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Calendar</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//SourceFunction是并行度1的</span></span><br><span class="line"><span class="comment">//ParallelSourceFunction就是并行多的</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClickSource</span> <span class="keyword">extends</span> <span class="title">ParallelSourceFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 标志位</span></span><br><span class="line">  <span class="keyword">var</span> running = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Event</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 随机数生成器</span></span><br><span class="line">    <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">    <span class="comment">// 定义数据随机选择的范围</span></span><br><span class="line">    <span class="keyword">val</span> users = <span class="type">Array</span>(<span class="string">&quot;Marry&quot;</span>, <span class="string">&quot;Hkj&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Cary&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> urls = <span class="type">Array</span>(<span class="string">&quot;./home&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="string">&quot;./fav&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>,<span class="string">&quot;./prod?id=3&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键流程， 用标志位作为循环的判断条件，不停的发出数据</span></span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line">      <span class="keyword">val</span> event = <span class="type">Event</span>(</span><br><span class="line">        users(random.nextInt(users.length)),</span><br><span class="line">        urls(random.nextInt(users.length)),</span><br><span class="line">        <span class="type">Calendar</span>.getInstance.getTimeInMillis</span><br><span class="line">      )</span><br><span class="line"><span class="comment">//      // 为要发送的数据分配时间戳</span></span><br><span class="line"><span class="comment">//      ctx.collectWithTimestamp(event, event.timestamp)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//      // 向下游直接发送水位线</span></span><br><span class="line"><span class="comment">//      ctx.emitWatermark(new Watermark(event.timestamp - 1L))</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 调用ctx的方法向下游发送数据</span></span><br><span class="line">      ctx.collect(event)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 每隔一秒发送过一条数据</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2）使用自定义分区器"><a href="#2）使用自定义分区器" class="headerlink" title="2）使用自定义分区器"></a>2）使用自定义分区器</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PartitionReblanceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取自定义的数据流</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">ClickSource</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 轮询重分区之后打印输出</span></span><br><span class="line">    stream.rebalance.print(<span class="string">&quot;shuffle&quot;</span>).setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-5分流"><a href="#1-3-5分流" class="headerlink" title="1.3.5分流"></a>1.3.5分流</h3><h4 id="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"><a href="#所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。" class="headerlink" title="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"></a>所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。</h4><p><img src="https://pic1.imgdb.cn/item/6786298ad0e0a243d4f42daf.png"></p><h4 id="1-3-5-1-简单实现"><a href="#1-3-5-1-简单实现" class="headerlink" title="1.3.5.1 简单实现"></a>1.3.5.1 简单实现</h4><p>其实根据条件筛选数据的需求，本身非常容易实现：只要针对同一条流多次独立调用.filter()方法进行筛选，就可以得到拆分之后的流了。</p><p><strong>案例需求：</strong>读取一个整数数字流，将数据流划分为奇数流和偶数流。</p><p><strong>代码实现：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByFilter</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">      </span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                                                           .map(Integer::valueOf);</span><br><span class="line">        <span class="comment">//将ds 分为两个流 ，一个是奇数流，一个是偶数流</span></span><br><span class="line">        <span class="comment">//使用filter 过滤两次</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds1 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">0</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds2 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;偶数&quot;</span>);</span><br><span class="line">        ds2.print(<span class="string">&quot;奇数&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"><a href="#这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？" class="headerlink" title="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"></a>这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？</h5><h4 id="1-3-5-2-使用侧输出流"><a href="#1-3-5-2-使用侧输出流" class="headerlink" title="1.3.5.2 使用侧输出流"></a>1.3.5.2 使用侧输出流</h4><p>关于处理函数中侧输出流的用法，我们已经在7.5节做了详细介绍。简单来说，只需要调用上下文ctx的.output()方法，就可以输出任意类型的数据了。而侧输出流的标记和提取，都离不开一个“输出标签”（OutputTag），指定了侧输出流的id和类型。</p><p><strong>代码实现：</strong>将WaterSensor按照Id类型进行分流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByOutputTag</span> &#123;    </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">              .map(<span class="keyword">new</span> <span class="title class_">WaterSensorMapFunction</span>());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s1 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s1&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s2 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s2&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">       <span class="comment">//返回的都是主流</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds1 = ds.process(<span class="keyword">new</span> <span class="title class_">ProcessFunction</span>&lt;WaterSensor, WaterSensor&gt;()</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="string">&quot;s1&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s1, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;s2&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s2, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">//主流</span></span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;主流，非s1,s2的传感器&quot;</span>);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s1DS = ds1.getSideOutput(s1);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s2DS = ds1.getSideOutput(s2);</span><br><span class="line"></span><br><span class="line">        s1DS.printToErr(<span class="string">&quot;s1&quot;</span>);</span><br><span class="line">        s2DS.printToErr(<span class="string">&quot;s2&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-6-基本合流操作"><a href="#1-3-6-基本合流操作" class="headerlink" title="1.3.6 基本合流操作"></a><strong>1.3.6</strong> <strong>基本合流操作</strong></h3><h5 id="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"><a href="#在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。" class="headerlink" title="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"></a>在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。</h5><h3 id="1-3-6-1"><a href="#1-3-6-1" class="headerlink" title="1.3.6.1"></a>1.3.6.1</h3><h4 id="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"><a href="#最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。" class="headerlink" title="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"></a>最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。</h4><p><img src="https://pic1.imgdb.cn/item/6786448fd0e0a243d4f434cf.png"></p><h5 id="在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："><a href="#在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：" class="headerlink" title="在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："></a>在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream1.union(stream2, stream3, ...)</span><br></pre></td></tr></table></figure><h4 id="注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"><a href="#注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。" class="headerlink" title="注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"></a>注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。</h4><p><strong>代码实现：</strong>我们可以用下面的代码做一个简单测试：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UnionExample</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds1 = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds2 = env.fromElements(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; ds3 = env.fromElements(<span class="string">&quot;2&quot;</span>, <span class="string">&quot;2&quot;</span>, <span class="string">&quot;3&quot;</span>);</span><br><span class="line"></span><br><span class="line">        ds1.union(ds2,ds3.map(Integer::valueOf))</span><br><span class="line">           .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-6-2-连接（Connect）"><a href="#1-3-6-2-连接（Connect）" class="headerlink" title="1.3.6.2 连接（Connect）"></a>1.3.6.2 连接（Connect）</h4><h4 id="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"><a href="#流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。" class="headerlink" title="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"></a>流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。</h4><h5 id="1）连接流（ConnectedStreams）"><a href="#1）连接流（ConnectedStreams）" class="headerlink" title="1）连接流（ConnectedStreams）"></a>1）连接流（ConnectedStreams）</h5><p><img src="https://pic1.imgdb.cn/item/67864547d0e0a243d4f4350b.png"></p><p><strong>代码实现：</strong>需要分为两步：首先基于一条DataStream调用.connect()方法，传入另外一条DataStream作为参数，将两条流连接起来，得到一个ConnectedStreams；然后再调用同处理方法得到DataStream。这里可以的调用的同处理方法有.map()&#x2F;.flatMap()，以及.process()方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;Integer&gt; source1 = env.fromElements(1, 2, 3);</span></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;String&gt; source2 = env.fromElements(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);</span></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; source1 = env</span><br><span class="line">                .socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(i -&gt; Integer.parseInt(i));</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source2 = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">8888</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 使用 connect 合流</span></span><br><span class="line"><span class="comment">         * 1、一次只能连接 2条流</span></span><br><span class="line"><span class="comment">         * 2、流的数据类型可以不一样</span></span><br><span class="line"><span class="comment">         * 3、 连接后可以调用 map、flatmap、process来处理，但是各处理各的</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ConnectedStreams&lt;Integer, String&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connect.map(<span class="keyword">new</span> <span class="title class_">CoMapFunction</span>&lt;Integer, String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map1</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于数字流:&quot;</span> + value.toString();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于字母流:&quot;</span> + value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码中，ConnectedStreams有两个类型参数，分别表示内部包含的两条流各自的数据类型；由于需要“一国两制”，因此调用.map()方法时传入的不再是一个简单的MapFunction，而是一个CoMapFunction，表示分别对两条流中的数据执行map操作。这个接口有三个类型参数，依次表示第一条流、第二条流，以及合并后的流中的数据类型。需要实现的方法也非常直白：.map1()就是对第一条流中数据的map操作，.map2()则是针对第二条流。</p><p>2）CoProcessFunction</p><p>与CoMapFunction类似，如果是调用.map()就需要传入一个CoMapFunction，需要实现map1()、map2()两个方法；而调用.process()时，传入的则是一个CoProcessFunction。它也是“处理函数”家族中的一员，用法非常相似。它需要实现的就是processElement1()、processElement2()两个方法，在每个数据到来时，会根据来源的流调用其中的一个方法进行处理。</p><p>值得一提的是，ConnectedStreams也可以直接调用.keyBy()进行按键分区的操作，得到的还是一个ConnectedStreams：</p><p>connectedStreams.keyBy(keySelector1, keySelector2);</p><p>这里传入两个参数keySelector1和keySelector2，是两条流中各自的键选择器；当然也可以直接传入键的位置值（keyPosition），或者键的字段名（field），这与普通的keyBy用法完全一致。ConnectedStreams进行keyBy操作，其实就是把两条流中key相同的数据放到了一起，然后针对来源的流再做各自处理，这在一些场景下非常有用。</p><p>案例需求：连接两条流，输出能根据id匹配上的数据（类似inner join效果）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectKeybyDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Tuple2&lt;Integer, String&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a1&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a2&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">2</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa1&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa2&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="string">&quot;bb&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="string">&quot;cc&quot;</span>, <span class="number">1</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 多并行度下，需要根据 关联条件 进行keyby，才能保证key相同的数据到一起去，才能匹配上</span></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connectKey = connect.keyBy(s1 -&gt; s1.f0, s2 -&gt; s2.f0);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connectKey.process(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">CoProcessFunction</span>&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 定义 HashMap，缓存来过的数据，key=id，value=list&lt;数据&gt;</span></span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple2&lt;Integer, String&gt;&gt;&gt; s1Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt;&gt; s2Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement1</span><span class="params">(Tuple2&lt;Integer, String&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s1数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple2&lt;Integer, String&gt;&gt; s1Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s1Values.add(value);</span><br><span class="line">                            s1Cache.put(id, s1Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s1Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s2的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple3&lt;Integer, String, Integer&gt; s2Element : s2Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + value + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + s2Element);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement2</span><span class="params">(Tuple3&lt;Integer, String, Integer&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s2数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; s2Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s2Values.add(value);</span><br><span class="line">                            s2Cache.put(id, s2Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s2Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s1的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple2&lt;Integer, String&gt; s1Element : s1Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + s1Element + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + value);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4输出算子"><a href="#1-4输出算子" class="headerlink" title="1.4输出算子"></a>1.4输出算子</h3><h4 id="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"><a href="#Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。" class="headerlink" title="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"></a>Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。</h4><p><img src="https://pic1.imgdb.cn/item/678645c8d0e0a243d4f43592.png"></p><h3 id="1-4-1-连接到外部系统"><a href="#1-4-1-连接到外部系统" class="headerlink" title="1.4.1 连接到外部系统"></a><strong>1.4.1</strong> <strong>连接到外部系统</strong></h3><h4 id="Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"><a href="#Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。" class="headerlink" title="Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"></a>Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。</h4><h5 id="Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。"><a href="#Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。" class="headerlink" title="Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。"></a>Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。</h5><h5 id="stream-addSink-new-SinkFunction-…"><a href="#stream-addSink-new-SinkFunction-…" class="headerlink" title="stream.addSink(new SinkFunction(…));"></a>stream.addSink(new SinkFunction(…));</h5><h5 id="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"><a href="#addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。" class="headerlink" title="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"></a>addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。</h5><h5 id="Flink1-12开始，同样重构了Sink架构，"><a href="#Flink1-12开始，同样重构了Sink架构，" class="headerlink" title="Flink1.12开始，同样重构了Sink架构，"></a>Flink1.12开始，同样重构了Sink架构，</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.sinkTo(…)</span><br></pre></td></tr></table></figure><h4 id="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："><a href="#当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：" class="headerlink" title="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："></a>当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：</h4><p><img src="https://pic1.imgdb.cn/item/678645f5d0e0a243d4f435e2.png"></p><h4 id="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"><a href="#我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。" class="headerlink" title="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"></a>我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。</h4><h4 id="除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"><a href="#除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。" class="headerlink" title="除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"></a>除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。</h4><p><img src="https://pic1.imgdb.cn/item/6786460cd0e0a243d4f43614.png"></p><h4 id="除此以外，就需要用户自定义实现sink连接器了。"><a href="#除此以外，就需要用户自定义实现sink连接器了。" class="headerlink" title="除此以外，就需要用户自定义实现sink连接器了。"></a>除此以外，就需要用户自定义实现sink连接器了。</h4><h3 id="5-4-2-输出到文件"><a href="#5-4-2-输出到文件" class="headerlink" title="5.4.2 输出到文件"></a><strong>5.4.2</strong> <strong>输出到文件</strong></h3><h4 id="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"><a href="#Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。" class="headerlink" title="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"></a>Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。</h4><h4 id="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："><a href="#FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：" class="headerlink" title="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："></a>FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：</h4><ul><li>行编码： FileSink.forRowFormat（basePath，rowEncoder）。</li><li>批量编码： FileSink.forBulkFormat（basePath，bulkWriterFactory）。</li></ul><h4 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ch.qos.logback.core.util.<span class="type">TimeUtil</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringEncoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.<span class="type">Path</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.<span class="type">StreamingFileSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToFileTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 直接以文本形式分布式的写入到文件中</span></span><br><span class="line">    <span class="comment">// SimpleStringEncoder作用是将String转成char方便写入</span></span><br><span class="line">    <span class="keyword">val</span> fileSink = <span class="type">StreamingFileSink</span></span><br><span class="line">      .forRowFormat(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;./output&quot;</span>), <span class="keyword">new</span> <span class="type">SimpleStringEncoder</span>[<span class="type">String</span>](<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    stream.broadcast.map(_.toString).addSink(fileSink)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-3-输出到Kafka"><a href="#1-4-3-输出到Kafka" class="headerlink" title="1.4.3 输出到Kafka"></a><strong>1.4.3</strong> 输出到Kafka</h3><h3 id="（1）添加Kafka-连接器依赖"><a href="#（1）添加Kafka-连接器依赖" class="headerlink" title="（1）添加Kafka 连接器依赖"></a>（1）添加Kafka 连接器依赖</h3><h3 id="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"><a href="#由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。" class="headerlink" title="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"></a>由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。</h3><h3 id="（2）启动Kafka集群"><a href="#（2）启动Kafka集群" class="headerlink" title="（2）启动Kafka集群"></a>（2）启动Kafka集群</h3><h3 id="（3）编写输出到Kafka的示例代码"><a href="#（3）编写输出到Kafka的示例代码" class="headerlink" title="（3）编写输出到Kafka的示例代码"></a>（3）编写输出到Kafka的示例代码</h3><h3 id="输出无key的record"><a href="#输出无key的record" class="headerlink" title="输出无key的record:"></a>输出无key的record:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.&#123;<span class="type">FlinkKafkaConsumer</span>, <span class="type">FlinkKafkaProducer</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;lhxcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="comment">// trim就是去空格</span></span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong).toString</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 讲数据写入到kafka</span></span><br><span class="line">    stream.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](<span class="string">&quot;master:9092&quot;</span>, <span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="然后开一个消费者查看是否到数据"><a href="#然后开一个消费者查看是否到数据" class="headerlink" title="然后开一个消费者查看是否到数据"></a>然后开一个消费者查看是否到数据</h3><h3 id="1-4-4-输出到MySQL（JDBC）"><a href="#1-4-4-输出到MySQL（JDBC）" class="headerlink" title="1.4.4 输出到MySQL（JDBC）"></a>1.4.4 输出到MySQL（JDBC）</h3><h3 id="写入数据的MySQL的测试步骤如下。"><a href="#写入数据的MySQL的测试步骤如下。" class="headerlink" title="写入数据的MySQL的测试步骤如下。"></a>写入数据的MySQL的测试步骤如下。</h3><h3 id="（1）添加依赖"><a href="#（1）添加依赖" class="headerlink" title="（1）添加依赖"></a>（1）添加依赖</h3><h3 id="添加MySQL驱动："><a href="#添加MySQL驱动：" class="headerlink" title="添加MySQL驱动："></a>添加MySQL驱动：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径："><a href="#官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径：" class="headerlink" title="官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径："></a>官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>apache-snapshots<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>apache snapshots<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.apache.org/content/repositories/snapshots/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="添加依赖："><a href="#添加依赖：" class="headerlink" title="添加依赖："></a>添加依赖：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.17-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："><a href="#如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：" class="headerlink" title="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："></a>如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">            &lt;id&gt;aliyunmaven&lt;/id&gt;</span><br><span class="line">            &lt;mirrorOf&gt;*,!apache-snapshots&lt;/mirrorOf&gt;</span><br><span class="line">            &lt;name&gt;阿里云公共仓库&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;https:<span class="comment">//maven.aliyun.com/repository/public&lt;/url&gt;</span></span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）启动MySQL，在test库下建表ws"><a href="#（2）启动MySQL，在test库下建表ws" class="headerlink" title="（2）启动MySQL，在test库下建表ws"></a>（2）启动MySQL，在test库下建表ws</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span>     </span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `ws` (</span><br><span class="line">  `id` <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `ts` <span class="type">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `vc` <span class="type">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (`id`)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8</span><br></pre></td></tr></table></figure><h4 id="（3）编写输出到MySQL的示例代码"><a href="#（3）编写输出到MySQL的示例代码" class="headerlink" title="（3）编写输出到MySQL的示例代码"></a>（3）编写输出到MySQL的示例代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.&#123;<span class="type">JdbcConnectionOptions</span>, <span class="type">JdbcSink</span>, <span class="type">JdbcStatementBuilder</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">PreparedStatement</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToMysqlTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    stream.addSink(<span class="type">JdbcSink</span>.sink(</span><br><span class="line">      <span class="string">&quot;insert into shop (name, area, dizhi, price) values(?, ?, ?, ?)&quot;</span>, <span class="comment">// 定义写入Mysql的语句</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcStatementBuilder</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(t: <span class="type">PreparedStatement</span>, u: <span class="type">Event</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          t.setString(<span class="number">1</span>, u.user)</span><br><span class="line">          t.setString(<span class="number">2</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">3</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">4</span>, u.url)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcConnectionOptions</span>.<span class="type">JdbcConnectionOptionsBuilder</span>()</span><br><span class="line">        .withUrl(<span class="string">&quot;jdbc:mysql://master:3306/test?useSSL=false&quot;</span>)</span><br><span class="line">        .withDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">        .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">        .withPassword(<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">        .build()</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"><a href="#（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。" class="headerlink" title="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"></a>（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。</h4><h3 id="1-4-5-自定义Sink输出"><a href="#1-4-5-自定义Sink输出" class="headerlink" title="1.4.5 自定义Sink输出"></a><strong>1.4.5</strong> <strong>自定义Sink输出</strong></h3><h4 id="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。"><a href="#如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。" class="headerlink" title="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。"></a>如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> <span class="type">MySinkFunction</span>&lt;<span class="type">String</span>&gt;());</span><br></pre></td></tr></table></figure><h4 id="在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"><a href="#在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。" class="headerlink" title="在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"></a>在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。</h4><h4 id="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"><a href="#这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。" class="headerlink" title="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"></a>这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。</h4>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据可视化学习路线</title>
      <link href="/posts/1083.html"/>
      <url>/posts/1083.html</url>
      
        <content type="html"><![CDATA[<h1 id="大数据数据可视化学习路线"><a href="#大数据数据可视化学习路线" class="headerlink" title="大数据数据可视化学习路线"></a>大数据数据可视化学习路线</h1><h5 id="前提：具备一定的Python基础"><a href="#前提：具备一定的Python基础" class="headerlink" title="前提：具备一定的Python基础"></a>前提：具备一定的Python基础</h5><h2 id="1-python的基础学习，以及进阶学习"><a href="#1-python的基础学习，以及进阶学习" class="headerlink" title="1.python的基础学习，以及进阶学习"></a>1.python的基础学习，以及进阶学习</h2><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href>https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="2-numpy、pandas的入门学习（先学numpy）"><a href="#2-numpy、pandas的入门学习（先学numpy）" class="headerlink" title="2.numpy、pandas的入门学习（先学numpy）"></a>2.numpy、pandas的入门学习（先学numpy）</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="3-数据可视化matplotlib库的基础绘图"><a href="#3-数据可视化matplotlib库的基础绘图" class="headerlink" title="3.数据可视化matplotlib库的基础绘图"></a>3.数据可视化matplotlib库的基础绘图</h3><p>【千锋教育python数据可视化Matplotlib绘图教程，Matplotlib柱状图｜Matplotlib动态图｜Matplotlib散点图】<a href>https://www.bilibili.com/video/BV1nM411m7Cf?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看"><a href="#4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看" class="headerlink" title="4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)"></a>4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="5-数据可视化pyecharts可交互式基础绘图"><a href="#5-数据可视化pyecharts可交互式基础绘图" class="headerlink" title="5.数据可视化pyecharts可交互式基础绘图"></a>5.数据可视化pyecharts可交互式基础绘图</h3><p>【千锋教育PyEcharts数据可视化快速入门教程，大数据分析Python交互绘图实用利器】<a href>https://www.bilibili.com/video/BV1nM411F7GT?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="6-数据可视化tableau图标绘图"><a href="#6-数据可视化tableau图标绘图" class="headerlink" title="6.数据可视化tableau图标绘图"></a>6.数据可视化tableau图标绘图</h3><p>【【Tableau教程】Tableau零基础教程，带你解锁当下最受欢迎的数据可视化软件】<a href>https://www.bilibili.com/video/BV1E4411B7ef?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="7-数据交互式可视化进阶绘图前提基础html"><a href="#7-数据交互式可视化进阶绘图前提基础html" class="headerlink" title="7.数据交互式可视化进阶绘图前提基础html"></a>7.数据交互式可视化进阶绘图前提基础html</h3><p>【黑马程序员pink老师前端入门教程，零基础必看的h5(html5)+css3+移动端前端视频教程】<a href>https://www.bilibili.com/video/BV14J4114768?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="8-数据可视化echarts可交互式进阶绘图"><a href="#8-数据可视化echarts可交互式进阶绘图" class="headerlink" title="8.数据可视化echarts可交互式进阶绘图"></a>8.数据可视化echarts可交互式进阶绘图</h3><p>【电商平台数据可视化实时监控系统-Echarts-vue项目综合练习-pink老师推荐(持续更新)素材已经更新】<a href>https://www.bilibili.com/video/BV1bh41197p8?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="9-使用vue进行数据可视化前提基础Node-js"><a href="#9-使用vue进行数据可视化前提基础Node-js" class="headerlink" title="9.使用vue进行数据可视化前提基础Node.js"></a>9.使用vue进行数据可视化前提基础Node.js</h3><p>【黑马程序员Node.js全套入门教程，nodejs新教程含es6模块化+npm+express+webpack+promise等_Nodejs实战案例详解】<a href>https://www.bilibili.com/video/BV1a34y167AZ?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="10-使用vue进行数据可视化"><a href="#10-使用vue进行数据可视化" class="headerlink" title="10.使用vue进行数据可视化"></a>10.使用vue进行数据可视化</h3><p>【千锋Echarts+Vue3.0数据可视化项目构建_入门必备前端项目实战教程】<a href>https://www.bilibili.com/video/BV14u411D7qK?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习路线 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习路线 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据处理学习路线</title>
      <link href="/posts/1082.html"/>
      <url>/posts/1082.html</url>
      
        <content type="html"><![CDATA[<ul><li><h1 id="大数据数据处理学习路线"><a href="#大数据数据处理学习路线" class="headerlink" title="大数据数据处理学习路线"></a>大数据数据处理学习路线</h1><p><strong>注意</strong>：下列无标注的全部要看</p><h2 id="第一阶段：基础部分"><a href="#第一阶段：基础部分" class="headerlink" title="第一阶段：基础部分"></a>第一阶段：基础部分</h2><ul><li>语言基础：python、java基础<ul><li>python：【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注释：第一阶段</li><li>java：【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>数据库基础：mysql<ul><li>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：基础篇</li></ul></li><li>linux：命令基础（）<ul><li>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></li></ul></li></ul><h2 id="第二部分：简单工具使用"><a href="#第二部分：简单工具使用" class="headerlink" title="第二部分：简单工具使用"></a>第二部分：简单工具使用</h2><ul><li><p>python第三方库：pandas、numpy、requests、bs4、jieba、snownlp</p><table><thead><tr><th>第三方库</th><th>链接</th></tr></thead><tbody><tr><td>pandas、numpy</td><td>【千锋教育python数据分析教程200集，Python数据分析师入门必备视频】<a href="https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E6%B3%A8%EF%BC%9Ap38-p117%EF%BC%89">https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58（注：p38-p117）</a></td></tr><tr><td>requests、bs4</td><td>【尚硅谷Python爬虫教程小白零基础速通（含python基础+爬虫案例）】<a href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a> (注：p52-最后)</td></tr><tr><td>jieba</td><td>【Python Jieba 中文分词工具】<a href="https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></td></tr><tr><td>snownlp</td><td>【Lecture 12 基于Snownlp的文本情感分析】<a href="https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E7%9C%8B%E5%AE%8C%E8%BF%99%E9%9B%86%E5%B0%B1%E8%A1%8C%E4%BA%86%EF%BC%89">https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58（看完这集就行了）</a></td></tr></tbody></table></li><li><p>excel：函数使用以及操作</p><ul><li>【2025必看！全网最新最细最实用Excel零基础入门到精通全套教程！专为零基础小白打造！内容富含Excel表格基础操作、实用函数讲解、项目实战等！】<a href="https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li><p>hadoop：hadoop的基本使用命令</p><ul><li><a href="https://blog.csdn.net/m0_43405302/article/details/122243263">hadoop的HDFS的shell命令大全（一篇文章就够了）_shell统计hdfs-CSDN博客</a></li></ul></li></ul><h2 id="第三部分：集群工具使用"><a href="#第三部分：集群工具使用" class="headerlink" title="第三部分：集群工具使用"></a>第三部分：集群工具使用</h2><ul><li><p>scala：scala语言基础</p><ul><li>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：看完前十章</li></ul></li><li><p>mapreduce：了解基本使用以及自定义方法</p><ul><li><p>【黑马程序员大数据Hadoop3.x全套教程，一套精通Hadoop的大数据入门教程】<a href="https://www.bilibili.com/video/BV11N411d7Zh?p=214&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV11N411d7Zh?p=214&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p171-214</p></li></ul></li><li><p>spark：掌握sparkcore sparksql</p><ul><li>【全网最全大数据Spark3.0教程 Spark3.0从入门到精通 黑马程序员大数据入门教程系列】<a href="https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：搭建部分不用看、只看sparkcore和sparksql</li></ul></li><li><p>hive：了解hive命令以及udf自定义函数</p><ul><li><p>【黑马程序员Hive全套教程，大数据Hive3.x数仓开发精讲到企业级实战应用】<a href="https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p0-p96</p></li></ul></li></ul><h2 id="第四部分：深入学习"><a href="#第四部分：深入学习" class="headerlink" title="第四部分：深入学习"></a>第四部分：深入学习</h2><ul><li>机器学习：了解机器学习原理以及sklearn使用<ul><li>【黑马程序员3天快速入门python机器学习】<a href="https://www.bilibili.com/video/BV1nt411r7tj?p=18&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1nt411r7tj?p=18&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>spark streaming：了解流式数据<ul><li>自己找视频或文档</li></ul></li><li>项目制作：尝试制作大数据项目</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习路线 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习路线 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集群搭建学习路线</title>
      <link href="/posts/1081.html"/>
      <url>/posts/1081.html</url>
      
        <content type="html"><![CDATA[<h1 id="大数据集群搭建学习路线"><a href="#大数据集群搭建学习路线" class="headerlink" title="大数据集群搭建学习路线"></a>大数据集群搭建学习路线</h1><h3 id="前提：熟练使用Linux的命令及其操作"><a href="#前提：熟练使用Linux的命令及其操作" class="headerlink" title="前提：熟练使用Linux的命令及其操作"></a>前提：熟练使用Linux的命令及其操作</h3><h3 id="0-CentosLinux的基操"><a href="#0-CentosLinux的基操" class="headerlink" title="0.CentosLinux的基操"></a>0.CentosLinux的基操</h3><p>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="1-Mysql数据库的入门学习"><a href="#1-Mysql数据库的入门学习" class="headerlink" title="1.Mysql数据库的入门学习"></a>1.Mysql数据库的入门学习</h3><p>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="2-Hadoop集群的搭建及其使用和Hive的使用"><a href="#2-Hadoop集群的搭建及其使用和Hive的使用" class="headerlink" title="2.Hadoop集群的搭建及其使用和Hive的使用"></a>2.Hadoop集群的搭建及其使用和Hive的使用</h3><p>【黑马程序员大数据入门到实战教程，大数据开发必会的Hadoop、Hive，云平台实战项目全套一网打尽】<a href="https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="3-Java的基础学习（上部）"><a href="#3-Java的基础学习（上部）" class="headerlink" title="3.Java的基础学习（上部）"></a>3.Java的基础学习（上部）</h3><p>【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="4-scala语言的学习"><a href="#4-scala语言的学习" class="headerlink" title="4.scala语言的学习"></a>4.scala语言的学习</h3><p>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="5-Python基础的学习"><a href="#5-Python基础的学习" class="headerlink" title="5.Python基础的学习"></a>5.Python基础的学习</h3><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="6-zookeeper组件的学习"><a href="#6-zookeeper组件的学习" class="headerlink" title="6.zookeeper组件的学习"></a>6.zookeeper组件的学习</h3><p>【黑马程序员Zookeeper视频教程，快速入门zookeeper技术】<a href="https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="7-kafka组件的利用"><a href="#7-kafka组件的利用" class="headerlink" title="7.kafka组件的利用"></a>7.kafka组件的利用</h3><p>【尚硅谷Kafka教程，2024新版kafka视频，零基础入门到实战】<a href="https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="8-Flume的使用"><a href="#8-Flume的使用" class="headerlink" title="8.Flume的使用"></a>8.Flume的使用</h3><p>【【海牛大数据】Flume教程（学Flume看这个一套就够了，从基础到扩展到实战案例到组件配合全都有）】<a href="https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="9-Sqoop的运用"><a href="#9-Sqoop的运用" class="headerlink" title="9.Sqoop的运用"></a>9.Sqoop的运用</h3><p>【【海牛大数据】Sqoop教程（命令详解、各组件融合、实战案例）】<a href="https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="10-Spark的搭建"><a href="#10-Spark的搭建" class="headerlink" title="10.Spark的搭建"></a>10.Spark的搭建</h3><h4 id="Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）"><a href="#Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）" class="headerlink" title="Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）"></a>Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）</h4><p><a href="https://blog.csdn.net/JunLeon/article/details/123625680">(超详细) Spark环境搭建（Local模式、 StandAlone模式、Spark On Yarn模式）-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_44480968/article/details/119580363">Spark配置高可用（HA）_spark ha-CSDN博客</a></p><h3 id="11-Flink搭建及其使用-由于python版的使用还不是特别完整"><a href="#11-Flink搭建及其使用-由于python版的使用还不是特别完整" class="headerlink" title="11.Flink搭建及其使用(由于python版的使用还不是特别完整)"></a>11.Flink搭建及其使用(由于python版的使用还不是特别完整)</h3><h4 id="scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="scala版：【【尚硅谷】Flink1.13教程（Scala版）】https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>scala版：【【尚硅谷】Flink1.13教程（Scala版）】<a href="https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h4 id="java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】<a href="https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h3 id="11-Redis的搭建及其使用"><a href="#11-Redis的搭建及其使用" class="headerlink" title="11.Redis的搭建及其使用"></a>11.Redis的搭建及其使用</h3><p>【黑马程序员Redis入门到实战教程，深度透析redis底层原理+redis分布式锁+企业解决方案+黑马点评实战项目】<a href="https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="12-Hbase的搭建及其使用"><a href="#12-Hbase的搭建及其使用" class="headerlink" title="12.Hbase的搭建及其使用"></a>12.Hbase的搭建及其使用</h3><p>【【好程序员】大数据全新分布式存储HBase精品课程】<a href="https://www.bilibili.com/video/BV1RM411i7XM?p=15&vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1RM411i7XM?p=15&amp;vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="13-Azkaban的搭建及其使用"><a href="#13-Azkaban的搭建及其使用" class="headerlink" title="13.Azkaban的搭建及其使用"></a>13.Azkaban的搭建及其使用</h3><p>【尚硅谷大数据Azkaban 3.x教程（全新发布）】<a href="https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习路线 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习路线 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop原理</title>
      <link href="/posts/3985.html"/>
      <url>/posts/3985.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png"></p>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop多map条件查询导入hdfs.md</title>
      <link href="/posts/3992.html"/>
      <url>/posts/3992.html</url>
      
        <content type="html"><![CDATA[<h2 id="多map条件查询导入hdfs"><a href="#多map条件查询导入hdfs" class="headerlink" title="多map条件查询导入hdfs"></a>多map条件查询导入hdfs</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect 数据库连接字符串 \</span><br><span class="line">--username 数据库用户名 \</span><br><span class="line">--password 数据库密码 \</span><br><span class="line">--target-dir hdfs位置 \</span><br><span class="line">--delete-target-dir \  <span class="comment"># 这个就是把目录删了，不然mapreduce会执行失败</span></span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \ <span class="comment"># 使用什么分隔符</span></span><br><span class="line">--num-mappers 3 \</span><br><span class="line">--split-by 切分数依据 \</span><br><span class="line">--query <span class="string">&#x27; SQL语句 and $CONDITIONS &#x27;</span></span><br></pre></td></tr></table></figure><h3 id="–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"><a href="#–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力" class="headerlink" title="–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"></a><strong>–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力</strong></h3><h3 id="CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理"><a href="#CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理" class="headerlink" title="$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理"></a><strong>$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理</strong></h3>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop实例命令</title>
      <link href="/posts/3986.html"/>
      <url>/posts/3986.html</url>
      
        <content type="html"><![CDATA[<h4 id="创建hive的表根据mysql上的表进行创建-create-hive-table"><a href="#创建hive的表根据mysql上的表进行创建-create-hive-table" class="headerlink" title="创建hive的表根据mysql上的表进行创建(create-hive-table)"></a><strong>创建hive的表根据mysql上的表进行创建(create-hive-table)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table --connect jdbc:mysql://master:3306/sqoop_db --username root --password 123456 --table city --hive-table hkjcpdd.city</span><br></pre></td></tr></table></figure><h4 id="查看mysql上有什么表-list-tables"><a href="#查看mysql上有什么表-list-tables" class="headerlink" title="查看mysql上有什么表(list-tables)"></a><strong>查看mysql上有什么表(list-tables)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-tables --connect jdbc:mysql://master:3306/sys --username root --password 123456</span><br></pre></td></tr></table></figure><h3 id="查看mysql上有什么库-list-databases"><a href="#查看mysql上有什么库-list-databases" class="headerlink" title="查看mysql上有什么库(list-databases)"></a><strong>查看mysql上有什么库(list-databases)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://master:3306/ --username root --password 12345</span><br></pre></td></tr></table></figure><h3 id="使用sql进行操作-eval-query"><a href="#使用sql进行操作-eval-query" class="headerlink" title="使用sql进行操作(eval   query)"></a><strong>使用sql进行操作(eval   query)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">eval</span> --connect jdbc:mysql://master:3306/sqoop_db --username root --password  123456 --query <span class="string">&quot;select * from city limit 2;&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导入其他格式文件</title>
      <link href="/posts/3994.html"/>
      <url>/posts/3994.html</url>
      
        <content type="html"><![CDATA[<h2 id="导入其他格式文件"><a href="#导入其他格式文件" class="headerlink" title="导入其他格式文件"></a>导入其他格式文件</h2><h3 id="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式"><a href="#导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式" class="headerlink" title="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)"></a><strong>导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /data/hkjcpdd \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-parquetfile \ <span class="comment">#文件格式</span></span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query <span class="string">&#x27;select * from city where id &lt; 10 and $CONDITIONS&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop应用案例</title>
      <link href="/posts/3987.html"/>
      <url>/posts/3987.html</url>
      
        <content type="html"><![CDATA[<h3 id="创建一个执行文件然后给予权限然后执行"><a href="#创建一个执行文件然后给予权限然后执行" class="headerlink" title="创建一个执行文件然后给予权限然后执行"></a><strong>创建一个执行文件然后给予权限然后执行</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">batch_date=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--usernmae root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir hive表hdfs目录 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query select * from city</span><br><span class="line"></span><br><span class="line">result=$?</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [<span class="variable">$result</span> != 0];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;执行失败&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd</span><br><span class="line"><span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span>  <span class="string">&quot;执行成功&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop搭建</title>
      <link href="/posts/3984.html"/>
      <url>/posts/3984.html</url>
      
        <content type="html"><![CDATA[<h2 id="Sqoop搭建"><a href="#Sqoop搭建" class="headerlink" title="Sqoop搭建"></a>Sqoop搭建</h2><h3 id="1-解压"><a href="#1-解压" class="headerlink" title="1.解压"></a>1.解压</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /root/software</span><br></pre></td></tr></table></figure><h3 id="更改名字"><a href="#更改名字" class="headerlink" title="更改名字"></a>更改名字</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/software</span><br><span class="line"><span class="built_in">mv</span> sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop</span><br></pre></td></tr></table></figure><h3 id="2-添加环境变量"><a href="#2-添加环境变量" class="headerlink" title="2.添加环境变量"></a>2.添加环境变量</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">export</span> SQOOP_HOME=/root/software/sqoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SQOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure><h3 id="3-配置-Sqoop-环境变量文件"><a href="#3-配置-Sqoop-环境变量文件" class="headerlink" title="3.配置 Sqoop 环境变量文件"></a>3.配置 Sqoop 环境变量文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切换到 Sqoop 配置文件目录</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$SQOOP_HOME</span>/conf</span><br><span class="line"><span class="comment"># 复制 Sqoop 环境变量模板文件</span></span><br><span class="line"><span class="built_in">cp</span> sqoop-env-template.sh sqoop-env.sh </span><br><span class="line"><span class="comment"># 编辑文件，指定相关路径</span></span><br><span class="line">vi sqoop-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最底下的配置加上，没有装的就不用去掉#号</span></span><br></pre></td></tr></table></figure><h3 id="4-MySQL-驱动"><a href="#4-MySQL-驱动" class="headerlink" title="4. MySQL 驱动"></a>4. MySQL 驱动</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拷贝 MySQL 驱动到 Sqoop 中的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> /opt/software/mysql-connector-java-5.1.37-bin.jar <span class="variable">$SQOOP_HOME</span>/lib</span><br></pre></td></tr></table></figure><h3 id="5-拷贝-Hive-文件"><a href="#5-拷贝-Hive-文件" class="headerlink" title="5. 拷贝 Hive 文件"></a>5. 拷贝 Hive 文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了后续方便操作 Hive，我们需要将 Hive 的驱动放入 Sqoop 的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> hive/lib/hive-common-3.1.2.jar sqoop/lib/</span><br></pre></td></tr></table></figure><h3 id="6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）"><a href="#6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）" class="headerlink" title="6.验证（输入 sqoop version，出现如下版本信息表示安装成功）"></a>6.验证（输入 <code>sqoop version</code>，出现如下版本信息表示安装成功）</h3><h3 id="7-展示Mysql中sys库下的所有表"><a href="#7-展示Mysql中sys库下的所有表" class="headerlink" title="7.展示Mysql中sys库下的所有表"></a>7.展示Mysql中sys库下的所有表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list<span class="operator">-</span>tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/sys \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123456</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导入到Hbase</title>
      <link href="/posts/3988.html"/>
      <url>/posts/3988.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-table hkjcpdd:city \</span><br><span class="line">--column-family cf \</span><br><span class="line">--hbase-row-key <span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --hbase-row-key:要求mysql表必须有主见，将主键作为rowkey,表示一行</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop</title>
      <link href="/posts/3989.html"/>
      <url>/posts/3989.html</url>
      
        <content type="html"><![CDATA[<h2 id="数据的导入导出"><a href="#数据的导入导出" class="headerlink" title="数据的导入导出"></a>数据的导入导出</h2><p>导入：import</p><p>导出：export</p><p><img src="https://pic1.imgdb.cn/item/6784f9d1d0e0a243d4f3f083.png"></p>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全量导入hive表和增量导入hive表</title>
      <link href="/posts/3991.html"/>
      <url>/posts/3991.html</url>
      
        <content type="html"><![CDATA[<h3 id="全量导入："><a href="#全量导入：" class="headerlink" title="全量导入："></a><strong>全量导入：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-import \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table hive的库.hive的表</span><br></pre></td></tr></table></figure><h4 id="增量导入："><a href="#增量导入：" class="headerlink" title="增量导入："></a><strong>增量导入：</strong></h4><h5 id="1-append方式"><a href="#1-append方式" class="headerlink" title="1.append方式"></a><strong>1.append方式</strong></h5><h5 id="2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"><a href="#2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）" class="headerlink" title="2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"></a>2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）</h5><p><img src="https://pic1.imgdb.cn/item/6784fbd6d0e0a243d4f3f0e9.png"></p><h2 id="这个是第一种方式：incremental-append"><a href="#这个是第一种方式：incremental-append" class="headerlink" title="这个是第一种方式：incremental append"></a><strong>这个是第一种方式：incremental append</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line"></span><br><span class="line">--target-dir hdfs路径 \  这个路径是表的路径可以在hive中show create table city;就可以看到hdfs的位置了</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column <span class="built_in">id</span> \</span><br><span class="line">--last-value 1  这个就是看你的数字哪一行，如果是1那就从2开始增量数据</span><br></pre></td></tr></table></figure><p>参数解释：</p><p>1)incremental : append或lastmodified，使用lastmodified方式导入数据要指定增量数据是要 –append（追加）还是要 –merge-key（合并）</p><p>2)check-column&lt;字段&gt;: 作为增量导入判断的列名</p><p>3)last-value val : 指定某一个值，用于标记增量导入的位置，这个值的数据不会被导入列表中，只用于标记当前表中最后的值。</p><h3 id="第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）"><a href="#第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）" class="headerlink" title="第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）"></a><strong>第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--append</span><br><span class="line"></span><br><span class="line">--注意：last-value的设置把包括2020-10-10 13:00:00 时间的数据做增量导入</span><br></pre></td></tr></table></figure><h3 id="第三种导入方式–incremental-lastmodified-–append-（进行合并的）"><a href="#第三种导入方式–incremental-lastmodified-–append-（进行合并的）" class="headerlink" title="第三种导入方式–incremental lastmodified –append （进行合并的）"></a><strong>第三种导入方式–incremental lastmodified –append （进行合并的）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--merge-key <span class="built_in">id</span> <span class="comment"># 根据id来进行操作</span></span><br><span class="line"></span><br><span class="line">--incremental lastmodified --merge-key的作用：修改过得数据和新增的数据（前提是满足last-value的条件）都会导入尽力啊，并且重复的数据（不需要满足last-value的条件）都会进行合并</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导出</title>
      <link href="/posts/3990.html"/>
      <url>/posts/3990.html</url>
      
        <content type="html"><![CDATA[<h3 id="1-从hdfs导出到mysql里"><a href="#1-从hdfs导出到mysql里" class="headerlink" title="1.从hdfs导出到mysql里"></a><strong>1.从hdfs导出到mysql里</strong></h3><ol><li><h4 id="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"><a href="#要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表" class="headerlink" title="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"></a>要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表</h4></li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir hdfs路径指定到文件 \  <span class="comment"># 要导出的文件</span></span><br><span class="line">--table emp \    <span class="comment"># 导出到哪张Mysql表</span></span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--input-fields-terminated-by <span class="string">&#x27;|&#x27;</span>  <span class="comment"># 分隔符</span></span><br></pre></td></tr></table></figure><h3 id="2-从hive导出到Mysql中"><a href="#2-从hive导出到Mysql中" class="headerlink" title="2.从hive导出到Mysql中"></a><strong>2.从hive导出到Mysql中</strong></h3><h4 id="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"><a href="#sqoop的export命令支持insert、update到关系型数据库，但是不支持merge" class="headerlink" title="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"></a><strong>sqoop的export命令支持insert、update到关系型数据库，但是不支持merge</strong></h4><h4 id="1-hive表导入Mysql数据库insert（直接写入）"><a href="#1-hive表导入Mysql数据库insert（直接写入）" class="headerlink" title="1.hive表导入Mysql数据库insert（直接写入）"></a><strong>1.hive表导入Mysql数据库insert（直接写入）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--table cityhkjcpdd </span><br><span class="line">--num-mappers 4 </span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="2-从hive表导入mysql数据库update（追加替换相同）"><a href="#2-从hive表导入mysql数据库update（追加替换相同）" class="headerlink" title="2.从hive表导入mysql数据库update（追加替换相同）"></a><strong>2.从hive表导入mysql数据库update（追加替换相同）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--update-key <span class="built_in">id</span> \    <span class="comment"># 根据id来进行去重</span></span><br><span class="line">--fields-terminated-by <span class="string">&#x27;\t&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark RDD</title>
      <link href="/posts/fa63.html"/>
      <url>/posts/fa63.html</url>
      
        <content type="html"><![CDATA[<h3 id="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心"><a href="#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心" class="headerlink" title="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心"></a><strong>RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心</strong></h3><h4 id><a href="#" class="headerlink" title></a></h4><h4 id="Dataset-数据集"><a href="#Dataset-数据集" class="headerlink" title="Dataset:(数据集)"></a><strong>Dataset</strong><strong>:(数据集)</strong></h4><ul><li><strong>RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数；</strong></li><li><strong>RDD 也可以缓存起来, 相当于存储具体数据。</strong></li></ul><h4 id="Distributed-："><a href="#Distributed-：" class="headerlink" title="Distributed****："></a><strong>Distributed****：</strong></h4><h4 id="RDD-支持分区-可以运行在集群中。"><a href="#RDD-支持分区-可以运行在集群中。" class="headerlink" title="RDD 支持分区, 可以运行在集群中。"></a><strong>RDD 支持分区, 可以运行在集群中。</strong></h4><h4 id="Resilient-："><a href="#Resilient-：" class="headerlink" title="Resilient****："></a><strong>Resilient****：</strong></h4><ul><li><strong>RDD 支持高效的容错；</strong></li><li><strong>RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中。</strong></li></ul><h3 id="1-RDD的特点："><a href="#1-RDD的特点：" class="headerlink" title="1.RDD的特点："></a><strong>1.RDD的特点：</strong></h3><ul><li><p>弹性</p><ul><li>容错的弹性:数据丢失可以自动恢复;</li><li>存储的弹性:内存与磁盘的自动切换;</li><li>计算的弹性:计算出错重试机制;</li><li>分片的弹性:可根据需要重新分片。</li></ul></li><li><p>分布式:数据存储在集群不同节点上&#x2F;计算分布式。</p></li><li><p>数据集: RDD封装了计算逻辑，并不保存数据。</p></li><li><p>数据抽象: RDD是一个抽象类，需要子类具体实现。</p></li><li><p>不可变: RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑。</p></li><li><p>可分区、并行计算。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark on yarn</title>
      <link href="/posts/fa64.html"/>
      <url>/posts/fa64.html</url>
      
        <content type="html"><![CDATA[<h1 id="Spark-on-Yarn"><a href="#Spark-on-Yarn" class="headerlink" title="Spark on Yarn"></a>Spark on Yarn</h1><h3 id="SparkOnYarn本质"><a href="#SparkOnYarn本质" class="headerlink" title="SparkOnYarn本质"></a><strong>SparkOnYarn本质</strong></h3><p>master角色由yarn的Resourcemanager担任</p><p>worker角色由yarn的nodemanager担任</p><p>deiver角色运行在Yarn容器内或提交任务的客户端进程中</p><p>真正干活的Executor运行在yarn提供的容器内</p><h3 id="部署："><a href="#部署：" class="headerlink" title="部署："></a><strong>部署：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在spark-env.sh上只要添加这两个即可</span></span><br><span class="line">HADOOP_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line"><span class="comment"># 添加环境变量之后还要添加一个</span></span><br><span class="line"><span class="built_in">which</span> python</span><br><span class="line"><span class="built_in">export</span> SPARK_PYTHON=<span class="built_in">which</span> python</span><br><span class="line"><span class="comment"># 然后启动，bin/pyspark --master yarn</span></span><br></pre></td></tr></table></figure><h3 id="Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"><a href="#Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式" class="headerlink" title="Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"></a><strong>Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式</strong></h3><h3 id="这两种模式的区别就是Driver运行的位置"><a href="#这两种模式的区别就是Driver运行的位置" class="headerlink" title="这两种模式的区别就是Driver运行的位置"></a><strong>这两种模式的区别就是Driver运行的位置</strong></h3><h3 id="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"><a href="#集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内" class="headerlink" title="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"></a><strong>集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内</strong></h3><h3 id="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"><a href="#客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中" class="headerlink" title="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"></a><strong>客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端模式</span></span><br><span class="line">SPARK HOME=/export/server/spark<span class="variable">$&#123;SPARK HOME&#125;</span>/bin/spark-submit\--master yarn</span><br><span class="line">--deploy-mode client \（默认是客户端模式，不加也可以）</span><br><span class="line"><span class="comment"># 下面的参数可加可不加</span></span><br><span class="line">--driver-memory 512m \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line"><span class="variable">$&#123;SPARK HOME&#125;</span>/examples/src/main/python/pi.py 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群模式</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark搭建</title>
      <link href="/posts/fa65.html"/>
      <url>/posts/fa65.html</url>
      
        <content type="html"><![CDATA[<h1 id="spark安装部署"><a href="#spark安装部署" class="headerlink" title="spark安装部署"></a>spark安装部署</h1><h3 id="先安装anacondea3然后再解压spark"><a href="#先安装anacondea3然后再解压spark" class="headerlink" title="先安装anacondea3然后再解压spark"></a><strong>先安装anacondea3然后再解压spark</strong></h3><h3 id="然后追加以下内容至-x2F-root-x2F-condarc"><a href="#然后追加以下内容至-x2F-root-x2F-condarc" class="headerlink" title="然后追加以下内容至&#x2F;root&#x2F; .condarc"></a><strong>然后追加以下内容至&#x2F;root&#x2F; .condarc</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - defaults</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/fastai/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br></pre></td></tr></table></figure><h3 id="配置pyspark"><a href="#配置pyspark" class="headerlink" title="配置pyspark"></a><strong>配置pyspark</strong></h3><h3 id="conda-create-n-pyspark-python-x3D-版本号然后回车就下载了"><a href="#conda-create-n-pyspark-python-x3D-版本号然后回车就下载了" class="headerlink" title="conda create -n pyspark python&#x3D;版本号然后回车就下载了"></a><strong>conda create -n pyspark python&#x3D;版本号然后回车就下载了</strong></h3><h3 id="安装完之后conda-activate-pyspark切换虚拟环境"><a href="#安装完之后conda-activate-pyspark切换虚拟环境" class="headerlink" title="安装完之后conda activate pyspark切换虚拟环境"></a><strong>安装完之后conda activate pyspark切换虚拟环境</strong></h3><h1 id="Local环境部署"><a href="#Local环境部署" class="headerlink" title="Local环境部署"></a><strong>Local环境部署</strong></h1><h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a><strong>添加环境变量</strong></h3><h2 id="如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath"><a href="#如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath" class="headerlink" title="如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)"></a><strong>如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)</strong></h2><h3 id="然后就可以启动了pysparkspark-shell"><a href="#然后就可以启动了pysparkspark-shell" class="headerlink" title="然后就可以启动了pysparkspark shell"></a><strong>然后就可以启动了pysparkspark shell</strong></h3><h3 id="运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动"><a href="#运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动" class="headerlink" title="运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动"></a><strong>运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动</strong></h3><h3 id="Spark集群搭建"><a href="#Spark集群搭建" class="headerlink" title="Spark集群搭建"></a>Spark集群搭建</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br><span class="line">    <span class="comment"># 指定 Java Home</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/export/servers/jdk1.8.0_221</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定 Spark Master 地址</span></span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_HOST=node01</span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> works.template works</span><br><span class="line">    将Localhost删了换成master slave1 slave2</span><br><span class="line">    </span><br><span class="line">配置 HistoryServer</span><br><span class="line"><span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vi spark-defaults.conf (将这两个前面<span class="comment">#去掉)</span></span><br><span class="line">    spark.eventLog.enabled  <span class="literal">true</span></span><br><span class="line">    spark.eventLog.<span class="built_in">dir</span>      hdfs://node01:8020/spark_log</span><br><span class="line">vi spark-env.sh末尾添加</span><br><span class="line">    <span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:8020/spark_log&quot;</span></span><br><span class="line"> 创建hdfs目录</span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /spark_log</span><br><span class="line"></span><br><span class="line">修改log4j2.properties.template改名为log4j2.properties然后将19行的info改为WARN</span><br><span class="line"></span><br><span class="line">然后分发</span><br><span class="line">启动历史服务器：sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">最后启动</span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">连接，在webui的那个</span><br><span class="line">pyspark --master spark://master:7077</span><br></pre></td></tr></table></figure><h2 id="spark基于zookeeper实现HA"><a href="#spark基于zookeeper实现HA" class="headerlink" title="spark基于zookeeper实现HA"></a><strong>spark基于zookeeper实现HA</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前提确保zookeeper和hdfs均已启动</span><br><span class="line">vi spark-env.sh文件</span><br><span class="line">将<span class="built_in">export</span> SPARK_MASTER_HOST=master和他的那个端口注释了</span><br><span class="line">追加SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181,hadoop03:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span><br><span class="line">然后重新启动spark并且再启动另外一台或者多台机的start-master.sh</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Put事务和Take事务</title>
      <link href="/posts/fe67.html"/>
      <url>/posts/fe67.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">在source和chanel的传输中是批量过去的，channels传输到sinks也是批量的</span><br><span class="line"></span><br><span class="line">在source到channel中呢就有一个缓冲区</span><br><span class="line"></span><br><span class="line">1.doPut操作，浆皮数据写入到临时缓冲区 Putlist中</span><br><span class="line"></span><br><span class="line">2.doCommit操作：检查channel队列中是否有足够空间用来存放数据 有的话就会执行doCommit操作</span><br><span class="line"></span><br><span class="line">3.如果channel空间不够就会执行回滚数据的操作（doRollBack）</span><br><span class="line"></span><br><span class="line">在channel到sink中的事务叫take事务，也是批量过去的</span><br><span class="line"></span><br><span class="line">1.doTake操作，拉取一批channel的数据，然后进入缓冲区</span><br><span class="line"></span><br><span class="line">2.takeList操作，临时缓冲</span><br><span class="line"></span><br><span class="line">3.doCommit操作，将这一批数据发送出去，发送失败就会回滚</span><br><span class="line"></span><br><span class="line">4.doRollBack发送失败就进行回滚</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：Flume avro sink</title>
      <link href="/posts/fe65.html"/>
      <url>/posts/fe65.html</url>
      
        <content type="html"><![CDATA[<h2 id="avro-sink"><a href="#avro-sink" class="headerlink" title="avro sink"></a>avro sink</h2><p>需要配置的：type &#x3D; avro</p><p>hostname &#x3D; 主机名</p><p>port  &#x3D; 端口</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; avro<br>a1.sources.r1.bind &#x3D; 192.168.1.122<br>a1.sources.r1.port &#x3D; 55555&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1 c2<br>a1.sinks &#x3D; k1 k2</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100<br>a1.channels.c2.type &#x3D; memory<br>a1.channels.c2.capacity &#x3D; 10000<br>a1.channels.c2.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; avro<br>a1.sinks.k1.hostname &#x3D; 192.168.1.123<br>a1.sinks.k1.port &#x3D; 55555<br>a1.sinks.k2.type &#x3D; avro<br>=&#x3D;a1.sinks.k2.hostname &#x3D; 192.168.1.124<br>a1.sinks.k2.port &#x3D; 55555</p><p>a1.sources.r1.channels &#x3D; c1 c2<br>a1.sinks.k1.channel &#x3D; c1<br>a1.sinks.k2.channel &#x3D; c2</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume source的type</title>
      <link href="/posts/fe55.html"/>
      <url>/posts/fe55.html</url>
      
        <content type="html"><![CDATA[<h5 id="不支持断点续传"><a href="#不支持断点续传" class="headerlink" title="不支持断点续传"></a>不支持断点续传</h5><h1 id="spooling-directory-source"><a href="#spooling-directory-source" class="headerlink" title="spooling directory source"></a>spooling directory source</h1><h2 id="监听某一个目录，只要目录下有文件，文件中的数据就会收集"><a href="#监听某一个目录，只要目录下有文件，文件中的数据就会收集" class="headerlink" title="监听某一个目录，只要目录下有文件，文件中的数据就会收集"></a>监听某一个目录，只要目录下有文件，文件中的数据就会收集</h2><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; spooldir a1.sources.r1.spoolDir &#x3D; &#x2F;root&#x2F;spool</p><p>a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 10000 a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>注意Dir大写</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：Flume exec sources监听命令</title>
      <link href="/posts/fe64.html"/>
      <url>/posts/fe64.html</url>
      
        <content type="html"><![CDATA[<h2 id="exec-sources"><a href="#exec-sources" class="headerlink" title="exec sources"></a>exec sources</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; exec</p><p>a1.sources.r1.command &#x3D; tail -f &#x2F;root&#x2F;exec.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>那个命令就是需要监听的命令</p><p>他是不断的去监听你文件添加了什么</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：Flume file channel</title>
      <link href="/posts/fe63.html"/>
      <url>/posts/fe63.html</url>
      
        <content type="html"><![CDATA[<h2 id="file-channel"><a href="#file-channel" class="headerlink" title="file channel"></a>file channel</h2><p>需要的参数 type &#x3D; file</p><p>dataDirs &#x3D; &#x2F;roort</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.channels.c1.type &#x3D; file&#x3D;&#x3D;<br>a1.channels.c1.capaciry&#x3D;10000<br>a1.channels.c1.transactioncapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume file_roll sink</title>
      <link href="/posts/fe62.html"/>
      <url>/posts/fe62.html</url>
      
        <content type="html"><![CDATA[<h2 id="File-roll-sink"><a href="#File-roll-sink" class="headerlink" title="File_roll sink"></a>File_roll sink</h2><p>必须的：type &#x3D; file_roll</p><p>sink.directory 保存在那个目录&amp;#x20;</p><p>非必须：sink.rollInterval &#x3D; 30 就是每过30s就会生成一个新的文件用来存储数据</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; file_roll</p><p>a1.sinks.k1.sink.directory &#x3D; &#x2F;root&#x2F;file_roll&amp;#x20;</p><p>a1.sinks.k1.sink.rollInterval &#x3D; 10</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume hdfs sink</title>
      <link href="/posts/fe60.html"/>
      <url>/posts/fe60.html</url>
      
        <content type="html"><![CDATA[<h2 id="hdfs-sink"><a href="#hdfs-sink" class="headerlink" title="hdfs sink"></a>hdfs sink</h2><p>hdfs sink是将flume收集到的数据写入到hdfs中，方便数据可靠的保存</p><p>sink输出到hdfs中，默认每10个event生成一个hdfs文件，hdfs文件目录会根据hdfs.path的配置自动创建</p><p>配置参数：</p><p>hdfs.pathhdfs目录路径</p><p>hdsf.filePrefix文件前缀，默认值是FlumeData</p><p>hdfs.fileSuffix文件后缀</p><p>hdfs.rollnterval就是滚动，设置多长时间创建一个新文件进行存储，默认是30s</p><p>hdfs.rollSize文件大小超过一定值后，然后再创建一个新文件进行存储，默认是1024</p><p>hdfs.rollCount写入了多少个事件然后再创建一个新文件进行存储，默认是10个，设置为0的话表示不基于事件个数</p><p>hdfs.file.Type文件格式，有三种格式可选择：SequenceFile（默认，二进制），DataStream(不压缩,以文本的形式【方便观察】)，CompressedStream（可压缩）</p><p>hdfs.batchSize批数次，HDFS Sink每次从Channel中拿的事件个数。默认值100（就是每100个事件就从sink中传到hdfs上一次）## 一般不设置</p><p>hdfs.maxOpenFiles允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭 ## 一般不设置</p><p>hdfs.callTimeouthdfs允许操作的事件，比如hdfs文件的open, write, flush, close操作，单位是ms，默认值10000</p><p>hdfs.codeC压缩编解码器。以下之一：gzip, bzip2, lzo, lzop, snappy</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; hdfs <br>a1.sinks.k1.hdfs.path &#x3D; &#x2F;data&#x2F;hkjcpdd&#x2F;%Y-%m-%d <br>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true <br>a1.sinks.k1.hdfs.rollInterval&#x3D; 10 <br>a1.sinks.k1.hdfs.fileType &#x3D; DataStream</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true这个是使用本地的事件戳</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume http source 监听Http</title>
      <link href="/posts/fe59.html"/>
      <url>/posts/fe59.html</url>
      
        <content type="html"><![CDATA[<h2 id="http-source-监听Http"><a href="#http-source-监听Http" class="headerlink" title="http source 监听Http"></a>http source 监听Http</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; http<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试办法curl -X POST -d ‘[{“headers”:{“key”:”Flume”},”body”:”TestEvent1”}]’ <a href="http://master:4141/">http://master:4141/</a></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume kafka channel</title>
      <link href="/posts/fe58.html"/>
      <url>/posts/fe58.html</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-channel"><a href="#kafka-channel" class="headerlink" title="kafka channel"></a>kafka channel</h2><p>需要指定的</p><blockquote><p>type &#x3D; org.apache.flume.channel.kafka.KafkaChannel</p><p>kafka.bootstrap.server &#x3D; hostname:port</p><p>kafka.topic &#x3D; hkjcpdd</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; org.apache.flume.channel.kafka.KafkaChannel <br>a1.channels.c1.kafka.bootstrap.servers &#x3D; master:9092</p><p>a1.channels.c1.kafka.topic&#x3D;hkjmjj</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume kafka sink</title>
      <link href="/posts/fe57.html"/>
      <url>/posts/fe57.html</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-sink"><a href="#kafka-sink" class="headerlink" title="kafka sink"></a>kafka sink</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; org.apache.flume.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.server &#x3D; master<br>a1.sinks.k1.kafka.topic &#x3D; hkjcpdd&#x3D;&#x3D;</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>flume监听nginx的access.log文件给kafka消费</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; taildir<br>a1.sources.r1.filegroups &#x3D; f1<br>a1.sources.r1.filegroups.f1 &#x3D; &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;access.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.servers &#x3D; master:9092<br>a1.sinks.k1.kafka.topic&#x3D;hkjcpdd</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Sink Processors sink处理器</title>
      <link href="/posts/fe66.html"/>
      <url>/posts/fe66.html</url>
      
        <content type="html"><![CDATA[<h2 id="Sink-Processors-sink处理器"><a href="#Sink-Processors-sink处理器" class="headerlink" title="Sink Processors sink处理器"></a>Sink Processors sink处理器</h2><h3 id="failover-sink-Processor故障转移处理器"><a href="#failover-sink-Processor故障转移处理器" class="headerlink" title="failover sink Processor故障转移处理器"></a><em><strong>failover sink Processor故障转移处</strong>理器</em></h3><h4 id="amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"><a href="#amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力" class="headerlink" title="&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"></a>&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力</h4><p>&amp;#x9;2)需要指定: processor.type  &#x3D; failover</p><p>&amp;#x9;processor.priority.&lt;sinkName&gt; &#x3D; 数字（就是优先级，数字越大优先级越高）</p><p>&amp;#x9;processor.maxpenalty &#x3D; 10000（就是允许你宕机以后给你重连的时间）</p><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>负载平衡处理器提供了在多个sink负载平衡流量的能力。支持两种模式：round<em>robin and random。round</em>_robin可以将数据负载均衡到多个sink上，random支持随机分发到不同的sink上</p><p>第一种就是随机给你发送（random随机）</p><p>第二种就是负载均衡（robin负载均衡）</p><blockquote><p>a1.sources&#x3D;r1 <br>a1.sinks&#x3D;k1 k2 <br>a1.channels&#x3D;c1 <br><br>a1.sources.r1.type&#x3D;netcat <br>a1.sources.r1.bind&#x3D;worker-1 <br>a1.sources.r1.port&#x3D;44444 <br><br>a1.channels.c1.type&#x3D;memory <br>a1.channels.c1.capacity&#x3D;100000 <br>a1.channels.c1.transactionCapacity&#x3D;100 <br><br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-1<br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-2 <br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinkgroups &#x3D; g1 <br>a1.sinkgroups.g1.sinks &#x3D; k1 k2 <br>a1.sinkgroups.g1.processor.type &#x3D; failover(故障转移)   or    load_<em>balance(随机)<br>a1.sinkgroups.g1.processor.selector &#x3D; random or    round</em>_<em>robin# 默认是故障转移的不写就是故障转移，写了就是随机random是随机的意思，而round</em>_robin是轮询的意思<br><br>a1.sources.r1.channels&#x3D;c1 <br>a1.sinks.k1.channel&#x3D;c1 <br>a1.sinks.k2.channel&#x3D;c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume source的type</title>
      <link href="/posts/fe56.html"/>
      <url>/posts/fe56.html</url>
      
        <content type="html"><![CDATA[<p>exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。</p><p>spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文件，并在新文件处显示从新文件中解析出来。 与exec source不同，spooling directory source是可靠的， 即使flume重新启动或被kill，也不会丢失数据，同时作为这种可靠的代价，指定目录中的被手机的文件必须是不可变的、唯一命名的。flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常；</p><p>taildir source 监控指定的一些文件，并在检测新的一行数据残生的时候几乎实时的读取他们，如果新的一行数据还没写完，taildir source 等到这行写完后读取</p><p>kafka source 就是一个apache kafka消费者， 他从kafka的topic中读取消息，如果运行了多个Kafka source 则可以把他们配置到同一个消费者组，以便每个source都读取一组唯一的topic分区</p><p>syslog sources 针对系统日志</p><p>http sources 发送get协议请求的</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume</title>
      <link href="/posts/fe69.html"/>
      <url>/posts/fe69.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume的netcat 监听端口</title>
      <link href="/posts/fe61.html"/>
      <url>/posts/fe61.html</url>
      
        <content type="html"><![CDATA[<h1 id="netcat-agent"><a href="#netcat-agent" class="headerlink" title="netcat agent"></a>netcat agent</h1><p>配置文件如下</p><blockquote><p>a1.sources &#x3D; r1&amp;#x20;</p><p>a1.channels &#x3D; c1&amp;#x20;</p><p>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat&amp;#x20;</p><p>=&#x3D;a1.sources.r1.bind &#x3D; localhost &#x3D;&#x3D;</p><p>=&#x3D;a1.sources.r1.port &#x3D; 44444&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory&amp;#x20;</p><p>a1.channels.c1.capacity &#x3D; 10000&amp;#x20;</p><p>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试方法：telnet localhost 44444</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Channel Selectors通道选择器</title>
      <link href="/posts/fe69.html"/>
      <url>/posts/fe69.html</url>
      
        <content type="html"><![CDATA[<h3 id="Channel-Selectors通道选择器"><a href="#Channel-Selectors通道选择器" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><h3 id="Channel-Selectors通道选择器-1"><a href="#Channel-Selectors通道选择器-1" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><p>多路复用通道选择器，source是通过event header来决定传输到哪一个channel。source是通过event header来决定传输到哪一个channel</p><p><img src="https://pic1.imgdb.cn/item/6784c37dd0e0a243d4f3d664.png"></p><p><strong>replicating type是复制选择器</strong></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Interceptor</title>
      <link href="/posts/fe68.html"/>
      <url>/posts/fe68.html</url>
      
        <content type="html"><![CDATA[<h4 id="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"><a href="#就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理" class="headerlink" title="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"></a>就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理</h4><p>timestamp Interceptor给event的头信息中添加时间戳</p><p>Static Interceptor 给event的头信息中添加自定义键值</p><p>Host Interceptor给event的头信息中添加主机名或者ip信息</p><p>Search and Replace Interceptor拦截信息进行匹配和替换</p><p>Regex File</p><h2 id="timestamp-interceptor-添加时间戳"><a href="#timestamp-interceptor-添加时间戳" class="headerlink" title="timestamp interceptor(添加时间戳)"></a>timestamp interceptor(添加时间戳)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; timestamp&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="host-interceptor（添加主机信息）"><a href="#host-interceptor（添加主机信息）" class="headerlink" title="host interceptor（添加主机信息）"></a>host interceptor（添加主机信息）</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1 &#x3D;&#x3D;<br>=&#x3D;a1.sources.r1.interceptors.i1.type &#x3D; host&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Static-Interceptor"><a href="#Static-Interceptor" class="headerlink" title="Static Interceptor"></a>Static Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.sources.r1.interceptors  &#x3D; i1 i2 i3</p><p>a1.sources.r1.interceptors.i3.type &#x3D; static<br>a1.sources.r1.interceptors.i3.key &#x3D; name<br>a1.sources.r1.interceptors.i3.value &#x3D; zhangsan</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Search-and-Replace-Interceptor"><a href="#Search-and-Replace-Interceptor" class="headerlink" title="Search and Replace Interceptor"></a>Search and Replace Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; search_replace<br>a1.sources.r1.interceptors.i1.searchPattern&#x3D;[a-z]<br>a1.sources.r1.interceptors.i1.replaceString&#x3D;*&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h2 id="Regex-Filering-Interceptor-过滤的用正则"><a href="#Regex-Filering-Interceptor-过滤的用正则" class="headerlink" title="Regex Filering Interceptor(过滤的用正则)"></a>Regex Filering Interceptor(过滤的用正则)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_filter<br>a1.sources.r1.interceptors.i1.regex&#x3D;^jp.*<br>a1.sources.r1.interceptors.i1.excludeEvents&#x3D;true&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Regex-Extractor-Interceptor-通过正则对event进行捕获"><a href="#Regex-Extractor-Interceptor-通过正则对event进行捕获" class="headerlink" title="Regex Extractor Interceptor(通过正则对event进行捕获)"></a>Regex Extractor Interceptor(通过正则对event进行捕获)</h3><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_extractor<br>a1.sources.r1.interceptors.i1.regex&#x3D;(^[a-zA-Z]&#x3D;&#x3D;<em>&#x3D;&#x3D;)\s([0-9]&#x3D;&#x3D;</em>&#x3D;&#x3D;$)<br>a1.sources.r1.interceptors.i1.serializers&#x3D;s1 s2<br>a1.sources.r1.interceptors.i1.serializers.s1.name&#x3D;word<br>a1.sources.r1.interceptors.i1.serializers.s2.name&#x3D;num&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume taildir agent监听一个或者多个文件</title>
      <link href="/posts/fe54.html"/>
      <url>/posts/fe54.html</url>
      
        <content type="html"><![CDATA[<h2 id="taildir-agent"><a href="#taildir-agent" class="headerlink" title="taildir agent"></a>taildir agent</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; taildir<br># 这个是用于断点续传，确保不被重复消费<br>a1.sources.r1.positionFile&#x3D;&#x2F;data&#x2F;flume&#x2F;position.json<br>a1.sources.r1.filegroups &#x3D; f1 a1.sources.r1.filegroups.f1 &#x3D; &#x2F;root&#x2F;taildir&#x2F;hkjcpdd.log&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>与之exev不同的是，他是监听的是文件内容，而exec是监听命令运行后的结果</p><p>监听多个文件的话那就多写几个</p><p>例如:</p><blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume</title>
      <link href="/posts/fe52.html"/>
      <url>/posts/fe52.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784c673d0e0a243d4f3d73f.png"></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：将nginx的日志实时传输到hdfs和kafka上</title>
      <link href="/posts/fe66.html"/>
      <url>/posts/fe66.html</url>
      
        <content type="html"><![CDATA[<p>案例：将nginx的日志实时传输到hdfs和kafka上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type=taildir</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1=/usr/local/nginx/logs/access.log</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line">a1.channels.c1.capacity=10000</span><br><span class="line">a1.channels.c1.transactionCapacity=100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type=memory</span><br><span class="line">a1.channels.c2.capacity=10000</span><br><span class="line">a1.channels.c2.transactionCapacity=100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=/data/hkjcpdd/%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=true</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval=10</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k2.kafka.bootstrap.servers = master:9092</span><br><span class="line">a1.sinks.k2.kafka.topic = hkjcpdd</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AzkabanAzkaban搭建</title>
      <link href="/posts/968.html"/>
      <url>/posts/968.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D-1024M并重启mysql服务"><a href="#1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D-1024M并重启mysql服务" class="headerlink" title="1.在搭建之前得有mysql环境,以及my.cnf文件中添加max_allowed_packet&#x3D;1024M并重启mysql服务"></a>1.在搭建之前得有mysql环境,以及my.cnf文件中添加max_allowed_packet&#x3D;1024M并重启mysql服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置mysql相关部分(db)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入Mysql然后创建azkaban库</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入库并<span class="built_in">source</span>进来azkaban-db/create-all-sql-3.84.4.sql</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改以下配置项(<span class="built_in">exec</span>)</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">executor.port=12321</span><br><span class="line">jetty.port=8061</span><br><span class="line">azkaban.webserver.url=http://master:8061</span><br><span class="line"></span><br><span class="line">mysql.host=master</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进行分发到其他机器，然后每台都要启动</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">最后进行激活</span></span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;master:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;slave1:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;slave2:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改一下配置项(web)</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">jetty.port=8061</span><br><span class="line"></span><br><span class="line">mysql.host=master</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置azkaban-users.xml</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加一行即可</span></span><br><span class="line">&lt;user password=&quot;123456&quot; roles=&quot;admin&quot; username=&quot;hkjcpdd&quot;/&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后启动即可</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban</title>
      <link href="/posts/969.html"/>
      <url>/posts/969.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> demo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的JavaProcess作业案例</title>
      <link href="/posts/966.html"/>
      <url>/posts/966.html</url>
      
        <content type="html"><![CDATA[<h2 id="JavaProcess作业类型案例"><a href="#JavaProcess作业类型案例" class="headerlink" title="JavaProcess作业类型案例"></a>JavaProcess作业类型案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JavaProcess类型可以运行一个自定义主类方法，<span class="built_in">type</span>类型为javaprocess,可用的配置为：</span></span><br><span class="line"></span><br><span class="line">​Xms: 最小堆</span><br><span class="line">​Xmx: 最大堆</span><br><span class="line">​classpath: 类路径</span><br><span class="line">​java.class: 要运行的Java对象，其中必须包含Main方法</span><br><span class="line">​main.args: main方法的参数</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">案例：</span></span><br><span class="line"></span><br><span class="line">1)新建一个azkaban的maven工程</span><br><span class="line">2）创建包名：com.hkjcpdd</span><br><span class="line">3)创建AzTest类</span><br><span class="line">4)打包jar包azkaban.jar</span><br><span class="line">5)新建testJava.flow</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">five.flow</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: test_java</span><br><span class="line">    type: javaprocess</span><br><span class="line">    config:</span><br><span class="line">      Xms: 96M</span><br><span class="line">      Xmx: 200M</span><br><span class="line">      java.class: com.hkjcpdd.TestJavaProcess</span><br><span class="line">      </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后将flow和jar和project打包在一起上传即可</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的最简单案例</title>
      <link href="/posts/967.html"/>
      <url>/posts/967.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># first.project</span><br><span class="line">azkaban-flow-version: <span class="number">2</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># first.flow</span><br><span class="line"><span class="function">nodes:</span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobA</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">hkjcpdd</span>&quot;</span></span><br></pre></td></tr></table></figure><p>然后将这两个打包成zip然后上传至webui界面提交即可</p>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban作业依赖</title>
      <link href="/posts/965.html"/>
      <url>/posts/965.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># second.flow</span><br><span class="line"><span class="function">nodes:</span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobA</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">aaa</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobB</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">bbb</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobC</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">dependsOn</span>:</span></span><br><span class="line"><span class="function">        - <span class="title">jobA</span></span></span><br><span class="line"><span class="function">        - <span class="title">jobB</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">ccc</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function"> # 添加<span class="title">dependsOn</span>就可以了，这个用处就是等<span class="title">AB</span>完成后再执行<span class="title">C</span></span></span><br><span class="line"><span class="function"> # 然后将<span class="title">second.fow</span>和<span class="title">first.project</span>打包<span class="title">zip</span>上传至<span class="title">webui</span>即可</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban失败重试</title>
      <link href="/posts/958.html"/>
      <url>/posts/958.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">自动失败重试</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">  type: command</span><br><span class="line">  config:</span><br><span class="line">    command: sh /not exists.sh</span><br><span class="line">    retries: 3</span><br><span class="line">    retry.backoff: 10000</span><br><span class="line">    </span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">参数说明：</span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">retries: 重试次数</span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">retry.backoff: 重试的时间间隔</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">手动失败重试</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">理想的就是从成功的地方跳过，失败的地方重试</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">需求：JobA =&gt; JobB(依赖于A) =&gt; JobC =&gt; JobD =&gt; JobE =&gt; JobF。生产环境中，任何Job都有可能挂掉，可以根据需求执行想要的Job</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">直接从webui操作</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.<span class="built_in">history</span>-&gt;flow-&gt;Prepare execution-&gt;execute</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2.就是从头开始，但是把成功的<span class="built_in">disable</span>就可以了</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban定时执行案例</title>
      <link href="/posts/959.html"/>
      <url>/posts/959.html</url>
      
        <content type="html"><![CDATA[<h2 id="定时执行案例"><a href="#定时执行案例" class="headerlink" title="定时执行案例"></a>定时执行案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">需求：JobA每间隔一分钟执行一次</span></span><br><span class="line">就是执行任务的那个绿色按钮</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban条件工作流案例</title>
      <link href="/posts/962.html"/>
      <url>/posts/962.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.运行时参数案例</span></span><br><span class="line">1) 基本原理</span><br><span class="line">  （1）父Job将参数写入JOB_OUTPUT_PROP_FILE环境变量所指向的文件</span><br><span class="line">  （2）子Job使用$&#123;jobName:param&#125;来获取父Job输出的参数并定义执行条件</span><br><span class="line">2）支持的条件运算符</span><br></pre></td></tr></table></figure><p><img src="https://pic1.imgdb.cn/item/6775fc45d0e0a243d4ed9ee4.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">案例需求：</span></span><br><span class="line">jobA执行一个shell脚本</span><br><span class="line">jobB执行一个shell脚本，但jobBJ不需要每天都执行，而只需要每个周一执行</span><br><span class="line"></span><br><span class="line">(1)新建jobA.sh</span><br><span class="line">(2)新建jobB.sh</span><br><span class="line">(3)condition.flow</span><br><span class="line">nodes:</span><br><span class="line">  - name: JobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh JobA.sh  </span><br><span class="line">  - name: JobB</span><br><span class="line">    type: command</span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobA</span><br><span class="line">    config:</span><br><span class="line">      command: sh JobB.sh</span><br><span class="line">    condition: $&#123;JobA:wk&#125;==1</span><br><span class="line">(4)将JobA.sh、JobB.sh、condition.flow和azkaban.project打包成condition.zip</span><br><span class="line">(5)创建condition项目=&gt;上传condition.zip文件=&gt;执行作业=&gt;观察结果</span><br><span class="line">(6)按照我们设定的条件，JobB会根据当日日期决定是否执行</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">预定义宏案例</span></span><br><span class="line"></span><br><span class="line">Azkaban中预置了几个特殊的判断条件，称为预定义宏</span><br><span class="line">预定义宏会根据所有父Job的完成情况进行判断，再决定是否执行。可用的预定义宏如下：</span><br><span class="line"></span><br><span class="line">- (1)all_success:表示父Job全部成功才执行（默认）</span><br><span class="line">- (2)all_done: 表示父Job全部完成才执行</span><br><span class="line">- (3)all_failed:表示父Job全部失败才执行</span><br><span class="line">- (4)one_success:表示父Job至少一个成功才执行</span><br><span class="line">- (5)one_failed: 表示父Job至少一个失败才执行</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">案例 需求：</span></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">JobA执行一个shell脚本</span><br><span class="line">JobB执行一个shell脚本</span><br><span class="line">JobC执行一个shell脚本</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">1.新建JobA.sh</span><br><span class="line">2.新建JobC.sh</span><br><span class="line">3.新建marco.flow</span><br><span class="line">nodes:</span><br><span class="line">  - name: JobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh aaa.sh</span><br><span class="line">  - name: JobB</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh jobB.sh</span><br><span class="line">  - name: JobC</span><br><span class="line">    type: command</span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobA</span><br><span class="line">      - JobB</span><br><span class="line">    config:</span><br><span class="line">      command: sh bbb.sh</span><br><span class="line">    condition: one_success</span><br><span class="line">4.打包上传，可以看到只要一个成功就可以执行jobC</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban邮件报警案例</title>
      <link href="/posts/964.html"/>
      <url>/posts/964.html</url>
      
        <content type="html"><![CDATA[<h2 id="邮件报警案例"><a href="#邮件报警案例" class="headerlink" title="邮件报警案例"></a>邮件报警案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">得先从邮箱那里开启STMP的服务，拿到授权码</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进入<span class="variable">$azkaban</span>-web/conf/azkaban.properties</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改22和23行</span></span><br><span class="line">mail.sender=邮箱地址</span><br><span class="line">mail.host=smtp.qq.com</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加上</span></span><br><span class="line">mail.user=邮箱地址</span><br><span class="line">mail.password=授权码</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新启动azkaban-web</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动任务的地方左侧有个Notification（通知）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后将邮箱地址放进那两个框，一个是失败发送给谁，一个是成功发送给谁，可以指定多个</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-匿名、柯里化、闭包</title>
      <link href="/posts/b9m1.html"/>
      <url>/posts/b9m1.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-匿名函数"><a href="#1-匿名函数" class="headerlink" title="1.匿名函数"></a>1.匿名函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 作为值的函数</span></span><br><span class="line"><span class="comment">// 定义列表，记录1-10之间的数据</span></span><br><span class="line"><span class="keyword">val</span> list1 = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="comment">// 创建函数对象，用来将int -&gt; String</span></span><br><span class="line"><span class="keyword">val</span> func = (x: <span class="type">Int</span>) =&gt; <span class="string">&quot;*&quot;</span> * x</span><br><span class="line"><span class="comment">//调用map方法，将第一步的列表转换成目标列表</span></span><br><span class="line"><span class="keyword">val</span> list2 = list1.map(func)</span><br><span class="line"><span class="comment">// 打印</span></span><br><span class="line">println(list2)</span><br><span class="line"><span class="comment">// 匿名函数</span></span><br><span class="line"><span class="keyword">val</span> list3 = list1.map((x: <span class="type">Int</span>) =&gt; <span class="string">&quot;*&quot;</span> * x)</span><br><span class="line"><span class="keyword">val</span> list3 = list1.map(_ =&gt; <span class="string">&quot;*&quot;</span> * _)</span><br><span class="line">println(list3)</span><br></pre></td></tr></table></figure><h2 id="2-柯里化"><a href="#2-柯里化" class="headerlink" title="2.柯里化"></a>2.柯里化</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需求，定义方法，完成两个字符串的拼接</span></span><br><span class="line"><span class="comment">// 方式1：普通方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergel</span></span>(s1: <span class="type">String</span>, s2: <span class="type">String</span>) = s1.toUpperCase + s2.toUpperCase</span><br><span class="line"><span class="comment">// 方式2：柯里化写法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge2</span></span>(s1: <span class="type">String</span>, s2: <span class="type">String</span>)(f1: (<span class="type">String</span>, <span class="type">String</span>) =&gt; <span class="type">String</span>) = f1(s1, s2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 柯里化</span></span><br><span class="line"><span class="comment">// 方式一：普通方法</span></span><br><span class="line"><span class="keyword">val</span> str1 = mergel(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;cpdd&quot;</span>)</span><br><span class="line">println(str1)</span><br><span class="line"><span class="comment">// 方式2：柯里化方法</span></span><br><span class="line"><span class="keyword">val</span> str2 = merge2(<span class="string">&quot;hkj&quot;</span>,<span class="string">&quot;cpdd&quot;</span>)(_.toUpperCase + _.toUpperCase)</span><br></pre></td></tr></table></figure><h2 id="3-闭包"><a href="#3-闭包" class="headerlink" title="3.闭包"></a>3.闭包</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 闭包</span></span><br><span class="line"><span class="comment">// 需求： 定义一个函数，用来获取两个整数的和，通过闭包的形式实现</span></span><br><span class="line"><span class="comment">// 闭包指的是可以访问不在当前作用于范围数据的一个函数</span></span><br><span class="line"><span class="comment">// 定义变量x, 初始值为10</span></span><br><span class="line"><span class="keyword">val</span> x = <span class="number">10</span></span><br><span class="line"><span class="comment">// 定义函数getSum(), 用来获取两个整数的和</span></span><br><span class="line"><span class="keyword">val</span> getSum = (y: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">  x + y</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用getSum(), 并打印</span></span><br><span class="line">println(getSum(<span class="number">20</span>))</span><br></pre></td></tr></table></figure><h2 id="4-控制抽象"><a href="#4-控制抽象" class="headerlink" title="4.控制抽象"></a>4.控制抽象</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 控制抽象</span></span><br><span class="line"><span class="comment">// 控制抽象也是函数的一种，它可以让我们更加灵活的使用函数，假设函数a的参数列表需要接受一个函数b，且函数b没有输入值也没有返回值，那门函数a就被称之为控制抽象函数</span></span><br><span class="line"><span class="comment">// 定义一个函数myShop, 该函数接受一个无参数无返回值的函数</span></span><br><span class="line"><span class="keyword">val</span> myShop = (f1: () =&gt; <span class="type">Unit</span>) =&gt; &#123;</span><br><span class="line">  println(<span class="string">&quot;hkjcpdd&quot;</span>)</span><br><span class="line">  <span class="comment">// 在myShop函数中调用f1函数</span></span><br><span class="line">  f1() <span class="comment">//表示顾客具体购买的商品</span></span><br><span class="line">  println(<span class="string">&quot;hkjmjj&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用myShop函数</span></span><br><span class="line">myShop&#123;</span><br><span class="line">  () =&gt; &#123;</span><br><span class="line">    println(<span class="string">&quot;我要买hkj&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;我要买hkjcpdd&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;我要买hkjmjj&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;我要买hkjyjj&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-案例：计算器"><a href="#5-案例：计算器" class="headerlink" title="5.案例：计算器"></a>5.案例：计算器</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个方法，用来完成两个int类型数字的计算（加减乘除）</span></span><br><span class="line"><span class="comment">// 方式一：普通写法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a + b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a -b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a * b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a / b</span><br><span class="line"><span class="comment">// 方式二：通过柯里化方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>)(func: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>) = func(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 案例：计算器</span></span><br><span class="line">println(add(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">println(subtract(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">println(multiply(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">println(divide(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment">// 柯里化计算器</span></span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ + _))</span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ - _))</span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ * _))</span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ / _))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-泛型</title>
      <link href="/posts/b0a7.html"/>
      <url>/posts/b0a7.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-泛型方法"><a href="#1-泛型方法" class="headerlink" title="1.泛型方法"></a>1.泛型方法</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛型-意思是指某种具体的数据类型</span></span><br><span class="line"><span class="comment">// 方式1：不采用泛型，即：普通的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMiddleElement</span></span>(arr: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Int</span> = arr(arr.length / <span class="number">2</span>)</span><br><span class="line"><span class="comment">// 方式二：采用自定义的泛型方法来实现</span></span><br><span class="line"><span class="comment">// T就是type单词的缩写</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMiddleElement1</span></span>[<span class="type">T</span>](arr: <span class="type">Array</span>[<span class="type">T</span>]) = arr(arr.length / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 泛型-意思是指某种具体的数据类型</span></span><br><span class="line"><span class="comment">// 测试getMiddleElement（）方法</span></span><br><span class="line">println(getMiddleElement(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)))</span><br><span class="line">println(getMiddleElement(<span class="type">Array</span>(<span class="string">&quot;hkjcpdd&quot;</span>, <span class="string">&quot;hkjmjj&quot;</span>, <span class="string">&quot;hkjljj&quot;</span>)))</span><br><span class="line">println(getMiddleElement1(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)))</span><br><span class="line">println(getMiddleElement1(<span class="type">Array</span>(<span class="string">&quot;hkjcpdd&quot;</span>, <span class="string">&quot;hkjmjj&quot;</span>, <span class="string">&quot;hkjljj&quot;</span>)))</span><br></pre></td></tr></table></figure><h2 id="2-泛型类"><a href="#2-泛型类" class="headerlink" title="2.泛型类"></a>2.泛型类</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛型类</span></span><br><span class="line"><span class="comment">// 定义一个Pair泛型类，该类包含两个字段，且两个字段的类型不固定</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val message: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  println(message)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 泛型类</span></span><br><span class="line"><span class="comment">// 创建不同类型的Pair泛型类对象，并打印</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">Pair</span>[<span class="type">Int</span>](<span class="number">1234</span>)</span><br><span class="line"><span class="keyword">new</span> <span class="type">Pair</span>[<span class="type">String</span>](<span class="string">&quot;hkjcpdd&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="3-泛型特质"><a href="#3-泛型特质" class="headerlink" title="3.泛型特质"></a>3.泛型特质</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛型特质</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Logger</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> a: <span class="type">T</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">show</span></span>() = println(a)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ConsoleLogger</span> <span class="keyword">extends</span> <span class="title">Logger</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> a: <span class="type">String</span> = <span class="string">&quot;hkjcpdd&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 泛型特质</span></span><br><span class="line"><span class="type">ConsoleLogger</span>.show()</span><br></pre></td></tr></table></figure><h2 id="4-泛型上下界之上界"><a href="#4-泛型上下界之上界" class="headerlink" title="4.泛型上下界之上界"></a>4.泛型上下界之上界</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">使用t &lt; 类型名表示给类型添加一个上界，表示泛型参数必须从该列（或本身）继承.</span><br><span class="line">格式：</span><br><span class="line">[<span class="type">T</span> &lt;: 类型]</span><br><span class="line">例如：[<span class="type">T</span> &lt;: <span class="type">Person</span>]的意思是，泛型<span class="type">T</span>的数据类型必须是<span class="type">Person</span>类型或者<span class="type">Person</span>的子类型</span><br><span class="line"></span><br><span class="line"><span class="comment">// 上界上界是包子不包父</span></span><br><span class="line"><span class="comment">// 1.定义一个Person类</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name: <span class="type">String</span></span><br><span class="line">  <span class="keyword">val</span> age: <span class="type">Int</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 2. 定义一个Student类，继承Person类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> name: <span class="type">String</span> = <span class="string">&quot;hkjcpdd&quot;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> age: <span class="type">Int</span> = <span class="number">12</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 3. 定义一个泛型方法demo(),该方法接受一个Array参数</span></span><br><span class="line"><span class="comment">// 4.限定Demo方法的Array元素类型只能是Person或者Person的子类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span></span>[<span class="type">T</span> &lt;: <span class="type">Person</span>](array: <span class="type">Array</span>[<span class="type">T</span>]): <span class="type">Unit</span> = println(array)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 上界</span></span><br><span class="line"><span class="keyword">val</span> s1 = <span class="keyword">new</span> <span class="type">Student</span>()</span><br><span class="line">demo(<span class="type">Array</span>(s1))</span><br></pre></td></tr></table></figure><h2 id="5-泛型上下界之下界"><a href="#5-泛型上下界之下界" class="headerlink" title="5.泛型上下界之下界"></a>5.泛型上下界之下界</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">使用<span class="type">T</span> &gt;: 数据类型 表示给类型添加一个下界，表示泛型参数必须是从类型本身或该类型的父类型.</span><br><span class="line">格式：</span><br><span class="line">[<span class="type">T</span> &gt;: 类型]</span><br><span class="line">注意：</span><br><span class="line"><span class="number">1.</span>例如：[<span class="type">T</span> &gt;: <span class="type">Person</span>]的意思是，泛型<span class="type">T</span>的数据烈性必须是<span class="type">Person</span>类型或者<span class="type">Person</span>的父类型</span><br><span class="line"><span class="number">2.</span>如果泛型既有上界又有下界。下界写在前面，上界写在后面，即：[<span class="type">T</span> &gt;:类型<span class="number">1</span> &lt; 类型<span class="number">2</span>]</span><br><span class="line"><span class="comment">// 下界</span></span><br><span class="line"><span class="comment">// 1.定义一个Person类</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person1</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name: <span class="type">String</span></span><br><span class="line">  <span class="keyword">val</span> age: <span class="type">Int</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 2. 定义一个Policeman类，继承Person类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policeman</span> <span class="keyword">extends</span> <span class="title">Person1</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> name: <span class="type">String</span> = <span class="string">&quot;hkjmjj&quot;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> age: <span class="type">Int</span> = <span class="number">14</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 3.定义一个Superman类，继承Policeman类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Superman</span> <span class="keyword">extends</span> <span class="title">Policeman</span></span></span><br><span class="line"><span class="comment">// 4.定义一个demo泛型方法，该方法接受一个Array参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo1</span></span>[<span class="type">T</span> &gt;: <span class="type">Policeman</span>](arr: <span class="type">Array</span>[<span class="type">T</span>]) = println(arr)</span><br><span class="line"><span class="comment">// 5.测试调用demo,传入不同元素类型的Array</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 下界</span></span><br><span class="line">demo1(<span class="type">Array</span>(<span class="keyword">new</span> <span class="type">Policeman</span>))</span><br><span class="line"><span class="comment">// 以下就不可以了，因为设置了下界，下界包父不包子</span></span><br><span class="line"><span class="comment">// demo1(Array(new Superman))</span></span><br></pre></td></tr></table></figure><h2 id="6-泛型注意点"><a href="#6-泛型注意点" class="headerlink" title="6.泛型注意点"></a>6.泛型注意点</h2><blockquote><p>[!NOTE]</p><p>如果泛型既有上界又有下界。下界写在前面，上界写在后面，即：[T &gt;:类型1 &lt; 类型2]</p><p>如果是同一个同事设置了上下界，其实就是直接固定类型，和直接在里面写类型没区别</p></blockquote><h2 id="7-协变、逆变、非变"><a href="#7-协变、逆变、非变" class="headerlink" title="7.协变、逆变、非变"></a>7.协变、逆变、非变</h2><blockquote><p>[!IMPORTANT]</p><ul><li>非变：类A和类B之间是父子类关系，但是Pair[A]和Pair[B]之间没有任何关系.</li><li>协变：类A和类B之间是父子类关系，Pair[A]和Pair[B]之间也有父子类关系.</li><li>逆变：类A和类B之间是父子类关系，但是Pair[A]和Pair[B]之间是子父关系.</li></ul></blockquote><p><img src="https://pic.imgdb.cn/item/674582b8d0e0a243d4d12c3e.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.定义一个Super类、以及一个Sub类继承自Super类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Super</span> <span class="comment">// 父类</span></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sub</span> <span class="keyword">extends</span> <span class="title">Super</span></span></span><br><span class="line"><span class="comment">// 2.使用协变、逆变、非变分别定义三个泛型类</span></span><br><span class="line"><span class="comment">// 非变</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Temp1</span>[<span class="type">T</span>]</span></span><br><span class="line"><span class="comment">// 协变</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Temp2</span>[+<span class="type">T</span>]</span></span><br><span class="line"><span class="comment">// 逆变</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Temp3</span>[-<span class="type">T</span>]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 分别创建泛型类对象来掩饰协、逆变、非变</span></span><br><span class="line"><span class="comment">// 3.1测试非变</span></span><br><span class="line"><span class="keyword">val</span> t1: <span class="type">Temp1</span>[<span class="type">Sub</span>] = <span class="keyword">new</span> <span class="type">Temp1</span>[<span class="type">Sub</span>]</span><br><span class="line">  <span class="keyword">val</span> t2: <span class="type">Temp1</span>[<span class="type">Super</span>] = t1 <span class="comment">// 编译报错，因为是非变，super和Sub有父子类关系，但是Temp1[Super]和Temp1[Sub]无关系</span></span><br><span class="line"><span class="comment">// 3.2测试协变</span></span><br><span class="line"><span class="keyword">val</span> t3:<span class="type">Temp2</span>[<span class="type">Sub</span>] = <span class="keyword">new</span> <span class="type">Temp2</span>[<span class="type">Sub</span>]</span><br><span class="line"><span class="keyword">val</span> t4:<span class="type">Temp2</span>[<span class="type">Super</span>] = t3  <span class="comment">// 协变，super和Sub有父子类关系，但是Temp2[Super]和Temp2[Sub]有父子类关系</span></span><br><span class="line"><span class="comment">// 3.3测试逆变</span></span><br><span class="line">  <span class="keyword">val</span> t5: <span class="type">Temp3</span>[<span class="type">Sub</span>] = <span class="keyword">new</span> <span class="type">Temp3</span>[<span class="type">Sub</span>]</span><br><span class="line">  <span class="keyword">val</span> t6: <span class="type">Temp3</span>[<span class="type">Super</span>] = t5 <span class="comment">// 编译报错，逆变是,super和Sub有父子类关系，但是Temp3[Super]和Temp3[Sub]变成子父类关系</span></span><br><span class="line"><span class="keyword">val</span> t7: <span class="type">Temp3</span>[<span class="type">Super</span>] = <span class="keyword">new</span> <span class="type">Temp3</span>[<span class="type">Super</span>]</span><br><span class="line"><span class="keyword">val</span> t8: <span class="type">Temp3</span>[<span class="type">Sub</span>] = t7 <span class="comment">// 逆变，不会报错，子父类</span></span><br></pre></td></tr></table></figure><h2 id="8-案例：列表去重排序"><a href="#8-案例：列表去重排序" class="headerlink" title="8.案例：列表去重排序"></a>8.案例：列表去重排序</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例：列表去重排序</span></span><br><span class="line"><span class="comment">// 一直当前项目下的data文件夹有一个1.txt文本文件</span></span><br><span class="line"><span class="comment">// 对上述数据去重排序后，重新写入到data文件夹下的2.txt文本文件中</span></span><br><span class="line"><span class="comment">// 1.定义数据源对象，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> source = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\123.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 2.从指定的文件中读取所有的对象</span></span><br><span class="line"><span class="keyword">val</span> list1 = source.mkString.split(<span class="string">&quot;\\s+&quot;</span>).toList</span><br><span class="line"><span class="comment">// 所有的数据 -&gt; 按照空白字符切割，Array(&quot;&quot;, &quot;&quot;...)</span></span><br><span class="line"><span class="comment">// println(list1)</span></span><br><span class="line"><span class="comment">// 3. 把List[String] -&gt; List[Int]</span></span><br><span class="line"><span class="keyword">val</span> list2: <span class="type">List</span>[<span class="type">Int</span>] = list1.map(_.toInt)</span><br><span class="line"><span class="comment">// 4. 把List[Int] -&gt; Set[Int], 对列表元素去重</span></span><br><span class="line"><span class="keyword">val</span> set1: <span class="type">Set</span>[<span class="type">Int</span>] = list2.toSet</span><br><span class="line"><span class="comment">// 5. Set[Int] -&gt; List[Int], 然后升序排列.</span></span><br><span class="line"><span class="keyword">val</span> list3: <span class="type">List</span>[<span class="type">Int</span>] = set1.toList.sorted</span><br><span class="line"><span class="comment">// 6. 把所有的数据写入到指定的目的地文件中</span></span><br><span class="line"><span class="comment">// 6.1 创建字符缓冲流，用来写入数据到自定的目的地文件中.</span></span><br><span class="line"><span class="keyword">val</span> bw = <span class="keyword">new</span> <span class="type">BufferedWriter</span>(<span class="keyword">new</span> <span class="type">FileWriter</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\123456.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 6.2 遍历list3列表，获取每一个数字</span></span><br><span class="line"><span class="keyword">for</span> (i &lt;- list3) &#123;</span><br><span class="line">  <span class="comment">// i 就表示列表中的每一个</span></span><br><span class="line">  <span class="comment">// 6.3 将获取到的数字转换成字符串在写入</span></span><br><span class="line">  bw.write(i.toString)</span><br><span class="line">  <span class="comment">// 6.4 记得加换行</span></span><br><span class="line">  bw.newLine()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 7. 释放资源.</span></span><br><span class="line">bw.close()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-隐式操作</title>
      <link href="/posts/n9a7.html"/>
      <url>/posts/n9a7.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-隐式转换-手动导入"><a href="#1-隐式转换-手动导入" class="headerlink" title="1.隐式转换-手动导入"></a>1.隐式转换-手动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 手动导入</span></span><br><span class="line"><span class="comment">// 定义 RichFile 类，用来丰富 File 类的功能</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile</span>(<span class="params">file: <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// 定义 read() 方法，用来讲数据读取到一个字符串中</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="type">Source</span>.fromFile(file).mkString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 定义单例对象 ImplicitDemo，该单例对象中有一个隐式转换方法</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImplicitDemo</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 隐式转换方法 file2RichFile，是用来将 File 对象转换成 RichFile 对象</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">file2RichFile</span></span>(file: <span class="type">File</span>): <span class="type">RichFile</span> = <span class="keyword">new</span> <span class="type">RichFile</span>(file)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> <span class="type">ImplicitDemo</span>.file2RichFile</span><br><span class="line"><span class="comment">// 创建普通的 File 对象，尝试调用其 read() 功能</span></span><br><span class="line"><span class="keyword">val</span> file = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line">  println(file.read())</span><br></pre></td></tr></table></figure><h2 id="2-隐式转换-自动导入"><a href="#2-隐式转换-自动导入" class="headerlink" title="2.隐式转换-自动导入"></a>2.隐式转换-自动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动导入隐式转换</span></span><br><span class="line"><span class="comment">// 定义一个RichFile类，里面定义一个read方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile1</span>(<span class="params">file: <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>() = <span class="type">Source</span>.fromFile(file).mkString</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自动导入隐式转换</span></span><br><span class="line"><span class="comment">// 需求：通过隐式转换，让file类的对象具有read功能</span></span><br><span class="line"><span class="comment">// 2.定义一个饮食转换方法，用来将普通的File对象-&gt;RichFile对象.</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">file2RichFile</span></span>(file: <span class="type">File</span>) = <span class="keyword">new</span> <span class="type">RichFile1</span>(file)</span><br><span class="line"><span class="comment">// 3. 创建file对象，尝试调用read（）方法.</span></span><br><span class="line"><span class="keyword">val</span> file1 = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="3-隐式参数-手动导入"><a href="#3-隐式参数-手动导入" class="headerlink" title="3.隐式参数-手动导入"></a>3.隐式参数-手动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 隐式参数: 如果方法的某个参数列表使用了implicit修饰，则该参数列表就是：隐式参数</span></span><br><span class="line"><span class="comment">// 手动导入</span></span><br><span class="line"><span class="comment">// 定义一个show 方法</span></span><br><span class="line">  delimit: (<span class="type">String</span>, <span class="type">String</span>)是个元组</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(name: <span class="type">String</span>)(<span class="keyword">implicit</span> delimit: (<span class="type">String</span>, <span class="type">String</span>)) = delimit._1 + name + delimit._2</span><br><span class="line"><span class="comment">// 定义一个单利对象，用来给隐式参数设置默认值</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImplicitParam</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> delimit_default = (<span class="string">&quot;&lt;&lt;&lt;&quot;</span>, <span class="string">&quot;&gt;&gt;&gt;&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 隐式参数</span></span><br><span class="line"><span class="comment">// 1.手动导入隐式参数</span></span><br><span class="line"><span class="keyword">import</span> <span class="type">ImplicitParam</span>.delimit_default</span><br><span class="line"><span class="comment">// println(show(&quot;hkj&quot;))</span></span><br><span class="line">println(show(<span class="string">&quot;hkj&quot;</span>)(<span class="string">&quot;((&quot;</span>, <span class="string">&quot;))&quot;</span>))</span><br></pre></td></tr></table></figure><h2 id="4-隐式参数-自动导入"><a href="#4-隐式参数-自动导入" class="headerlink" title="4.隐式参数-自动导入"></a>4.隐式参数-自动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动导入</span></span><br><span class="line"><span class="comment">// 定义show方法，接受一个姓名，再接受一个前缀和后缀</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show1</span></span>(name: <span class="type">String</span>)(<span class="keyword">implicit</span> delimit: (<span class="type">String</span>, <span class="type">String</span>)) = delimit._1 + name + delimit._2</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.自动导入隐式参数</span></span><br><span class="line"><span class="comment">// 通过隐式值，力给隐式参数设置初始值.</span></span><br><span class="line"><span class="comment">// 由程序自动导入的</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> delimit_default = <span class="string">&quot;&lt;&lt;&lt;&quot;</span> -&gt; <span class="string">&quot;&gt;&gt;&gt;&quot;</span></span><br><span class="line"><span class="comment">// 3. 调用show1（）方法，打印结果</span></span><br><span class="line">println(show1(<span class="string">&quot;hkj&quot;</span>))</span><br><span class="line">println(show1(<span class="string">&quot;hkj&quot;</span>)(<span class="string">&quot;&lt;&lt;&lt;&quot;</span> -&gt; <span class="string">&quot;&gt;&gt;&gt;&quot;</span>))</span><br></pre></td></tr></table></figure><h3 id="5-案例：获取列表元素平均值"><a href="#5-案例：获取列表元素平均值" class="headerlink" title="5.案例：获取列表元素平均值"></a>5.案例：获取列表元素平均值</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hkjcpdd.stu</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ClassDemo14_fin</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1.定义一个RichList类，用来给普通的List添加avg方法，用于获取列表元素的平均值</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">RichList</span>(<span class="params">list: <span class="type">List</span>[<span class="type">Int</span>]</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// 2.定义avg方法，用来获取List列表中的所有元素的平均值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">avg</span></span>() = &#123;</span><br><span class="line">      <span class="keyword">if</span> (list.isEmpty) <span class="type">None</span></span><br><span class="line">      <span class="keyword">else</span> <span class="type">Some</span>(list.sum / list.size)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 获取列表元素平均值</span></span><br><span class="line">    <span class="comment">// 3.定义隐式转换方法，用来将普通List独享转换为RichList对象</span></span><br><span class="line">    <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">list2RichList</span></span>(list:<span class="type">List</span>[<span class="type">Int</span>]) = <span class="keyword">new</span> <span class="type">RichList</span>(list)</span><br><span class="line">    <span class="comment">// 4. 定义List列表，获取其中所有元素的平均值</span></span><br><span class="line">    <span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">    println(list1.avg())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h2><h3 id="隐式操作主要理解为："><a href="#隐式操作主要理解为：" class="headerlink" title="隐式操作主要理解为："></a>隐式操作主要理解为：</h3><h3 id="1-将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）"><a href="#1-将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）" class="headerlink" title="1.将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）"></a>1.将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）</h3><h3 id="2-将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）"><a href="#2-将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）" class="headerlink" title="2.将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）"></a>2.将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）</h3>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-递归</title>
      <link href="/posts/p9a7.html"/>
      <url>/posts/p9a7.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-递归"><a href="#1-递归" class="headerlink" title="1.递归"></a>1.递归</h2><h3 id="递归简介"><a href="#递归简介" class="headerlink" title="递归简介"></a>递归简介</h3><blockquote><p>[!IMPORTANT]</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 递归操作-是指方法自己调用自己的情况</span></span><br><span class="line"><span class="comment">// 递归必须有出口，否则容易造成死递归</span></span><br><span class="line"><span class="comment">// 递归必须要有规律</span></span><br><span class="line"><span class="comment">// 构造方法不能递归</span></span><br><span class="line"><span class="comment">// 递归方法必有返回值的数据类型</span></span><br></pre></td></tr></table></figure></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个show()方法，用来掩饰递归</span></span><br><span class="line"><span class="comment">// 定义变量，记录show()方法的调用次数</span></span><br><span class="line"><span class="keyword">var</span> count = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="comment">// 递归指的就是方法自己调用自己</span></span><br><span class="line">  <span class="comment">// 1.2打印show()方法的调用次数</span></span><br><span class="line">  println(<span class="string">s&quot;<span class="subst">$&#123;count&#125;</span>调用show()&quot;</span>)</span><br><span class="line">  <span class="comment">// 1.3修改count变量的值</span></span><br><span class="line">  count = count + <span class="number">1</span></span><br><span class="line">  show()</span><br><span class="line">  <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用show()方法</span></span><br><span class="line">show()</span><br></pre></td></tr></table></figure><h2 id="2-递归案例-阶乘"><a href="#2-递归案例-阶乘" class="headerlink" title="2.递归案例-阶乘"></a>2.递归案例-阶乘</h2><p> <img src="https://pic.imgdb.cn/item/67452c6788c538a9b5bbf858.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.递归案例：阶乘</span></span><br><span class="line"><span class="comment">// 1.定义方法factorial()，用来获取指定数字的阶乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(n: <span class="type">Int</span>):<span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="comment">// 出口</span></span><br><span class="line">  <span class="keyword">if</span> (n == <span class="number">1</span>) <span class="number">1</span></span><br><span class="line">  <span class="comment">// 规律</span></span><br><span class="line">  <span class="keyword">else</span> n * factorial(n - <span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.递归案例：阶乘</span></span><br><span class="line"><span class="comment">// 2.调用factorial()方法，获取指定数字的阶乘</span></span><br><span class="line"><span class="keyword">val</span> result = factorial(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// 3. 打印结果</span></span><br><span class="line">println(result)</span><br></pre></td></tr></table></figure><h2 id="3-案列：斐波那契-不死神兔"><a href="#3-案列：斐波那契-不死神兔" class="headerlink" title="3.案列：斐波那契-不死神兔"></a>3.案列：斐波那契-不死神兔</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例二：斐波那契数列</span></span><br><span class="line"><span class="comment">// 已知数列1, 1, 2, 3, 5, 8, 13... 问第十二个是多少</span></span><br><span class="line"><span class="comment">// 出口：第一个月和第二个月的兔子对数都是 1</span></span><br><span class="line"><span class="comment">// 规律从第三个月开始，每月的兔子对数 = 它前两个月的兔子对数之和.</span></span><br><span class="line"><span class="comment">// 1.定义方法rabbit(), 用来获取兔子的对数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rabbit</span></span>(month: <span class="type">Int</span>): <span class="type">Int</span>= &#123;</span><br><span class="line">  <span class="comment">// 出口</span></span><br><span class="line">  <span class="keyword">if</span> (month == <span class="number">1</span> || month == <span class="number">2</span>) <span class="number">1</span></span><br><span class="line">  <span class="comment">// 规律</span></span><br><span class="line">  <span class="keyword">else</span> rabbit(month - <span class="number">1</span>) + rabbit(month - <span class="number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 案例二：斐波那契数列</span></span><br><span class="line"><span class="comment">// 2.调用方法，获取第12个月的兔子对数</span></span><br><span class="line"><span class="keyword">val</span> result1 = rabbit(<span class="number">12</span>)</span><br><span class="line"><span class="comment">// 3. 打印</span></span><br><span class="line">println(result1)</span><br></pre></td></tr></table></figure><h2 id="4-案例：打印目录文件"><a href="#4-案例：打印目录文件" class="headerlink" title="4.案例：打印目录文件"></a>4.案例：打印目录文件</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例三：打印目录文件</span></span><br><span class="line"><span class="comment">// 1.定义printFile(dir: File), 用来打印该目录下所有的文件路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printFile</span></span>(dir: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 1.1 判断用户传入的路径是否是文件夹路径，如果不是，直接提示</span></span><br><span class="line">  <span class="keyword">if</span> (!dir.isDirectory) &#123;</span><br><span class="line">    println(<span class="string">&quot;您录入的路径不合法，不是文件夹的路径.&quot;</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 1.2 如果用户录入的是文件夹路径，程序继续执行</span></span><br><span class="line">    <span class="comment">// listFile就是dir目录下的每一个文件或者是文件夹对象</span></span><br><span class="line">    <span class="comment">// 1.3 通过File#listFile()获取到该目录下所有的文件或者文件夹的file对象形式</span></span><br><span class="line">    <span class="keyword">val</span> listFiles = dir.listFiles()</span><br><span class="line">    <span class="comment">// 1.4 遍历，获取到上一部(1.3) 获取到的每一个File对象</span></span><br><span class="line">    <span class="keyword">for</span> (listFile &lt;- listFiles) &#123;</span><br><span class="line">      <span class="comment">// listFile：表示dir目录下每一个具体的文件或者是文件夹对象</span></span><br><span class="line">      <span class="comment">// 1.5 判断，如果是文件，就直接输出 就是出口</span></span><br><span class="line">      <span class="keyword">if</span> (listFile.isFile) println(listFile)</span><br><span class="line">      <span class="comment">// 1.6 如果是文件夹路径，就递归 就是规律</span></span><br><span class="line">      <span class="keyword">else</span> printFile(listFile)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 案例三：打印目录文件</span></span><br><span class="line"><span class="comment">// 2.调用printFile()方法</span></span><br><span class="line">println(printFile(<span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目&quot;</span>)))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-读取写入序列化</title>
      <link href="/posts/b9a6.html"/>
      <url>/posts/b9a6.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-读取数据"><a href="#1-读取数据" class="headerlink" title="1.读取数据"></a>1.读取数据</h2><h3 id="1-1按行读取"><a href="#1-1按行读取" class="headerlink" title="1.1按行读取"></a>1.1按行读取</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 按行读取</span></span><br><span class="line"><span class="comment">// 创建Source对象，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> source = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 以行为单位，来读取数据</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Iterator</span>[<span class="type">String</span>] = source.getLines()</span><br><span class="line"><span class="comment">// 将读取到的数据封装到list列表中</span></span><br><span class="line"><span class="keyword">val</span> list: <span class="type">List</span>[<span class="type">String</span>] = lines.toList</span><br><span class="line"><span class="comment">// 打印结果</span></span><br><span class="line"><span class="keyword">for</span> (data &lt;- list) println(data)</span><br><span class="line"><span class="comment">// 关闭Source对象</span></span><br><span class="line">source.close()</span><br></pre></td></tr></table></figure><h3 id="1-2按字符读取"><a href="#1-2按字符读取" class="headerlink" title="1.2按字符读取"></a>1.2按字符读取</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 按字符读取</span></span><br><span class="line"><span class="keyword">val</span> source1 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 拆干见source对象关联数据源</span></span><br><span class="line"><span class="keyword">val</span> iter: <span class="type">BufferedIterator</span>[<span class="type">Char</span>] = source1.buffered</span><br><span class="line"><span class="comment">// 已字符为单位来读取数据</span></span><br><span class="line"><span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">  <span class="comment">// 打印读取到的数据 hasNext(), next()</span></span><br><span class="line">  print(iter.next())</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 关闭source对象</span></span><br><span class="line">source.close()</span><br></pre></td></tr></table></figure><h3 id="1-3优化版（字符串版）"><a href="#1-3优化版（字符串版）" class="headerlink" title="1.3优化版（字符串版）"></a>1.3优化版（字符串版）</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 优化版，如果文件中的内容较少，我们可以直接把它读取到一个字符串中</span></span><br><span class="line"><span class="keyword">val</span> source2 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 将数据读取到一个字符串中</span></span><br><span class="line"><span class="keyword">val</span> str: <span class="type">String</span> = source2.mkString</span><br><span class="line"><span class="comment">// 打印</span></span><br><span class="line">println(str)</span><br><span class="line"><span class="comment">// 关闭source对象</span></span><br><span class="line">source2.close()</span><br></pre></td></tr></table></figure><h3 id="1-4读取词法单词和数字"><a href="#1-4读取词法单词和数字" class="headerlink" title="1.4读取词法单词和数字"></a>1.4读取词法单词和数字</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取词法单元和数字</span></span><br><span class="line"><span class="keyword">val</span> source3 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\2.txt&quot;</span>)</span><br><span class="line"><span class="comment">//将其所有数据封装到一个字符串中</span></span><br><span class="line"><span class="keyword">val</span> str1 = source3.mkString</span><br><span class="line"><span class="comment">// 按照空白字符进行切割，获取到字符串数组</span></span><br><span class="line"><span class="keyword">val</span> strArray: <span class="type">Array</span>[<span class="type">String</span>] = str1.split(<span class="string">&quot;\\s+&quot;</span>)</span><br><span class="line"><span class="comment">// 将上诉的字符串数组转换成int类型的数组</span></span><br><span class="line"><span class="keyword">val</span> intArray: <span class="type">Array</span>[<span class="type">Int</span>] = strArray.map(_.toInt)</span><br><span class="line"><span class="keyword">for</span> (data &lt;- intArray) print(data + <span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="comment">// 关闭source</span></span><br><span class="line">source3.close()</span><br></pre></td></tr></table></figure><h3 id="1-5从Url或者其他源中读取数据"><a href="#1-5从Url或者其他源中读取数据" class="headerlink" title="1.5从Url或者其他源中读取数据"></a>1.5从Url或者其他源中读取数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从url或者其他源读取数据</span></span><br><span class="line"><span class="comment">//读取传智播客官网的数据</span></span><br><span class="line"><span class="keyword">val</span> source4 = <span class="type">Source</span>.fromURL(<span class="string">&quot;https://www.itcast.cn&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> str2 = source4.mkString</span><br><span class="line">println(str2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 直接从字符串 黑马程序员中读取数据</span></span><br><span class="line"><span class="keyword">val</span> source5 = <span class="type">Source</span>.fromString(<span class="string">&quot;黑马程序员&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> str3 = source5.mkString</span><br><span class="line">println(str3)</span><br></pre></td></tr></table></figure><h3 id="1-6读取二进制文件"><a href="#1-6读取二进制文件" class="headerlink" title="1.6读取二进制文件"></a>1.6读取二进制文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取二进制文件</span></span><br><span class="line"><span class="comment">// 创建file对象，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> file = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1732176591422.png&quot;</span>)</span><br><span class="line"><span class="comment">// 创建字节输入流，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> fis = <span class="keyword">new</span> <span class="type">FileInputStream</span>(file)</span><br><span class="line"><span class="comment">// 创建字节数组，用来存储读取到的内容</span></span><br><span class="line"><span class="keyword">val</span> bys = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Byte</span>](file.length().toInt)</span><br><span class="line"><span class="comment">// 开始读取，将读取到的数据存储到字节数组中，病返回读取到的有效字节数</span></span><br><span class="line"><span class="keyword">val</span> len = fis.read(bys)</span><br><span class="line"><span class="comment">// 打印结果</span></span><br><span class="line">println(<span class="string">&quot;读取到的有效字节数：&quot;</span> + len)</span><br><span class="line">println(<span class="string">&quot;字节数组的长度&quot;</span> + bys.length)</span><br><span class="line"><span class="comment">// 关闭</span></span><br><span class="line">fis.close()</span><br></pre></td></tr></table></figure><h2 id="2-写入数据"><a href="#2-写入数据" class="headerlink" title="2.写入数据"></a>2.写入数据</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 写入数据</span></span><br><span class="line"><span class="comment">// 创建字节输出流对象，关联目的地文件</span></span><br><span class="line"><span class="keyword">val</span> fos = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\4.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 直接往目的地文件中编写指定的内容</span></span><br><span class="line">fos.write(<span class="string">&quot;黄凯君cpdd\r\n 黄凯君mjj&quot;</span>.getBytes)</span><br><span class="line"><span class="comment">// 关闭字节输出流</span></span><br><span class="line">fos.close()</span><br></pre></td></tr></table></figure><h2 id="3-序列化和反序列化"><a href="#3-序列化和反序列化" class="headerlink" title="3.序列化和反序列化"></a>3.序列化和反序列化</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 序列化和反序列化</span></span><br><span class="line"><span class="comment">// 演示序列化操作，即：将对象写入到文件中.</span></span><br><span class="line"><span class="comment">// 创建Person类型的对象</span></span><br><span class="line"><span class="keyword">val</span> p = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;黄凯君&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="comment">// 创建序列哈流，用来讲对象写入到文件中</span></span><br><span class="line"><span class="keyword">val</span> oos = <span class="keyword">new</span> <span class="type">ObjectOutputStream</span>(<span class="keyword">new</span> <span class="type">FileOutputStream</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\5.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 调用writeObject()方法， 将对象写入到文件中</span></span><br><span class="line">oos.writeObject(p)</span><br><span class="line">oos.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 反序列化操作，即：从文件中直接读取对象</span></span><br><span class="line"><span class="comment">// 创建反序列化流，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> ois = <span class="keyword">new</span> <span class="type">ObjectInputStream</span>(<span class="keyword">new</span> <span class="type">FileInputStream</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\5.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 调用readObject()方法，从数据源文件中读取指定的对象</span></span><br><span class="line"><span class="comment">// 细节：我们获取到的对象是AnyRef类型，所以需要转换成Person类型</span></span><br><span class="line"><span class="keyword">val</span> p1 = ois.readObject().asInstanceOf[<span class="type">Person</span>]</span><br><span class="line"><span class="comment">// 打印结果</span></span><br><span class="line">println(p1.name, p1.age)</span><br><span class="line"><span class="comment">// 关闭</span></span><br><span class="line">ois.close()</span><br></pre></td></tr></table></figure><h2 id="案例：学生成绩单"><a href="#案例：学生成绩单" class="headerlink" title="案例：学生成绩单"></a>案例：学生成绩单</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例：学院成绩表</span></span><br><span class="line"><span class="comment">// 创建source对象并且关联数据源</span></span><br><span class="line"><span class="keyword">val</span> source9 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\Student.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 讲数据原一行一行读出来，并且已空格为分割符存入数组</span></span><br><span class="line"><span class="keyword">val</span> stuArray = source9.getLines().map(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="comment">// 创建可变列表</span></span><br><span class="line"><span class="keyword">val</span> studentList = <span class="type">ListBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line"><span class="comment">// 将数据数组中的每一个存入列表中 这里的转换是方便添加总成绩进去，list方便动态加入</span></span><br><span class="line"><span class="keyword">for</span> (s &lt;- stuArray) &#123;</span><br><span class="line">  studentList += <span class="keyword">new</span> <span class="type">Student</span>(s(<span class="number">0</span>), s(<span class="number">1</span>).toInt, s(<span class="number">2</span>).toInt, s(<span class="number">3</span>).toInt)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将列表中的数据按照总分降序排列</span></span><br><span class="line"><span class="keyword">val</span> sortList = studentList.sortBy(_.getSum).reverse.toList</span><br><span class="line"><span class="comment">// 将处理好的数据存入指定位置</span></span><br><span class="line"><span class="keyword">val</span> bw = <span class="keyword">new</span> <span class="type">BufferedWriter</span>(<span class="keyword">new</span> <span class="type">FileWriter</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\Student1.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 将数据写入</span></span><br><span class="line"><span class="keyword">for</span> (s &lt;- sortList) &#123;</span><br><span class="line">  <span class="comment">// s 表示排序后每一个学生的信息</span></span><br><span class="line">  bw.write(<span class="string">s&quot;<span class="subst">$&#123;s.name&#125;</span>, <span class="subst">$&#123;s.Chinese&#125;</span>, <span class="subst">$&#123;s.Math&#125;</span>, <span class="subst">$&#123;s.English&#125;</span>, <span class="subst">$&#123;s.getSum&#125;</span>&quot;</span>)</span><br><span class="line">  <span class="comment">// 每写完一个学生进行换行</span></span><br><span class="line">  bw.newLine()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 关闭</span></span><br><span class="line">bw.close()</span><br><span class="line">source9.close()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala模式匹配</title>
      <link href="/posts/h2m8.html"/>
      <url>/posts/h2m8.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-模式匹配"><a href="#1-模式匹配" class="headerlink" title="1.模式匹配"></a>1.模式匹配</h2><h3 id="1-1简单模式匹配"><a href="#1-1简单模式匹配" class="headerlink" title="1.1简单模式匹配"></a>1.1简单模式匹配</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 简单模式匹配</span></span><br><span class="line">print(<span class="string">&quot;请输入一个字符串：&quot;</span>)</span><br><span class="line"><span class="comment">//    val UserStdIn: Any = StdIn.readLine()</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">UserStdIn</span>: <span class="type">Any</span> = <span class="string">&quot;hadoop&quot;</span></span><br><span class="line"><span class="type">UserStdIn</span> <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;hadoop&quot;</span> =&gt; println(<span class="string">&quot;你真是个天才&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;flume&quot;</span> =&gt; println(<span class="string">&quot;你也是个天才&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;flink&quot;</span> =&gt; println(<span class="string">&quot;人才&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;默认项&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2匹配类型"><a href="#1-2匹配类型" class="headerlink" title="1.2匹配类型"></a>1.2匹配类型</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配类型</span></span><br><span class="line"><span class="type">UserStdIn</span> <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> _: <span class="type">String</span> =&gt; println(<span class="string">&quot;这是一个字符串&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _: <span class="type">Int</span> =&gt; println(<span class="string">&quot;这是一个整形&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _: <span class="type">Char</span> =&gt; println(<span class="string">&quot;这是一个字符&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;默认项&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3守卫"><a href="#1-3守卫" class="headerlink" title="1.3守卫"></a>1.3守卫</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 守卫 所谓的守卫指的是在case语句中添加if条件判断，这样可以让我们的代码更简洁，更优雅</span></span><br><span class="line"><span class="comment">//    val NumStdin = StdIn.readInt()</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">NumStdin</span> = <span class="number">22</span></span><br><span class="line"><span class="type">NumStdin</span> <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> (x % <span class="number">2</span> == <span class="number">0</span>) =&gt; println(<span class="string">&quot;是个偶数&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> (x == <span class="number">0</span>) =&gt; println(<span class="string">&quot;是零&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;是别的数字&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4匹配样例类"><a href="#1-4匹配样例类" class="headerlink" title="1.4匹配样例类"></a>1.4匹配样例类</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配样例类  可以快速拿到传入类的参数</span></span><br><span class="line"><span class="keyword">val</span> c: <span class="type">Any</span> = <span class="type">Customer</span>(<span class="string">&quot;张三&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="keyword">val</span> o: <span class="type">Any</span> = <span class="type">Order</span>(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">c <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Customer</span>(name, age) =&gt; println(<span class="string">s&quot;Customer类型的对象, name=<span class="subst">$&#123;name&#125;</span>, age=<span class="subst">$&#123;age&#125;</span>&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Order</span>(id) =&gt; println(<span class="string">s&quot;Order类型的对象, id=<span class="subst">$&#123;id&#125;</span>&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;未匹配&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-5匹配数组、元祖、集合、列表、映射、集"><a href="#1-5匹配数组、元祖、集合、列表、映射、集" class="headerlink" title="1.5匹配数组、元祖、集合、列表、映射、集"></a>1.5匹配数组、元祖、集合、列表、映射、集</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配数组、元祖、集合、列表、映射、集</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>)</span><br><span class="line"><span class="keyword">val</span> arr2 = <span class="type">Array</span>(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">val</span> arr3 = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">arr1 <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Array</span>(<span class="number">1</span>, x, y) =&gt; println(<span class="string">s&quot;匹配到数组：长度为3,首元素为1，剩下两个元素无所谓，这里剩下的两个元素分别是：<span class="subst">$&#123;x&#125;</span>, <span class="subst">$&#123;y&#125;</span>&quot;</span>)</span><br><span class="line">     <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>) =&gt; println(<span class="string">&quot;匹配到数组：长度为1，且只有一个元素0&quot;</span>)</span><br><span class="line">     <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>, _*) =&gt; println(<span class="string">&quot;匹配到数组：以元素0开头，后边的元素无所谓&quot;</span>)</span><br><span class="line">     <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;未匹配&quot;</span>)</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="1-6匹配列表"><a href="#1-6匹配列表" class="headerlink" title="1.6匹配列表"></a>1.6匹配列表</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配列表</span></span><br><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = (<span class="number">0</span> to <span class="number">100</span>).toList</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="type">List</span>(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">list1 <span class="keyword">match</span> &#123;</span><br><span class="line">   <span class="keyword">case</span> <span class="number">0</span> :: <span class="type">Nil</span> =&gt; println(<span class="string">&quot;只包含一个0&quot;</span>)</span><br><span class="line">   <span class="keyword">case</span> <span class="number">0</span> :: tail =&gt; println(<span class="string">&quot;以0开头的列表，数量不固定&quot;</span>)</span><br><span class="line">   <span class="keyword">case</span> x :: y :: <span class="type">Nil</span> =&gt; println(<span class="string">s&quot;只有两个数字的列表，数字分别是: <span class="subst">$&#123;x&#125;</span>, <span class="subst">$&#123;y&#125;</span>&quot;</span>)</span><br><span class="line">   <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;不匹配&quot;</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="1-7匹配元组"><a href="#1-7匹配元组" class="headerlink" title="1.7匹配元组"></a>1.7匹配元组</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配元祖</span></span><br><span class="line"><span class="keyword">val</span> tuple1 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> tuple2 = (<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> tuple3 = (<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">tuple1 <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> (<span class="number">1</span>, x, y) =&gt; println(<span class="string">s&quot;以1开头的元祖，一共三个元素，另外两个元素为<span class="subst">$&#123;x&#125;</span>, <span class="subst">$&#123;y&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> (x, y, <span class="number">5</span>) =&gt; println(<span class="string">&quot;一共三个元素，最后一个元素为5的元祖&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;不匹配&quot;</span>)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="1-8变量生命中的模式匹配"><a href="#1-8变量生命中的模式匹配" class="headerlink" title="1.8变量生命中的模式匹配"></a>1.8变量生命中的模式匹配</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 变量声明中的模式匹配</span></span><br><span class="line"><span class="keyword">val</span> arr4 = (<span class="number">0</span> to <span class="number">10</span>).toArray</span><br><span class="line"><span class="comment">// 获得第二个，第三个，第四个元素</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(_, x, y, z, _*) = arr4</span><br><span class="line">println(x, y, z)</span><br><span class="line"><span class="comment">// 获得第一个，第二个元素</span></span><br><span class="line"><span class="keyword">val</span> list5 = (<span class="number">0</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="keyword">val</span> <span class="type">List</span>(a, b, _*) = list5</span><br><span class="line">println(a, b)</span><br></pre></td></tr></table></figure><h3 id="1-9匹配for表达式"><a href="#1-9匹配for表达式" class="headerlink" title="1.9匹配for表达式"></a>1.9匹配for表达式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配for表达式</span></span><br><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>(<span class="string">&quot;张三&quot;</span> -&gt; <span class="number">13</span>, <span class="string">&quot;李四&quot;</span> -&gt; <span class="number">34</span>, <span class="string">&quot;王五&quot;</span> -&gt; <span class="number">34</span>, <span class="string">&quot;赵柳&quot;</span> -&gt; <span class="number">44</span>)</span><br><span class="line"><span class="comment">// 方式1</span></span><br><span class="line"><span class="keyword">for</span> ((k, v) &lt;- map1 <span class="keyword">if</span> (v == <span class="number">34</span>)) println(k, v)</span><br><span class="line"><span class="comment">// 方式2</span></span><br><span class="line"><span class="keyword">for</span> ((k, <span class="number">34</span>) &lt;- map1) println(k, <span class="number">34</span>)</span><br></pre></td></tr></table></figure><h3 id="2-Option类型"><a href="#2-Option类型" class="headerlink" title="2.Option类型"></a>2.Option类型</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>)= &#123;</span><br><span class="line">  <span class="keyword">if</span> (b == <span class="number">0</span>) <span class="type">None</span></span><br><span class="line">  <span class="keyword">else</span> <span class="type">Some</span>(a / b)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 思路一</span></span><br><span class="line"><span class="keyword">val</span> result1 = divide(<span class="number">10</span>, <span class="number">0</span>)</span><br><span class="line">println(result1)</span><br><span class="line"><span class="comment">// 思路二</span></span><br><span class="line">result1 <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(x) =&gt; println(x)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; println(<span class="string">&quot;不能为0&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 思路三</span></span><br><span class="line">println(result1.getOrElse(<span class="string">&quot;不能为0&quot;</span>))</span><br></pre></td></tr></table></figure><h3 id="3-偏函数"><a href="#3-偏函数" class="headerlink" title="3.偏函数"></a>3.偏函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 偏函数</span></span><br><span class="line"><span class="keyword">val</span> pf: <span class="type">PartialFunction</span>[<span class="type">Int</span>, <span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;一&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">2</span> =&gt; <span class="string">&quot;二&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="string">&quot;三&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">4</span> =&gt; <span class="string">&quot;四&quot;</span></span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="string">&quot;未匹配&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(pf(<span class="number">1</span>))</span><br><span class="line">println(pf(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="3-1结合map函数使用偏函数"><a href="#3-1结合map函数使用偏函数" class="headerlink" title="3.1结合map函数使用偏函数"></a>3.1结合map函数使用偏函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 结合map函数使用偏函数</span></span><br><span class="line"><span class="keyword">val</span> list11 = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="comment">//核心： 偏函数 结合 集合的函数式编程来使用</span></span><br><span class="line"><span class="keyword">val</span> list12 = list11.map &#123;</span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> x &gt;= <span class="number">1</span> &amp;&amp; x &lt;= <span class="number">3</span> =&gt; <span class="string">&quot;[1-3]&quot;</span></span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> x &gt;= <span class="number">4</span> &amp;&amp; x &lt;= <span class="number">8</span> =&gt; <span class="string">&quot;[4-8]&quot;</span></span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="string">&quot;(8-*]&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(list12)</span><br></pre></td></tr></table></figure><h3 id="3-2正则表达式"><a href="#3-2正则表达式" class="headerlink" title="3.2正则表达式"></a>3.2正则表达式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 正则表达式</span></span><br><span class="line"><span class="keyword">val</span> email = <span class="string">&quot;hkjcpdd123@163.com&quot;</span></span><br><span class="line"><span class="keyword">val</span> regex = <span class="string">&quot;&quot;&quot;.+@.+\..+&quot;&quot;&quot;</span>.r</span><br><span class="line"><span class="keyword">if</span> (regex.findAllMatchIn(email).nonEmpty) &#123; <span class="comment">// 那个nonEmpty就是i.size != 0</span></span><br><span class="line">  <span class="comment">// 能走到这里， 说明是合法邮箱</span></span><br><span class="line">  println(<span class="string">s&quot;<span class="subst">$&#123;email&#125;</span>是合法的邮箱&quot;</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 走到这里，说明是非法的邮箱</span></span><br><span class="line">  println(<span class="string">s&quot;<span class="subst">$&#123;email&#125;</span>是合法的邮箱&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正则2：过滤所有不合法的邮箱</span></span><br><span class="line"><span class="keyword">val</span> list14 = <span class="type">List</span>(<span class="string">&quot;123456789@qq.com&quot;</span>, <span class="string">&quot;a1da7897689@gmail.com&quot;</span>, <span class="string">&quot;123afaadd.com&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regex1 = <span class="string">&quot;&quot;&quot;.+@.+\..+&quot;&quot;&quot;</span>.r</span><br><span class="line"><span class="keyword">val</span> filterList = list14.filter(regex1.findAllMatchIn(_).nonEmpty)</span><br><span class="line">println(filterList)</span><br><span class="line"><span class="comment">// 正则3：获取邮箱运营商</span></span><br><span class="line"><span class="keyword">val</span> list15 = <span class="type">List</span>(<span class="string">&quot;hkjcpdd@qq.com&quot;</span>, <span class="string">&quot;hkjmjj@gmail.com&quot;</span>, <span class="string">&quot;zhangsan@163.com&quot;</span>, <span class="string">&quot;123foshana.com&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regex2 = <span class="string">&quot;&quot;&quot;.+@(.+)\..+&quot;&quot;&quot;</span>.r</span><br><span class="line"><span class="keyword">val</span> filterList1 = list15.map &#123;</span><br><span class="line">  <span class="comment">//固定格式 表示要校验的邮箱，固定格式  正则对象（对应的是正则中的分组内容）  固定格式  邮箱-&gt;运营商</span></span><br><span class="line">  <span class="comment">// 这里@是匹配的意思，就是后者正则匹配成功的就赋值给x</span></span><br><span class="line">  <span class="keyword">case</span> x<span class="meta">@regex</span>2(company) =&gt; x -&gt; company</span><br><span class="line">  <span class="keyword">case</span> x =&gt; x -&gt; <span class="string">&quot;未匹配&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(filterList1)</span><br></pre></td></tr></table></figure><h3 id="4-异常处理"><a href="#4-异常处理" class="headerlink" title="4.异常处理"></a>4.异常处理</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 异常处理</span></span><br><span class="line"><span class="comment">// 方式一：补货异常</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">// 可能出现问题的代码</span></span><br><span class="line">  <span class="comment">//      val i = 10 / 0</span></span><br><span class="line">  <span class="keyword">val</span> i = <span class="number">10</span> / <span class="number">1</span></span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="comment">// 出现问题后的解决方案</span></span><br><span class="line">  <span class="comment">//      case ex:ArithmeticException =&gt; println(&quot;算术异常&quot;)</span></span><br><span class="line">  <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; ex.printStackTrace()</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="comment">// 这里一般用来释放资源的</span></span><br><span class="line">  println(<span class="string">&quot;我是用来释放资源的&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 方式二：抛出异常</span></span><br><span class="line"><span class="comment">//    throw new Exception(&quot;我是一个异常&quot;)</span></span><br></pre></td></tr></table></figure><h3 id="5-提取器"><a href="#5-提取器" class="headerlink" title="5.提取器"></a>5.提取器</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 提取器</span></span><br><span class="line"><span class="comment">// 方式1 普通写法</span></span><br><span class="line"><span class="keyword">val</span> s1 = <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">&quot;张三&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="comment">// 方式2  免new</span></span><br><span class="line"><span class="keyword">val</span> s2 = <span class="type">Student</span>(<span class="string">&quot;李四&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="comment">// 获取对象中的各个属性值，然后打印</span></span><br><span class="line"><span class="comment">// 方式1 普通获取</span></span><br><span class="line">println(s1.name, s1.age, s2.name, s2.age)</span><br><span class="line"><span class="comment">// 方式2 直接调用unapply方法</span></span><br><span class="line"><span class="type">Student</span>.unapply(s1)</span><br><span class="line"><span class="comment">// 方式3 通过模式匹配获取</span></span><br><span class="line">s1 <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Student</span>(name, age) =&gt; println(name, age)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;未匹配&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6-案例：随机职业"><a href="#6-案例：随机职业" class="headerlink" title="6.案例：随机职业"></a>6.案例：随机职业</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 随机职业</span></span><br><span class="line"><span class="comment">// 1.提示用户录入整数，匹配对应的职业</span></span><br><span class="line">println(<span class="string">&quot;请录入一个整数1~5， 我来告诉你上辈子的职业:&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> num = <span class="type">StdIn</span>.readInt()</span><br><span class="line"><span class="keyword">val</span> occupation = num <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;人机&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;人才&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;黄凯君&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;黄开俊&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;黄凯军&quot;</span></span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="string">&quot;未匹配&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(occupation)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala - 函数式编程</title>
      <link href="/posts/a3f7.html"/>
      <url>/posts/a3f7.html</url>
      
        <content type="html"><![CDATA[<h2 id="scala函数式编程"><a href="#scala函数式编程" class="headerlink" title="scala函数式编程"></a>scala函数式编程</h2><h3 id="1-foreach方法"><a href="#1-foreach方法" class="headerlink" title="1.foreach方法"></a>1.foreach方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 遍历列表、数组</span></span><br><span class="line"><span class="keyword">val</span> list1 = (<span class="number">1</span> to <span class="number">5</span>).toList</span><br><span class="line"><span class="comment">// foreach方法</span></span><br><span class="line">list1.foreach(println(_))</span><br></pre></td></tr></table></figure><h3 id="2-映射map"><a href="#2-映射map" class="headerlink" title="2.映射map"></a>2.映射map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将结果整理成想要的</span></span><br><span class="line"><span class="keyword">val</span> list2 = list1.map(<span class="string">&quot;*&quot;</span> * _)</span><br><span class="line">println(<span class="string">&quot;映射&quot;</span> + list2)</span><br></pre></td></tr></table></figure><h3 id="3-扁平化映射flatMap"><a href="#3-扁平化映射flatMap" class="headerlink" title="3.扁平化映射flatMap"></a>3.扁平化映射flatMap</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将结果先进行映射然后进行扁平化</span></span><br><span class="line"><span class="keyword">val</span> list3 = <span class="type">List</span>((<span class="string">&quot;hkjcpdd hkjmjj hkjppp&quot;</span>), (<span class="string">&quot;hkjcpdd hkjmjj hkjppp&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list4 = list3.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(<span class="string">&quot;扁平化映射&quot;</span> + list4)</span><br></pre></td></tr></table></figure><h3 id="4-排序"><a href="#4-排序" class="headerlink" title="4.排序"></a>4.排序</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统排序sorted如果需要反转在用完sorted之后reverse就好了</span></span><br><span class="line"><span class="keyword">val</span> list6 = <span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">0</span>)</span><br><span class="line">println(<span class="string">&quot;传统排序&quot;</span> + list6.sorted.rever</span><br><span class="line">        </span><br><span class="line"><span class="comment">// 指定字段排序</span></span><br><span class="line"><span class="keyword">val</span> list7 = <span class="type">List</span>(<span class="string">&quot;01 hadoop&quot;</span>, <span class="string">&quot;02 flume&quot;</span>, <span class="string">&quot;03 spark&quot;</span>)</span><br><span class="line">println(<span class="string">&quot;指定字段排序&quot;</span> + list7.sortBy(_.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>))</span><br><span class="line">                                             </span><br><span class="line"><span class="comment">// 自定义排序</span></span><br><span class="line">println(<span class="string">&quot;自定义排序&quot;</span> + list1.sortWith((x, y) =&gt; x &gt; y))</span><br></pre></td></tr></table></figure><h3 id="5-分组"><a href="#5-分组" class="headerlink" title="5.分组"></a>5.分组</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分组</span></span><br><span class="line"><span class="keyword">val</span> list8 = <span class="type">List</span>(<span class="string">&quot;刘德华&quot;</span> -&gt; <span class="string">&quot;男&quot;</span>, <span class="string">&quot;黄凯军&quot;</span> -&gt; <span class="string">&quot;男&quot;</span>, <span class="string">&quot;黄凯俊&quot;</span> -&gt; <span class="string">&quot;女&quot;</span> ,<span class="string">&quot;黄凯君&quot;</span> -&gt; <span class="string">&quot;男&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> map1 = list8.groupBy(_._2)</span><br><span class="line"><span class="keyword">val</span> map2 = map1.map(x =&gt; x._1 -&gt; x._2.size)</span><br><span class="line"><span class="type">Console</span>.println(map2)</span><br></pre></td></tr></table></figure><h3 id="6-聚合操作"><a href="#6-聚合操作" class="headerlink" title="6.聚合操作"></a>6.聚合操作</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 聚合操作 reduce用来对集合元素进行聚合计算 fold用来对集合进行折叠计算</span></span><br><span class="line"><span class="keyword">val</span> list9 = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="comment">// 使用reduce计算所有元素的和</span></span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;reduce聚合= &quot;</span> + list9.reduce((x, y) =&gt; x - y))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;reduceLeft聚合 = &quot;</span> + list9.reduceLeft((x, y) =&gt; x - y))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;reduceRight聚合 = &quot;</span> + list9.reduceRight((x, y) =&gt; x -y))</span><br><span class="line"></span><br><span class="line"><span class="comment">// fold与reduce很像,只不过多了一个指定初始值参数 100表示初始化值, 后面表示函数对象</span></span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;fold = &quot;</span> + list9.fold(<span class="number">100</span>)((x, y) =&gt; x + y))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;foldLeft&quot;</span> + list9.foldLeft(<span class="number">100</span>)(_ + _))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;foldRight&quot;</span> + list9.foldRight(<span class="number">100</span>)(_ + _))</span><br></pre></td></tr></table></figure><h3 id="7-具体案例（学生成绩单）"><a href="#7-具体案例（学生成绩单）" class="headerlink" title="7.具体案例（学生成绩单）"></a>7.具体案例（学生成绩单）</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 学生成绩单</span></span><br><span class="line"><span class="keyword">val</span> listStu = <span class="type">List</span>((<span class="string">&quot;李四&quot;</span>, <span class="number">21</span>, <span class="number">89</span> ,<span class="number">43</span>), (<span class="string">&quot;王五&quot;</span>, <span class="number">56</span>, <span class="number">89</span>, <span class="number">98</span>), (<span class="string">&quot;赵柳&quot;</span>, <span class="number">66</span>, <span class="number">33</span>, <span class="number">55</span>), (<span class="string">&quot;黄凯君&quot;</span>, <span class="number">23</span>, <span class="number">54</span>, <span class="number">22</span>))</span><br><span class="line"><span class="comment">// 获取所有语文成绩在60及以上的</span></span><br><span class="line"><span class="keyword">val</span> filterList = listStu.filter(_._2 &gt;= <span class="number">60</span>)</span><br><span class="line">println(filterList.map(_._1))</span><br><span class="line"><span class="comment">// 获取所有学生的总成绩</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SumList</span> = listStu.map(x =&gt; x._1 -&gt; (x._2 + x._3 + x._4))</span><br><span class="line">println(<span class="string">&quot;所有学生的总成绩: &quot;</span> + <span class="type">SumList</span>)</span><br><span class="line"><span class="comment">// 按照总成绩降序排列</span></span><br><span class="line">println(<span class="string">&quot;按照总成绩进行排序&quot;</span> + <span class="type">SumList</span>.sortWith((x, y) =&gt; x._2 &lt; x._2))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala</title>
      <link href="/posts/b9a7.html"/>
      <url>/posts/b9a7.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala位运算符</title>
      <link href="/posts/d591.html"/>
      <url>/posts/d591.html</url>
      
        <content type="html"><![CDATA[<h3 id="位运算符"><a href="#位运算符" class="headerlink" title="位运算符"></a>位运算符</h3><p>  <img src="https://pic.imgdb.cn/item/67242470d29ded1a8cd11909.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; print(a &amp; b)# 相同为<span class="number">1</span>=上下想乘</span><br><span class="line">scala&gt; print(a | b) # 有<span class="number">1</span>则<span class="number">1</span></span><br><span class="line">scala&gt; print(a ^ b) # 不同为<span class="number">1</span></span><br><span class="line">scala&gt; print(~a) # 按位取反 因为拿到的是补码，<span class="number">-1</span>然后的到反码，反码符号位不变，其他按位取反</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(第二套)ZZ052大数据应用与服务赛项赛题</title>
      <link href="/posts/b2b7.html"/>
      <url>/posts/b2b7.html</url>
      
        <content type="html"><![CDATA[<h2 id="模块一：平台搭建与运维"><a href="#模块一：平台搭建与运维" class="headerlink" title="模块一：平台搭建与运维"></a>模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a><strong>（一）任务一：大数据平台搭建</strong></h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a><strong>1．子任务一：基础环境准备</strong></h4><p>（1）对三台环境更新主机名，配置hosts文件，以node01作为时钟源并进行时间同步；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">其他三台一样</span></span><br><span class="line">[root@localhost ~]# hostnamectl set-hostname master</span><br><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure><p>（2）执行命令生成公钥、私钥，实现三台机器间的免秘登陆；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">其他三台一样</span></span><br><span class="line">[root@localhost ~]# ssh-keygen</span><br><span class="line">[root@localhost ~]# ssh-copy-id master</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave1</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave2</span><br></pre></td></tr></table></figure><p>（3）从宿主机&#x2F;root 目录下将文件 jdk-8u212-linux-x64.tar.gz 复制到容器 node01 中的&#x2F;root&#x2F;software 路径中（若路径不存在，则需新建），将 node01 节点 JDK 安装包解压到&#x2F;root&#x2F;software 路径中(若路径不存在，则需新建)；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# mkdir /root/software</span><br><span class="line">[root@localhost software]# tar -zxvf /opt/software/jdk-8u391-linux-x64.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（4）修改容器中&#x2F;etc&#x2F;profile 文件，设置 JDK 环境变量并使其生效，配置完毕后在 node01 节点分别执行“java - version”和“javac”命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新增</span></span><br><span class="line">export JAVA_HOME=/root/software/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">[root@localhost software]# source /etc/profile</span><br><span class="line">[root@localhost software]# java -version</span><br><span class="line">java version &quot;1.8.0_391&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br><span class="line">[root@localhost software]# javac</span><br><span class="line">用法: javac &lt;options&gt; &lt;source files&gt;</span><br><span class="line">其中, 可能的选项包括:</span><br><span class="line">  -g                         生成所有调试信息</span><br><span class="line">  -g:none                    不生成任何调试信息</span><br><span class="line">  -g:&#123;lines,vars,source&#125;     只生成某些调试信息</span><br><span class="line">  -nowarn                    不生成任何警告</span><br><span class="line">  -verbose                   输出有关编译器正在执行的操作的消息</span><br><span class="line">  -deprecation               输出使用已过时的 API 的源位置</span><br><span class="line">  -classpath &lt;路径&gt;            指定查找用户类文件和注释处理程序的位置</span><br><span class="line">  -cp &lt;路径&gt;                   指定查找用户类文件和注释处理程序的位置</span><br><span class="line">  -sourcepath &lt;路径&gt;           指定查找输入源文件的位置</span><br><span class="line">  -bootclasspath &lt;路径&gt;        覆盖引导类文件的位置</span><br><span class="line">  -extdirs &lt;目录&gt;              覆盖所安装扩展的位置</span><br><span class="line">  -endorseddirs &lt;目录&gt;         覆盖签名的标准路径的位置</span><br><span class="line">  -proc:&#123;none,only&#125;          控制是否执行注释处理和/或编译。</span><br><span class="line">  -processor &lt;class1&gt;[,&lt;class2&gt;,&lt;class3&gt;...] 要运行的注释处理程序的名称; 绕过默认的搜索进程</span><br><span class="line">  -processorpath &lt;路径&gt;        指定查找注释处理程序的位置</span><br><span class="line">  -parameters                生成元数据以用于方法参数的反射</span><br><span class="line">  -d &lt;目录&gt;                    指定放置生成的类文件的位置</span><br><span class="line">  -s &lt;目录&gt;                    指定放置生成的源文件的位置</span><br><span class="line">  -h &lt;目录&gt;                    指定放置生成的本机标头文件的位置</span><br><span class="line">  -implicit:&#123;none,class&#125;     指定是否为隐式引用文件生成类文件</span><br><span class="line">  -encoding &lt;编码&gt;             指定源文件使用的字符编码</span><br><span class="line">  -source &lt;发行版&gt;              提供与指定发行版的源兼容性</span><br><span class="line">  -target &lt;发行版&gt;              生成特定 VM 版本的类文件</span><br><span class="line">  -profile &lt;配置文件&gt;            请确保使用的 API 在指定的配置文件中可用</span><br><span class="line">  -version                   版本信息</span><br><span class="line">  -help                      输出标准选项的提要</span><br><span class="line">  -A关键字[=值]                  传递给注释处理程序的选项</span><br><span class="line">  -X                         输出非标准选项的提要</span><br><span class="line">  -J&lt;标记&gt;                     直接将 &lt;标记&gt; 传递给运行时系统</span><br><span class="line">  -Werror                    出现警告时终止编译</span><br><span class="line">  @&lt;文件名&gt;                     从文件读取选项和文件名</span><br></pre></td></tr></table></figure><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop需要配置前置环境。命令中要求使用绝对路径，具体要求如下:</p><p>（1）在 node01 将 Hadoop 解压到&#x2F;root&#x2F;software(若路径不存在，则需新建)目录下，并将解压包分发至 node02、node03 中，其中三节点节点均作为 datanode，配置好相关环境，初始化 Hadoop 环境 namenode；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# tar -zxvf /opt/software/hadoop-3.3.6.tar.gz -C /root/software/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分发</span></span><br><span class="line">[root@localhost software]# scp -r jdk/ hadoop/ slave1:`pwd`</span><br><span class="line">[root@localhost software]# scp -r jdk/ hadoop/ slave2:`pwd`</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化</span></span><br><span class="line">[root@localhost software]# hadoop namenode -format</span><br></pre></td></tr></table></figure><p>（2）开启集群，查看各节点进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动</span></span><br><span class="line">[root@localhost software]# start-all.sh</span><br><span class="line">[root@localhost software]# jps</span><br><span class="line">47810 ResourceManager</span><br><span class="line">48597 WebAppProxyServer</span><br><span class="line">49205 JobHistoryServer</span><br><span class="line">47450 SecondaryNameNode</span><br><span class="line">49371 Jps</span><br><span class="line">48060 NodeManager</span><br><span class="line">46766 NameNode</span><br><span class="line">47134 DataNode</span><br></pre></td></tr></table></figure><h4 id="3．子任务三：Hive-安装配置"><a href="#3．子任务三：Hive-安装配置" class="headerlink" title="3．子任务三：Hive 安装配置"></a><strong>3．子任务三：Hive 安装配置</strong></h4><p>（1）从宿主机&#x2F;root 目录下将文件 apache-hive-3.1.2-bin.tar.gz、mysql-connector-java-5.1.37.jar 复制到容器 node03 中的&#x2F;root&#x2F;software 路径中（若路径不存在，则需新建），将 node03 节点 Hive 安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost conf]# tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2）设置 Hive 环境变量，并使环境变量生效，执行命令 hive –version 查看版本信息；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost conf]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/root/software/jdk</span><br><span class="line">export HADOOP_HOME=/root/software/hadoop</span><br><span class="line">export HIVE_HOME=/root/software/hive</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@master conf]# hive --version</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive 3.1.3</span><br><span class="line">Git git://MacBook-Pro.fios-router.home/Users/ngangam/commit/hive -r 4df4d75bf1e16fe0af75aad0b4179c34c07fc975</span><br><span class="line">Compiled by ngangam on Sun Apr 3 16:58:16 EDT 2022</span><br><span class="line">From source with checksum 5da234766db5dfbe3e92926c9bbab2af</span><br></pre></td></tr></table></figure><p>（3）修改相关配置，添加依赖包，将 MySQL 数据库作为Hive 元数据库，初始化 Hive 元数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置好hive-env.sh和hive-site.xml</span></span><br><span class="line">[root@master conf]# cp /opt/software/mysql-connector-java-5.1.34.jar /root/software/hive/lib/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化</span></span><br><span class="line">[root@master conf]# schematool -initSchema -dbType mysql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动</span></span><br><span class="line">[root@master hive]# mkdir logs</span><br><span class="line">[root@master hive]# nohup hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;</span><br><span class="line">[1] 128547</span><br><span class="line"></span><br><span class="line">[root@master hive]# hive</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">which: no hbase in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/software/jdk/bin:/root/software/hadoop/bin:/root/software/hadoop/sbin:/root/software/hive/bin:/root/bin)</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive Session ID = 7e262d28-438c-4922-af9c-1323adfa7f88</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/root/software/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 195fb209-4ef9-4452-bbdd-90b81a8d04bc</span><br><span class="line"><span class="meta prompt_">hive&gt;</span></span><br></pre></td></tr></table></figure><p><strong>4．子任务四：Flume 安装配置</strong> </p><p>（1）从宿主机&#x2F;root 目录下将文件 apache-flume-1.11.0-bin.tar.gz复制到容器node03中的&#x2F;root&#x2F;software路径中（若路径不存在，则需新建），将 node03 节点 Flume安装包解压到&#x2F;root&#x2F;software 目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -zxvf /opt/software/apache-flume-1.11.0-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2）完善相关配置，配置 Flume 环境变量，并使环境变量生效，执行命令 flume-ng version。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新增</span></span><br><span class="line">export FLUME_HOME_HOME=/root/software/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[root@master software]# source /etc/profile</span><br><span class="line"></span><br><span class="line">[root@master software]# flume-ng version</span><br><span class="line">Flume 1.11.0</span><br><span class="line">Source code repository: https://git.apache.org/repos/asf/flume.git</span><br><span class="line">Revision: 1a15927e594fd0d05a59d804b90a9c31ec93f5e1</span><br><span class="line">Compiled by rgoers on Sun Oct 16 14:44:15 MST 2022</span><br><span class="line">From source with checksum bbbca682177262aac3a89defde369a37</span><br></pre></td></tr></table></figure><h3 id="（二）任务二：数据库配置维护"><a href="#（二）任务二：数据库配置维护" class="headerlink" title="（二）任务二：数据库配置维护"></a><strong>（二）任务二：数据库配置维护</strong></h3><h4 id="1．子任务一：数据库配置"><a href="#1．子任务一：数据库配置" class="headerlink" title="1．子任务一：数据库配置"></a><strong>1．子任务一：数据库配置</strong></h4><p>（1）在主机 node3 上安装 mysql-community-server，启动 mySQL 服务，根据临时密码进入数据库，并修改本地密码为“123456”；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> software]# rpm <span class="operator">-</span>ivh mysql<span class="operator">-</span>community<span class="operator">-</span>server<span class="number">-5.7</span><span class="number">.44</span><span class="number">-1.</span>el7.x86_64.rpm </span><br><span class="line">警告：mysql<span class="operator">-</span>community<span class="operator">-</span>server<span class="number">-5.7</span><span class="number">.44</span><span class="number">-1.</span>el7.x86_64.rpm: 头V4 RSA<span class="operator">/</span>SHA256 Signature, 密钥 ID <span class="number">3</span>a79bd29: NOKEY</span><br><span class="line">准备中...                          ################################# [<span class="number">100</span><span class="operator">%</span>]</span><br><span class="line">正在升级<span class="operator">/</span>安装...</span><br><span class="line">   <span class="number">1</span>:mysql<span class="operator">-</span>community<span class="operator">-</span>server<span class="number">-5.7</span><span class="number">.44</span><span class="number">-1.</span>e################################# [<span class="number">100</span><span class="operator">%</span>]</span><br><span class="line">[root<span class="variable">@master</span> software]# systemctl <span class="keyword">start</span> mysqld</span><br><span class="line">[root<span class="variable">@master</span> software]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">2</span></span><br><span class="line">Server version: <span class="number">5.7</span><span class="number">.44</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2023</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">user</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）开启 MySQL 远程连接权限，所有 root 用户都可以使用 123456 进行登录连接。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">bind<span class="operator">-</span>address<span class="operator">=</span><span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;mysql.sys&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p><strong>2．子任务二：导入相关表</strong> </p><p>（1）将本地&#x2F;root&#x2F;eduhq&#x2F;equipment&#x2F;目录下的数据文件 root_sl_src.sql 导入 MySQL 对应数据库 root_sl_src；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">use root_sl_src</span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash"><span class="built_in">source</span> root_sl_src.sql</span></span><br></pre></td></tr></table></figure><p>（2）将本地&#x2F;root&#x2F;eduhq&#x2F;equipment&#x2F;目录下的数据文件 root_sl_ugoogds_src.sql 导 入 MySQL 对应数据库root_sl_ugoogds_src。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> use root_sl_ugoogds_src</span><br><span class="line">mysql<span class="operator">&gt;</span> source root_sl_ugoogds_src.sql</span><br></pre></td></tr></table></figure><p><strong>3．子任务三：维护数据表</strong> </p><p>结合已导入的两份 sql 数据，对其中的数据进行如下查询和操作。</p><p>（1）对‘root_sl_src’数据库中的‘province’数据表进行修改，修改字段 province_id 为 24 的记录的province_name，修改为‘内蒙古自治区’；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> province <span class="keyword">set</span> province_name<span class="operator">=</span><span class="string">&#x27;内蒙古自治区&#x27;</span> <span class="keyword">where</span> province_id<span class="operator">=</span><span class="number">24</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（2）对‘root_sl_src’数据库中的‘city’数据表进行删除，删除字段 city_id 为 142 的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">delete</span> <span class="keyword">from</span> city <span class="keyword">where</span> city_id<span class="operator">=</span><span class="number">142</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.03</span> sec)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 国赛赛题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(第一套)ZZ052大数据应用与服务赛项赛题</title>
      <link href="/posts/b2b6.html"/>
      <url>/posts/b2b6.html</url>
      
        <content type="html"><![CDATA[<h2 id="模块一：平台搭建与运维"><a href="#模块一：平台搭建与运维" class="headerlink" title="模块一：平台搭建与运维"></a>模块一：平台搭建与运维</h2><p><strong>（一）任务一：大数据平台搭建</strong> </p><p>本模块需要使用 root 用户完成相关配置；所有组件均</p><p>在&#x2F;root&#x2F;software 目录下。</p><h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">三台机都要</span></span><br><span class="line">[root@localhost ~]# systemctl stop firewalld</span><br><span class="line">[root@localhost ~]# systemctl disable firewalld</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# vi /etc/hosts</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加：</span></span><br><span class="line">192.168.1.122 master</span><br><span class="line">192.168.1.123 slave1</span><br><span class="line">192.168.1.124 slave2</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# ssh-keygen# 一直回车即可</span><br><span class="line">[root@localhost ~]# ssh-copy-id master# yes 然后密码</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave1</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">改主机名</span></span><br><span class="line">[root@localhost software]# hostnamectl set-hostname master# 以此类推第二台slave1第三台slave2</span><br></pre></td></tr></table></figure><p><strong>1．子任务一：基础环境准备</strong> </p><p>master、slave1、slave2三台节点都需要安装JDK</p><p>（1） 将JDK安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# tar -zxvf /opt/software/jdk-8u391-linux-x64.tar.gz -C /root/software/</span><br><span class="line">[root@localhost software]# cd /root/software/</span><br><span class="line">[root@localhost software]# mv jdk1.8.0_391/ jdk</span><br></pre></td></tr></table></figure><p>（2） 在“&#x2F;etc&#x2F;profile”文件中配置JDK环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加</span></span><br><span class="line">export JAVA_HOME=/root/software/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">[root@localhost software]# source /etc/profile</span><br></pre></td></tr></table></figure><p>JAVA_HOME和PATH的值，并让配置文件立即生效；</p><p>（3） 查看JDK版本，检测JDK是否安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# java -version</span><br><span class="line">java version &quot;1.8.0_391&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记得分发jdk给slave1和slave2</span></span><br><span class="line">[root@master software]# scp -r jdk/ slave1:`pwd`</span><br><span class="line">[root@master software]# scp -r jdk/ slave2:`pwd`</span><br></pre></td></tr></table></figure><p>在master节点操作</p><p>（1） 在master上生成SSH密钥对；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前置工作已做</span><br></pre></td></tr></table></figure><p>（2） 将master上的公钥拷贝到slave1和slave2上；在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# ssh slave1</span><br><span class="line">Last login: Mon Oct 28 11:43:50 2024 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]# exit;</span><br><span class="line">登出</span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@localhost software]# ssh slave1</span><br><span class="line">Last login: Mon Oct 28 11:58:39 2024 from master</span><br><span class="line">[root@slave1 ~]# exit</span><br><span class="line">登出</span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@localhost software]# ssh slave2</span><br><span class="line">Last login: Mon Oct 28 11:43:52 2024 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]# exit</span><br><span class="line">登出</span><br><span class="line">Connection to slave2 closed.</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong> </p><p>master、slave1、slave2三台节点都需要安装Hadoop</p><p>（1） 在 主 节 点 将 Hadoop 安 装 包 解 压 到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /opt/software/hadoop-3.3.6.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2） 依次配置hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml和workers配置</p><p>文件；Hadoop集群部署规划如下表；</p><p>表1    Hadoop集群部署规划</p><p><img src="https://pic.imgdb.cn/item/6721a0cdd29ded1a8cdcb200.png"></p><p>（3） 在master节点的Hadoop安装目录下依次创建hadoopDatas&#x2F;tempDatas 、 hadoopDatas&#x2F;namenodeDatas 、hadoopDatas&#x2F;datanodeDatas、hadoopDatas&#x2F;dfs&#x2F;nn&#x2F;edits、hadoopDatas&#x2F;dfs&#x2F;snn&#x2F;name 和hadoopDatas&#x2F;dfs&#x2F;nn&#x2F;snn&#x2F;edits目录；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/tempDatas</span><br><span class="line">[root@localhost hadoop]# mkdir hadoopDatas/namenodeDatas </span><br><span class="line">[root@localhost hadoop]# mkdir hadoopDatas/datanodeDatas</span><br><span class="line">[root@localhost hadoop]# mkdir hadoopDatas/dfs/nn/edits</span><br><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/dfs/nn/edits</span><br><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/dfs/snn/name</span><br><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/dfs/nn/snn/edits</span><br></pre></td></tr></table></figure><p>（4） 在master节点上使用scp命令将配置完的Hadoop安装目录直接拷贝至slave1和slave2；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /root/software/hadoop/ slave1:`pwd`</span><br><span class="line">scp -r /root/software/hadoop/ slave2:`pwd`</span><br></pre></td></tr></table></figure><p>（5） 三台节点的“&#x2F;etc&#x2F;profile”文件中配置Hadoop环境变量HADOOP_HOME和PATH的值，并让配置文件立即生效；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# scp /etc/profile slave1:/etc</span><br><span class="line">profile                                                        100% 1985     3.4MB/s   00:00    </span><br><span class="line">[root@master software]# scp /etc/profile slave2:/etc</span><br><span class="line">profile                                                        100% 1985     3.4MB/s   00:00</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">slave1</span></span><br><span class="line">[root@slave1 ~]# source /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">slave2</span></span><br><span class="line">[root@slave2 ~]# source /etc/profile</span><br></pre></td></tr></table></figure><p>（6） 在主节点格式化集群；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# hadoop namenode -format</span><br></pre></td></tr></table></figure><p>（7） 在主节点依次启动HDFS、YARN集群和历史服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@master /]# start-all.sh</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">上一次登录：一 10月 28 16:05:13 CST 2024pts/0 上</span><br><span class="line">Starting datanodes</span><br><span class="line">上一次登录：一 10月 28 16:05:25 CST 2024pts/0 上</span><br><span class="line">Starting secondary namenodes [master]</span><br><span class="line">上一次登录：一 10月 28 16:05:27 CST 2024pts/0 上</span><br><span class="line">Starting resourcemanager</span><br><span class="line">上一次登录：一 10月 28 16:05:33 CST 2024pts/0 上</span><br><span class="line">Starting nodemanagers</span><br><span class="line">上一次登录：一 10月 28 16:05:40 CST 2024pts/0 上</span><br><span class="line">上一次登录：一 10月 28 16:05:42 CST 2024pts/0 上</span><br><span class="line">[root@master /]# mapred --daemon start historyserver</span><br><span class="line">[root@master /]# jps</span><br><span class="line">70417 WebAppProxyServer</span><br><span class="line">68435 NameNode</span><br><span class="line">71267 JobHistoryServer</span><br><span class="line">69218 SecondaryNameNode</span><br><span class="line">71385 Jps</span><br><span class="line">69560 ResourceManager</span><br><span class="line">69816 NodeManager</span><br><span class="line">68718 DataNode</span><br></pre></td></tr></table></figure><p><strong>3．子任务三：MySQL 安装配置</strong> </p><p>只在master节点操作</p><p>（1） 将MySQL 5.7.25安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -xvf /opt/software/mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /root/software/</span><br><span class="line">mysql-community-client-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-common-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-devel-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-devel-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-server-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-test-5.7.44-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>（2） 使 用 rpm -ivh 依 次 安 装 mysql-community-common、mysql-community- libs、mysql-community-libscompat 、 mysql-community-client 和 mysql-communityserver包；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>（3） 安装好MySQL后，使用mysql用户初始化和启动数据库；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span><br><span class="line">[root@master software]# systemctl start mysqld</span><br></pre></td></tr></table></figure><p>（4） 使用root用户无密码登录MySQL，然后将root用户的密码修改为123456，修改完成退出MySQL，重新登录验证密码是否修改成功；更改“mysql”数据库里的 user 表里的 host 项，从localhost 改成%即可实现用户远程登录；设置完成刷新配置信息，让其生效。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> software]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">3</span></span><br><span class="line">Server version: <span class="number">5.7</span><span class="number">.44</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2023</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">user</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> exit;</span><br><span class="line">Bye</span><br><span class="line">[root<span class="variable">@master</span> software]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">4</span></span><br><span class="line">Server version: <span class="number">5.7</span><span class="number">.44</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2023</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">user</span>,host <span class="keyword">from</span> mysql.user;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>          <span class="operator">|</span> host      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> mysql.session <span class="operator">|</span> localhost <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql.sys     <span class="operator">|</span> localhost <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> root          <span class="operator">|</span> localhost <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;mysql.sys&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;mysql.session&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>4．子任务四：Hive 安装配置</strong> </p><p>只在master节点操作。</p><p>（1） 将Hive 3.1.2的安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2） 在“&#x2F;etc&#x2F;profile”文件中配置Hive环境变量HIVE_HOME和PATH的值，并让配置文件立即生效；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# cd /root/software/</span><br><span class="line">[root@master software]# mv apache-hive-3.1.3-bin/ hive</span><br><span class="line">[root@master software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加</span></span><br><span class="line">export HIVE_HOME=/root/software/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">[root@master software]# source /etc/profile</span><br></pre></td></tr></table></figure><p>（3） 查看Hive版本，检测Hive环境变量是否设置成功；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# hive --version</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive 3.1.3</span><br><span class="line">Git git://MacBook-Pro.fios-router.home/Users/ngangam/commit/hive -r 4df4d75bf1e16fe0af75aad0b4179c34c07fc975</span><br><span class="line">Compiled by ngangam on Sun Apr 3 16:58:16 EDT 2022</span><br><span class="line">From source with checksum 5da234766db5dfbe3e92926c9bbab2af</span><br></pre></td></tr></table></figure><p>（4） 切换到 $HIVE_HOME&#x2F;conf 目录下，将 hiveenv.sh.template文件复制一份并重命名为hive-env.sh；然后，使用vim编辑器进行编辑，在文件中配置HADOOP_HOME、HIVE_CONF_DIR以及HIVE_AUX_JARS_PATH参数的值，将原有值删除并将前面的注释符#去掉；配置完成，保存退出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# cp hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置这些值</span></span><br><span class="line">HADOOP_HOME=/root/software/hadoop</span><br><span class="line">export HIVE_CONF_DIR=/root/software/hive</span><br><span class="line">export HIVE_AUX_JARS_PATH=/root/software/hive/lib</span><br></pre></td></tr></table></figure><p>（5） 将 &#x2F;root&#x2F;software 目 录 下 的 MySQL 驱动包mysql-connector-java-5.1.47-bin.jar 拷 贝 到$HIVE_HOME&#x2F;lib目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# cp /opt/software/mysql-connector-java-5.1.34.jar /root/software/hive/lib/</span><br></pre></td></tr></table></figure><p>（6） 在$HIVE_HOME&#x2F;conf目录下创建一个名为hive-site.xml的文件，并使用vim编辑器进行编辑；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# vi hive-site.xml</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进行配置</span></span><br></pre></td></tr></table></figure><p>配置如下内容：</p><p><img src="https://pic.imgdb.cn/item/6721a102d29ded1a8cdd10d5.png"></p><p>（7） 使用schematool命令，通过指定元数据库类型为“mysql”，来初始化源数据库的元数据；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure><p>（8） 使用CLI启动Hive，进入Hive客户端；在Hive默认数据库下创建一个名为student的管理表；</p><p><img src="https://pic.imgdb.cn/item/6721a127d29ded1a8cdd5fdd.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# hive</span><br><span class="line">hive&gt; create table student(</span><br><span class="line">    &gt; id int,</span><br><span class="line">    &gt; name string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.73 seconds</span><br></pre></td></tr></table></figure><p>（9） 通过insert语句往student表中插入一条测试数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into table student values(1, &#x27;hkj&#x27;);</span><br></pre></td></tr></table></figure><p><strong>5．子任务五：Flume 安装配置</strong> </p><p>只在 master 节点操作。</p><p>（1） 将 Flume 1.11.0 的安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -zxvf /opt/software/apache-flume-1.11.0-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2） 在“&#x2F;etc&#x2F;profile”文件中配置Flume环境变量FLUME_HOME和PATH的值，并让配置文件立即生效；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# cd /root/software/</span><br><span class="line">[root@master software]# mv apache-flume-1.11.0-bin/ flume</span><br><span class="line">[root@master software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加</span></span><br><span class="line">export FLUME_HOME=/root/software/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[root@master software]# source /etc/profile</span><br></pre></td></tr></table></figure><p>（3） 使 用 cd 命 令 进 入 &#x2F;root&#x2F;software&#x2F;apache&#x2F;flume-1.11.0-bin&#x2F;conf 目 录 下 ， 使 用 cp 命令将 flumeenv.sh.template文件复制一份，并重命名为flume-env.sh；使 用 vim 命 令 打 开 “flume-env.sh” 配 置 文 件 ， 找 到JAVA_HOME参数位置，将前面的“#”去掉，将值修改为本机JDK的实际位置；修改完成，保存退出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# cd /root/software/flume/conf/</span><br><span class="line">[root@master conf]# cp flume-env.sh.template flume-env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将java_home配置项的<span class="comment">#去掉然后更换地址</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例：<span class="built_in">export</span> JAVA_HOME=/root/software/jdk</span></span><br></pre></td></tr></table></figure><p>（4） 查看Flume版本，检测Flume是否安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# flume-ng version</span><br><span class="line">Flume 1.11.0</span><br><span class="line">Source code repository: https://git.apache.org/repos/asf/flume.git</span><br><span class="line">Revision: 1a15927e594fd0d05a59d804b90a9c31ec93f5e1</span><br><span class="line">Compiled by rgoers on Sun Oct 16 14:44:15 MST 2022</span><br><span class="line">From source with checksum bbbca682177262aac3a89defde369a37</span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库配置维护</strong> </p><p><strong>1．子任务一：数据库配置</strong> </p><p>在 Hive 中创建一个名为 comm 的数据库，如果数据库已经存在，则不进行创建。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# hive</span><br><span class="line">hive&gt; create database if not exist comm;</span><br><span class="line">FAILED: ParseException line 1:23 missing KW_EXISTS at &#x27;exist&#x27; near &#x27;&lt;EOF&gt;&#x27;</span><br><span class="line">line 1:29 extraneous input &#x27;comm&#x27; expecting EOF near &#x27;&lt;EOF&gt;&#x27;</span><br><span class="line">hive&gt; create database if not exists comm;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.297 seconds</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：创建相关表</strong> </p><p>（1） 在 comm 数 据 库 下 创 建 一 个 名 为ods_behavior_log的外部表，如果表已存在，则先删除；分区字段为dt，即根据日期进行分区；同时，使用location关键 字 将 表 的 存 储 路 径 设 置 为 HDFS 的&#x2F;behavior&#x2F;ods&#x2F;ods_behavior_log目录；字段类型如下表所示；</p><p><img src="https://pic.imgdb.cn/item/6721a147d29ded1a8cddbd2e.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use comm;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.042 seconds</span><br><span class="line">hive&gt; CREATE TABLE IF NOT EXISTS ods_behavior_log (</span><br><span class="line">    &gt;   line STRING</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; PARTITIONED BY (dt STRING)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.244 seconds</span><br></pre></td></tr></table></figure><p>（2） 使 用 load data 子 句 将 本 地&#x2F;root&#x2F;eduhq&#x2F;data&#x2F;app_log&#x2F;behavior目录下的每个数据文件依次加载到外部表ods_behavior_log的对应分区中，按照日志文件对应日期定义静态分区（例如：dt&#x3D;’2023-01-01’）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-01.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-01&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.831 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-02.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-02&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.628 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-03.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-03&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.491 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-04.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-04&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.506 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-05.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-05&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.465 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-06.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-06&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.44 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-07.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-07&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.423 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-01.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-01&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.432 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-02.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-02&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.397 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-03.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-03&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.425 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-04.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-04&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.43 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-05.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-05&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.395 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-06.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-06&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.422 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-07.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-07&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.773 seconds</span><br></pre></td></tr></table></figure><p>（3） 查看ods_behavior_log表的所有现有分区、前3行数据，并统计外部表ods_behavior_log数据总行数；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># 查看所有现有分区</span><br><span class="line">hive&gt; SHOW PARTITIONS ods_behavior_log;</span><br><span class="line">OK</span><br><span class="line">dt=2023-01-01</span><br><span class="line">dt=2023-01-02</span><br><span class="line">dt=2023-01-03</span><br><span class="line">dt=2023-01-04</span><br><span class="line">dt=2023-01-05</span><br><span class="line">dt=2023-01-06</span><br><span class="line">dt=2023-01-07</span><br><span class="line">Time taken: 0.089 seconds, Fetched: 7 row(s)</span><br><span class="line"></span><br><span class="line"># 前三行数据</span><br><span class="line">hive&gt; SELECT * FROM ods_behavior_log LIMIT 3;</span><br><span class="line">OK</span><br><span class="line">&#123;&quot;client_ip&quot;: &quot;222.86.237.202&quot;, &quot;device_type&quot;: &quot;pc&quot;, &quot;time&quot;: 1672580908000, &quot;type&quot;: &quot;WIFI&quot;, &quot;device&quot;: &quot;357c5dd65e834b8bafac861bdae5d76d&quot;, &quot;url&quot;: &quot;http://wan.baidu.com/home?idfrom=4087&quot;&#125;        2023-01-01</span><br><span class="line">&#123;&quot;client_ip&quot;: &quot;210.42.113.5&quot;, &quot;device_type&quot;: &quot;pc&quot;, &quot;time&quot;: 1672539082000, &quot;type&quot;: &quot;WIFI&quot;, &quot;device&quot;: &quot;6cab446d0c664de4b09c76bae90fc1bf&quot;, &quot;url&quot;: &quot;http://go.hao123.com/sites&quot;&#125;     2023-01-01</span><br><span class="line">&#123;&quot;client_ip&quot;: &quot;222.34.42.67&quot;, &quot;device_type&quot;: &quot;mobile&quot;, &quot;time&quot;: 1672524854000, &quot;type&quot;: &quot;5G&quot;, &quot;device&quot;: &quot;36b4213510e74c7e8455b61fb481b576&quot;, &quot;url&quot;: &quot;https://4366yy.381pk.com/1251/&quot;&#125;       2023-01-01</span><br><span class="line">Time taken: 1.805 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line"># 统计数据总行数</span><br><span class="line">hive&gt; SELECT COUNT(*) FROM ods_behavior_log;</span><br><span class="line">Query ID = root_20241028190258_2de0fddd-343e-4256-b248-f252f3400a9b</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Starting Job = job_1730102747884_0003, Tracking URL = http://master:8089/proxy/application_1730102747884_0003/</span><br><span class="line">Kill Command = /root/software/hadoop/bin/mapred job  -kill job_1730102747884_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2024-10-28 19:03:31,071 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-10-28 19:03:42,782 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:04:43,115 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:05:43,251 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:06:44,246 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:06:53,529 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.32 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 4 seconds 320 msec</span><br><span class="line">Ended Job = job_1730102747884_0003</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.32 sec   HDFS Read: 37824491 HDFS Write: 106 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 4 seconds 320 msec</span><br><span class="line">OK</span><br><span class="line">198413</span><br><span class="line">Time taken: 235.981 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>（4） 在 comm 数 据 库 下 创 建 一 个 名 为dwd_behavior_log的外部表，如果表已存在，则先删除；分区字段为dt，即根据日期进行分区；另外，要求指定表的存储路径为HDFS的&#x2F;behavior&#x2F;dwd&#x2F;dwd_behavior_log目录，存储文件类型为“orc”，文件的压缩类型为“snappy”；字段类型如下表所示；</p><p><img src="https://pic.imgdb.cn/item/6721a16ad29ded1a8cde1313.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS comm.dwd_behavior_log (</span><br><span class="line">    &gt;   client_ip STRING,</span><br><span class="line">    &gt;   device_type STRING,</span><br><span class="line">    &gt;   type STRING,</span><br><span class="line">    &gt;   device STRING,</span><br><span class="line">    &gt;   url STRING,</span><br><span class="line">    &gt;   province STRING,</span><br><span class="line">    &gt;   city STRING,</span><br><span class="line">    &gt;   ts BIGINT</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; PARTITIONED BY (dt STRING)</span><br><span class="line">    &gt; STORED AS ORC</span><br><span class="line">    &gt; LOCATION &#x27;/behavior/dwd/dwd_behavior_log&#x27;</span><br><span class="line">    &gt; TBLPROPERTIES (&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.091 seconds</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 国赛赛题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker-dockerfile</title>
      <link href="/posts/d3ce.html"/>
      <url>/posts/d3ce.html</url>
      
        <content type="html"><![CDATA[<p>404</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker-compose</title>
      <link href="/posts/713b.html"/>
      <url>/posts/713b.html</url>
      
        <content type="html"><![CDATA[<ul><li>首先需要compose,yaml文件</li><li>上线：docker compose up -d</li><li>下线：docker compse down </li><li>启动：docker compose start x1 x2 x3</li><li>停止：docker compose stop x1 x3</li><li>扩容：docker compose scale x2&#x3D;3</li></ul><h4 id="compose-yaml例子："><a href="#compose-yaml例子：" class="headerlink" title="compose.yaml例子："></a>compose.yaml例子：</h4><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">name: myblog</span><br><span class="line">services:</span><br><span class="line">  mysql:</span><br><span class="line">    container_name: mysql</span><br><span class="line">    image: mysql:<span class="number">8.0</span></span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">&quot;3306:3306&quot;</span></span><br><span class="line">    environment:</span><br><span class="line">      MYSQL_ROOT_PASSWORD: <span class="string">&quot;123456&quot;</span></span><br><span class="line">      MYSQL_DATABASE: wordpress</span><br><span class="line">    volumes:</span><br><span class="line">      - mysql-data:/var/lib/mysql</span><br><span class="line">      - ./myconf:/etc/mysql/conf.d </span><br><span class="line">    restart: always</span><br><span class="line">    networks:</span><br><span class="line">      - blog</span><br><span class="line"></span><br><span class="line">  wordpress:</span><br><span class="line">    image: wordpress</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">&quot;8080:80&quot;</span></span><br><span class="line">    environment: </span><br><span class="line">      WORDPRESS_DB_HOST: mysql</span><br><span class="line">      WORDPRESS_DB_USER: root</span><br><span class="line">      WORDPRESS_DB_PASSWORD: <span class="string">&quot;123456&quot;</span> </span><br><span class="line">      WORDPRESS_DB_NAME: wordpress </span><br><span class="line">    volumes:</span><br><span class="line">      - wordpress:/var/www/html</span><br><span class="line">    restart: always</span><br><span class="line">    networks:</span><br><span class="line">      - blog</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  mysql-data:</span><br><span class="line">  wordpress:</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  blog:</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker安装</title>
      <link href="/posts/1fef.html"/>
      <url>/posts/1fef.html</url>
      
        <content type="html"><![CDATA[<h3 id="先移除旧版的docker如果没有就跳过"><a href="#先移除旧版的docker如果没有就跳过" class="headerlink" title="先移除旧版的docker如果没有就跳过"></a>先移除旧版的docker如果没有就跳过</h3><ul><li>移除旧版本docker \</li><li>sudo yum remove docker \</li><li>docker-client \</li><li>docker-client-latest \</li><li>docker-common \</li><li>docker-latest \</li><li>docker-latest-logrotate \</li><li>docker-logrotate \</li><li>docker-engine</li></ul><h3 id="配置docker-yum源"><a href="#配置docker-yum源" class="headerlink" title="配置docker yum源"></a>配置docker yum源</h3><ul><li>sudo yum install -y yum-utils</li><li>sudo yum-config-manager </li><li>–add-repo </li><li><a href="http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo">http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</a></li><li>如果已经配置好yum为阿里云的就不用了</li></ul><h2 id="安装-最新-docker"><a href="#安装-最新-docker" class="headerlink" title="安装 最新 docker"></a>安装 最新 docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</span><br></pre></td></tr></table></figure><h2 id="启动-amp-开机启动docker；-enable-start-二合一"><a href="#启动-amp-开机启动docker；-enable-start-二合一" class="headerlink" title="启动&amp; 开机启动docker； enable + start 二合一"></a>启动&amp; 开机启动docker； enable + start 二合一</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable docker --now</span><br></pre></td></tr></table></figure><h3 id="配置加速"><a href="#配置加速" class="headerlink" title="配置加速"></a>配置加速</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors那地址</span><br><span class="line">sudo mkdir -p /etc/docker</span><br><span class="line">sudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">&#123;</span><br><span class="line">&quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line">这个加速不太行使用另外一个</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker保存镜像</title>
      <link href="/posts/287f.html"/>
      <url>/posts/287f.html</url>
      
        <content type="html"><![CDATA[<h3 id="提交：docker-commit"><a href="#提交：docker-commit" class="headerlink" title="提交：docker commit"></a>提交：docker commit</h3><ul><li>docker commit [option] 名字就是用着的那个镜像的名字 新建镜像的名字:版本号</li><li>docker commit -m “update index.html” mynginx mynginx:1.0</li><li>-m 版本信息之类的</li></ul><h3 id="保存-docker-save"><a href="#保存-docker-save" class="headerlink" title="保存: docker save"></a>保存: docker save</h3><ul><li>docker save -o mynginx.tar mynginx:1.0</li><li>-o 保存出来就是本地</li></ul><h3 id="加载：docker-load"><a href="#加载：docker-load" class="headerlink" title="加载：docker load"></a>加载：docker load</h3><p>docker load -i mynginx.tar</p><p>-i就是读取本地的镜像</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker分享社区</title>
      <link href="/posts/e3ea.html"/>
      <url>/posts/e3ea.html</url>
      
        <content type="html"><![CDATA[<h3 id="登录：docker-login"><a href="#登录：docker-login" class="headerlink" title="登录：docker login"></a>登录：docker login</h3><h3 id="命令-docker-tag"><a href="#命令-docker-tag" class="headerlink" title="命令:docker tag"></a>命令:docker tag</h3><ul><li>docker tag 原来的镜像名:版本号 新的镜像名:版本号</li><li>还可以docker tag mynginx:v1.0 lhx&#x2F;mynginx:v1.0 改名都是用户名&#x2F;镜像这样子的</li></ul><p>推荐在制作一个最新版本的镜像， 就是latest    docker tag mynginx:v1.0 lhx&#x2F;mynginx:latest然后在推送上去</p><h4 id="推送：docker-push-镜像名字"><a href="#推送：docker-push-镜像名字" class="headerlink" title="推送：docker push + 镜像名字"></a>推送：docker push + 镜像名字</h4><p><img src="/posts/e3ea.htm/Users\LHX\AppData\Roaming\Typora\typora-user-images\image-20241025115810209.png" alt="image-20241025115810209"></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker国内镜像源</title>
      <link href="/posts/ec39.html"/>
      <url>/posts/ec39.html</url>
      
        <content type="html"><![CDATA[<h3 id="进入配置文件registry-mirror：需要重启docker服务"><a href="#进入配置文件registry-mirror：需要重启docker服务" class="headerlink" title="进入配置文件registry mirror：需要重启docker服务"></a>进入配置文件registry mirror：需要重启docker服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker</span><br><span class="line">tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">&#123;</span><br><span class="line">&quot;registry-mirrors&quot;: [</span><br><span class="line">&quot;https://do.nark.eu.org&quot;,</span><br><span class="line">&quot;https://dc.j8.work&quot;,</span><br><span class="line">&quot;https://docker.m.daocloud.io&quot;,</span><br><span class="line">&quot;https://dockerproxy.com&quot;,</span><br><span class="line">&quot;https://docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">&quot;https://docker.nju.edu.cn&quot;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker存储</title>
      <link href="/posts/ade7.html"/>
      <url>/posts/ade7.html</url>
      
        <content type="html"><![CDATA[<ol><li>目录挂载：-v 外目录:内目录</li><li>例子：docker run -d -p 80:80 -v &#x2F;app&#x2F;nghtml:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html –name app01 nginx</li><li>卷映射 -v 卷名:内目录</li><li>例子：docker run -d -p 80:80 -v ngconf:&#x2F;etc&#x2F;nginx</li><li>卷的统一位置在：&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;</li><li>列出所有的卷docker volume ls</li><li>新建卷docker volume create hkjcpdd</li><li>查看卷的详情docker volume inspect ngconf</li></ol>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker架构</title>
      <link href="/posts/af13.html"/>
      <url>/posts/af13.html</url>
      
        <content type="html"><![CDATA[<h3 id="下载镜像的话就是，docker-pull-镜像-镜像-x3D-标签-版本-latest为最新"><a href="#下载镜像的话就是，docker-pull-镜像-镜像-x3D-标签-版本-latest为最新" class="headerlink" title="下载镜像的话就是，docker pull + 镜像  镜像&#x3D;标签+版本  latest为最新"></a>下载镜像的话就是，docker pull + 镜像  镜像&#x3D;标签+版本  latest为最新</h3><ul><li>下载别的版本的话就是去dockerhub上看</li><li>检索docker search + 名字</li><li>列表：docker images</li><li>删除：docker rmi + 完整标签或者id 如：docker rmi nginx:latest</li></ul><h3 id="运行镜像的话就是，docker-run-options-image-commend-arg…-后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加"><a href="#运行镜像的话就是，docker-run-options-image-commend-arg…-后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加" class="headerlink" title="运行镜像的话就是，docker run [options] image [commend] [arg…] 后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加"></a>运行镜像的话就是，docker run [options] image [commend] [arg…] 后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加</h3><ul><li>后台运行-d</li><li>起个名字 –name</li><li>端口映射 -p 80:80</li></ul><ol><li><p>查看docker ps</p></li><li><p>停止 docker stop</p></li><li><p>启动docker start</p></li><li><p>重启 docker restart </p></li><li><p>状态 docker stats</p></li><li><p>日志 docker logs</p></li><li><p>进入 docker exec</p><p>docker exec -it mynginx &#x2F;bin&#x2F;bash</p><p>-it是交互模式</p><p>后面的路径是让他在控制台上显示</p></li><li><p>删除 docker rm</p></li><li><p>用docker构建自己的软件包，docker build + 镜像</p></li><li><p>发给应用市场让dock保存的话就是，dock push + 镜像</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker网络</title>
      <link href="/posts/5796.html"/>
      <url>/posts/5796.html</url>
      
        <content type="html"><![CDATA[<p>查看容器的细节</p><p>docker inspect app2</p><p>docker network + 功能</p><p><img src="/posts/5796.htm/Users\LHX\AppData\Roaming\Typora\typora-user-images\image-20241025120003627.png" alt="image-20241025120003627"></p><h4 id="环境变量就用-e"><a href="#环境变量就用-e" class="headerlink" title="环境变量就用-e"></a>环境变量就用-e</h4>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker保存镜像</title>
      <link href="/posts/287f.html"/>
      <url>/posts/287f.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yum配置</title>
      <link href="/posts/dfc4.html"/>
      <url>/posts/dfc4.html</url>
      
        <content type="html"><![CDATA[<h2 id="普通配置"><a href="#普通配置" class="headerlink" title="普通配置"></a>普通配置</h2><ul><li><p>关闭防火墙</p></li><li><p>然后关闭selinux</p></li><li><p>查看网卡 nmcli c s</p></li><li><p>刷新网卡 nmcli c u ens33</p></li><li><p>开启ssh远程连接 vi &#x2F;etc&#x2F;ssh&#x2F;sshd_config</p></li><li><p>刷新服务 systemctl restart sshd</p></li><li><p>先暂时挂载mount &#x2F;opt&#x2F;镜像 &#x2F;mnt&#x2F;</p></li><li><p>然后永久挂载echo &#x2F;opt&#x2F;镜像 &#x2F;mnt&#x2F; iso9660 defaults 0 0 &gt;&gt; &#x2F;etc&#x2F;fstab</p></li><li><p>然后进入&#x2F;etc&#x2F;yum.repo</p></li><li><p>全部删除</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/yum.repo</span><br><span class="line">[1]</span><br><span class="line">name=1</span><br><span class="line">baseurl=file:///mnt/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">yum repolist all</span><br></pre></td></tr></table></figure></li></ul><h3 id="阿里云源："><a href="#阿里云源：" class="headerlink" title="阿里云源："></a>阿里云源：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">先安装wget    yum install wget -y</span><br><span class="line">然后拿文件</span><br><span class="line">wget -O /etc/yum.repo.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">然后清理缓存还有生成新的缓存</span><br><span class="line">清理 yum clean all</span><br><span class="line">生成 yum makecache</span><br><span class="line">接下来下载epel库</span><br><span class="line">yum install epel-release -y</span><br><span class="line">然后重复一次清理和生成就可以了</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> yum配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> yum配置 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
