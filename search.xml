<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink</title>
      <link href="/posts/pd11.html"/>
      <url>/posts/pd11.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink快速上手</title>
      <link href="/posts/pd13.html"/>
      <url>/posts/pd13.html</url>
      
        <content type="html"><![CDATA[<h1 id="Flink快速上手"><a href="#Flink快速上手" class="headerlink" title="Flink快速上手"></a>Flink快速上手</h1><blockquote><p>[!NOTE]</p><p>前提准备好相关maven环境和依赖</p></blockquote><h2 id="1-WordCount代码编写"><a href="#1-WordCount代码编写" class="headerlink" title="1.WordCount代码编写"></a>1.WordCount代码编写</h2><h5 id="需求：统计一段文字中，每个单词出现的频次。"><a href="#需求：统计一段文字中，每个单词出现的频次。" class="headerlink" title="需求：统计一段文字中，每个单词出现的频次。"></a>需求：统计一段文字中，每个单词出现的频次。</h5><h5 id="环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。"><a href="#环境准备：在src-x2F-main-x2F-java目录下，新建一个包，命名为com-atguigu-wc。" class="headerlink" title="环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。"></a>环境准备：在src&#x2F;main&#x2F;java目录下，新建一个包，命名为com.atguigu.wc。</h5><h3 id="1-1-批处理"><a href="#1-1-批处理" class="headerlink" title="1.1  批处理"></a>1.1  批处理</h3><h5 id="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"><a href="#批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。" class="headerlink" title="批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。"></a>批处理基本思路：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。</h5><h4 id="1-数据准备"><a href="#1-数据准备" class="headerlink" title="1)数据准备"></a>1)数据准备</h4><ul><li>（1）在工程根目录下新建一个input文件夹，并在下面创建文本文件words.txt</li><li>（2）在words.txt中输入一些文字，例如：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello flink</span><br><span class="line">hello world</span><br><span class="line">hello java</span><br></pre></td></tr></table></figure><h4 id="2-代码编写"><a href="#2-代码编写" class="headerlink" title="2)代码编写"></a>2)代码编写</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建一个执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineData = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据集进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineData.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneGroup = wordAndOne.groupBy(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合统计</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）输出"><a href="#（2）输出" class="headerlink" title="（2）输出"></a>（2）输出</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(flink,1)</span><br><span class="line">(world,1)</span><br><span class="line">(hello,3)</span><br><span class="line">(java,1)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>需要注意的是，这种代码的实现方式，是基于DataSet API的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上Flink本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的API来实现。所以从Flink 1.12开始，官方推荐的做法是直接使用DataStream API，在提交任务时通过将执行模式设为BATCH来进行批处理：</p><p>$ bin&#x2F;flink run -Dexecution.runtime-mode&#x3D;BATCH BatchWordCount.jar</p><p>这样，DataSet API就没什么用了，在实际应用中我们只要维护一套DataStream API就可以。这里只是为了方便大家理解，我们依然用DataSet API做了批处理的实现。</p></blockquote><h3 id="1-2流处理"><a href="#1-2流处理" class="headerlink" title="1.2流处理"></a>1.2流处理</h3><blockquote><p>[!CAUTION]</p><p>对于Flink而言，流才是整个处理逻辑的底层核心，所以流批统一之后的DataStream API更加强大，可以直接处理批处理和流处理的所有场景。</p></blockquote><h5 id="我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"><a href="#我们同样试图读取文档words-txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致" class="headerlink" title="我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致"></a>我们同样试图读取文档words.txt中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BoundedStreamWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建执行环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文本文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lineDataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对数据进行转换处理</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 按照单词进行分组</span></span><br><span class="line">    <span class="keyword">val</span> wordAndGroup = wordAndOne.keyBy(_._1)</span><br><span class="line">    <span class="comment">// 对分组数据进行sum聚合操作</span></span><br><span class="line">    <span class="keyword">val</span> sum = wordAndGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印</span></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行任务</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">3&gt; (java,1)</span><br><span class="line">5&gt; (hello,1)</span><br><span class="line">5&gt; (hello,2)</span><br><span class="line">5&gt; (hello,3)</span><br><span class="line">13&gt; (flink,1)</span><br><span class="line">9&gt; (world,1)</span><br></pre></td></tr></table></figure><h3 id="主要观察与批处理程序BatchWordCount的不同："><a href="#主要观察与批处理程序BatchWordCount的不同：" class="headerlink" title="主要观察与批处理程序BatchWordCount的不同："></a>主要观察与批处理程序BatchWordCount的不同：</h3><ul><li>创建执行环境的不同，流处理程序使用的是StreamExecutionEnvironment</li><li>转换处理之后，得到的数据对象类型不同</li><li>分组操作调用的是keyBy方法，可以传入一个匿名函数作为键选择器（KeySelector），指定当前分组的key是什么</li><li>代码末尾需要调用env的execute方法，开始执行任务</li></ul><h4 id="2）读取socket文本流"><a href="#2）读取socket文本流" class="headerlink" title="2）读取socket文本流"></a>2）读取socket文本流</h4><blockquote><p>[!NOTE]</p><p>在实际的生产环境中，真正的数据流其实是无界的，有开始却没有结束，这就要求我们需要持续地处理捕获的数据。为了模拟这种场景，可以监听socket端口，然后向该端口不断的发送数据。</p></blockquote><h4 id="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："><a href="#（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：" class="headerlink" title="（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下："></a>（1）将StreamWordCount代码中读取文件数据的readTextFile方法，替换成读取socket文本流的方法socketTextStream。具体代码实现如下：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day01</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.<span class="type">ParameterTool</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamWordCOunt</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> parameterTool = <span class="type">ParameterTool</span>.fromArgs(args)</span><br><span class="line">    <span class="keyword">val</span> hostname = parameterTool.get(<span class="string">&quot;host&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> port = parameterTool.getInt(<span class="string">&quot;port&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineDataStream = env.socketTextStream(hostname, port)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordAndOne = lineDataStream.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineAndOneGroup = wordAndOne.keyBy(_._1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sum = lineAndOneGroup.sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    sum.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"><a href="#（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试" class="headerlink" title="（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试"></a>（2）在Linux环境的主机hadoop102上，执行下列命令，发送数据进行测试</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ nc -lk 7777</span><br></pre></td></tr></table></figure><blockquote><p>[!IMPORTANT]</p><p>注意：要先启动端口，后启动StreamWordCount程序，否则会报超时连接异常。</p></blockquote><h4 id="（3）启动StreamWordCount程序"><a href="#（3）启动StreamWordCount程序" class="headerlink" title="（3）启动StreamWordCount程序"></a>（3）启动StreamWordCount程序</h4><blockquote><p>[!WARNING]</p><p>我们会发现程序启动之后没有任何输出、也不会退出。这是正常的，因为Flink的流处理是事件驱动的，当前程序会一直处于监听状态，只有接收到数据才会执行任务、输出统计结果。</p></blockquote><h5 id="（4）从hadoop102发送数据"><a href="#（4）从hadoop102发送数据" class="headerlink" title="（4）从hadoop102发送数据"></a>（4）从hadoop102发送数据</h5><h5 id="①在hadoop102主机中，输入“hello-flink”，输出如下内容"><a href="#①在hadoop102主机中，输入“hello-flink”，输出如下内容" class="headerlink" title="①在hadoop102主机中，输入“hello flink”，输出如下内容"></a>①在hadoop102主机中，输入“hello flink”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13&gt; (flink,1)</span><br><span class="line">5&gt; (hello,1)</span><br></pre></td></tr></table></figure><h5 id="②再输入“hello-world”，输出如下内容"><a href="#②再输入“hello-world”，输出如下内容" class="headerlink" title="②再输入“hello world”，输出如下内容"></a>②再输入“hello world”，输出如下内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2&gt; (world,1)</span><br><span class="line">5&gt; (hello,2)</span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</p><p>因为对于flatMap里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink集群部署搭建</title>
      <link href="/posts/pd12.html"/>
      <url>/posts/pd12.html</url>
      
        <content type="html"><![CDATA[<h1 id="Flink1-14-0集群部署"><a href="#Flink1-14-0集群部署" class="headerlink" title="Flink1.14.0集群部署"></a><strong>Flink1.14.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line"><span class="comment">#     将jobmanager.rpc.address的主机名改成master就好了</span></span><br><span class="line"><span class="comment"># 分发到其他主机，然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-17-0集群部署"><a href="#Flink1-17-0集群部署" class="headerlink" title="Flink1.17.0集群部署"></a><strong>Flink1.17.0集群部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改conf目录下的workers将三台主机名写进去</span></span><br><span class="line"><span class="comment"># 修改master为master:8081</span></span><br><span class="line"><span class="comment"># 修改flink-conf.yaml文件</span></span><br><span class="line">    <span class="comment"># JobManager节点地址.</span></span><br><span class="line">    jobmanager.rpc.address: master</span><br><span class="line">    jobmanager.bind-host: 0.0.0.0</span><br><span class="line">    rest.address: master</span><br><span class="line">    rest.bind-address: 0.0.0.0</span><br><span class="line">    <span class="comment"># TaskManager节点地址.需要配置为当前机器名</span></span><br><span class="line">    taskmanager.bind-host: 0.0.0.0</span><br><span class="line">    taskmanager.host: master</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发到其他主机</span></span><br><span class="line"><span class="comment"># 修改slave1的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave1</span><br><span class="line"><span class="comment"># 修改slave2的 taskmanager.host</span></span><br><span class="line">    taskmanager.host: slave2</span><br><span class="line"><span class="comment"># 然后就可以启动了，start-cluster.sh启动  web端为8081</span></span><br></pre></td></tr></table></figure><h1 id="Flink1-14-0高可用部署"><a href="#Flink1-14-0高可用部署" class="headerlink" title="Flink1.14.0高可用部署"></a><strong>Flink1.14.0高可用部署</strong></h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前面正常部署，后面在high中新增</span></span><br><span class="line"><span class="comment">#开启HA，使用文件系统作为快照存储</span></span><br><span class="line">state.backend: filesystem （选填）</span><br><span class="line"> </span><br><span class="line"><span class="comment">#启用检查点，可以将快照保存到HDFS （选填）</span></span><br><span class="line">state.backend.fs.checkpointdir: hdfs://node1:8020/flink-checkpoints</span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用zookeeper搭建高可用 （必填）</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 存储JobManager的元数据到HDFS （必填）</span></span><br><span class="line">high-availability.storageDir: hdfs://node1:8020/flink/ha/</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 配置ZK集群地址（必填）</span></span><br><span class="line">high-availability.zookeeper.quorum: node1:2181,node2:2181,node3:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定flink在zokkeper的位置（选填）</span></span><br><span class="line"> high-availability.zookeeper.path.root: /flink</span><br><span class="line"> </span><br><span class="line">  然后将flink-shaded-hadoop-2-uber-2.8.3-10.0.jar复制到/root/software/flink/lib中</span><br><span class="line"><span class="comment">#  然后scp最后就可以启动了</span></span><br></pre></td></tr></table></figure><h1 id="Flink-On-Yarn"><a href="#Flink-On-Yarn" class="headerlink" title="Flink On Yarn"></a>Flink On Yarn</h1><h3 id="1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode"><a href="#1-flink的Yarn部署模式分为三种方式：一种是Application-Mode，-一种是Per-lob-Mode，一种是Session-Mode" class="headerlink" title="1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode"></a><strong>1.flink的Yarn部署模式分为三种方式：一种是Application Mode， 一种是Per-lob Mode，一种是Session Mode</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">配置环境变量，增加环境变量配置如下：</span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=`hadoop classpath`</span><br></pre></td></tr></table></figure><h3 id="2-会话模式部署"><a href="#2-会话模式部署" class="headerlink" title="2 会话模式部署"></a>2 会话模式部署</h3><h3 id="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下："><a href="#YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN-Session）来启动Flink集群。具体步骤如下：" class="headerlink" title="YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下："></a>YARN的会话模式与独立集群略有不同，需要首先申请一个YARN会话（YARN Session）来启动Flink集群。具体步骤如下：</h3><h3 id="（1）启动集群"><a href="#（1）启动集群" class="headerlink" title="（1）启动集群"></a>（1）启动集群</h3><ul><li><h3 id="（1）启动Hadoop集群（HDFS、YARN）。"><a href="#（1）启动Hadoop集群（HDFS、YARN）。" class="headerlink" title="（1）启动Hadoop集群（HDFS、YARN）。"></a>（1）启动Hadoop集群（HDFS、YARN）。</h3></li><li><h3 id="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"><a href="#（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。" class="headerlink" title="（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。"></a>（2）执行脚本命令向YARN集群申请资源，开启一个YARN会话，启动Flink集群。</h3></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/yarn-session.sh -nm <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">-d：分离模式，如果你不想让Flink YARN客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，YARN session也可以后台运行。</span><br><span class="line">-jm（–jobManagerMemory）：配置JobManager所需内存，默认单位MB。</span><br><span class="line">-nm（–name）：配置在YARN UI界面上显示的任务名。</span><br><span class="line">-qu（–queue）：指定YARN队列名。</span><br><span class="line">-tm（–taskManager）：配置每个TaskManager所使用内存。</span><br></pre></td></tr></table></figure><h5 id="YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。"><a href="#YARN-Session启动之后会给出一个Web-UI地址以及一个YARN-application-ID，如下所示，用户可以通过Web-UI或者命令行两种方式提交作业。" class="headerlink" title="YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。"></a>YARN Session启动之后会给出一个Web UI地址以及一个YARN application ID，如下所示，用户可以通过Web UI或者命令行两种方式提交作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2022-11-17 15:20:52,711 INFO  org.apache.flink.yarn.YarnClusterDescriptor                  [] - Found Web Interface hadoop104:40825 of application <span class="string">&#x27;application_1668668287070_0005&#x27;</span>.</span><br><span class="line">JobManager Web Interface: http://hadoop104:40825</span><br></pre></td></tr></table></figure><h3 id="（2）通过命令行提交作业"><a href="#（2）通过命令行提交作业" class="headerlink" title="（2）通过命令行提交作业"></a><strong>（2）通过命令行提交作业</strong></h3><ul><li>① 将FlinkTutorial-1.0-SNAPSHOT.jar任务上传至集群。</li><li>② 执行以下命令将该任务提交到已经开启的Yarn-Session中运行。</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h3 id="3-单作业模式部署"><a href="#3-单作业模式部署" class="headerlink" title="3 单作业模式部署"></a><strong>3 单作业模式部署</strong></h3><h6 id="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"><a href="#在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群" class="headerlink" title="在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群"></a>在YARN环境中，由于有了外部平台做资源调度，所以我们也可以直接向YARN提交一个单独的作业，从而启动一个Flink集群</h6><h6 id="（1）执行命令提交作业"><a href="#（1）执行命令提交作业" class="headerlink" title="（1）执行命令提交作业"></a>（1）执行命令提交作业</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -d -t yarn-per-job -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><h6 id="注意：如果启动过程中报如下异常"><a href="#注意：如果启动过程中报如下异常" class="headerlink" title="注意：如果启动过程中报如下异常"></a>注意：如果启动过程中报如下异常</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread “Thread-5” java.lang.IllegalStateException: Trying to access closed classloader. Please check <span class="keyword">if</span> you store classloaders directly or indirectly <span class="keyword">in</span> static fields. If the stacktrace suggests that the leak occurs <span class="keyword">in</span> a third party library and cannot be fixed immediately, you can <span class="built_in">disable</span> this check with the configuration ‘classloader.check-leaked-classloader’.</span><br><span class="line">at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders</span><br></pre></td></tr></table></figure><h6 id="解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置"><a href="#解决办法：在flink的-x2F-opt-x2F-module-x2F-flink-1-17-0-x2F-conf-x2F-flink-conf-yaml配置文件中设置" class="headerlink" title="解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置"></a>解决办法：在flink的&#x2F;opt&#x2F;module&#x2F;flink-1.17.0&#x2F;conf&#x2F;flink-conf.yaml配置文件中设置</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim flink-conf.yaml</span><br><span class="line"></span><br><span class="line">classloader.check-leaked-classloader: <span class="literal">false</span></span><br></pre></td></tr></table></figure><h6 id="（2）可以使用命令行查看或取消作业，命令如下。"><a href="#（2）可以使用命令行查看或取消作业，命令如下。" class="headerlink" title="（2）可以使用命令行查看或取消作业，命令如下。"></a>（2）可以使用命令行查看或取消作业，命令如下。</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h3 id="4-应用模式部署"><a href="#4-应用模式部署" class="headerlink" title="4 应用模式部署"></a><strong>4 应用模式部署</strong></h3><h4 id="1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。"><a href="#1-应用模式同样非常简单，与单作业模式类似，直接执行flink-run-application命令即可。" class="headerlink" title="(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。"></a>(1)应用模式同样非常简单，与单作业模式类似，直接执行flink run-application命令即可。</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar </span><br></pre></td></tr></table></figure><h5 id="2-在命令行中查看或取消作业。"><a href="#2-在命令行中查看或取消作业。" class="headerlink" title="(2)在命令行中查看或取消作业。"></a>(2)在命令行中查看或取消作业。</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY &lt;jobId&gt;</span><br></pre></td></tr></table></figure><h4 id="2）上传HDFS提交"><a href="#2）上传HDFS提交" class="headerlink" title="2）上传HDFS提交"></a><strong>2）上传HDFS提交</strong></h4><h5 id="可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。"><a href="#可以通过yarn-provided-lib-dirs配置选项指定位置，将flink的依赖上传到远程。" class="headerlink" title="可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。"></a>可以通过yarn.provided.lib.dirs配置选项指定位置，将flink的依赖上传到远程。</h5><h6 id="（1）上传flink的lib和plugins到HDFS上"><a href="#（1）上传flink的lib和plugins到HDFS上" class="headerlink" title="（1）上传flink的lib和plugins到HDFS上"></a>（1）上传flink的lib和plugins到HDFS上</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -<span class="built_in">mkdir</span> /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put lib/ /flink-dist</span><br><span class="line">[atguigu@hadoop102 flink-1.17.0]$ hadoop fs -put plugins/ /flink-dist</span><br></pre></td></tr></table></figure><h5 id="（2）提交作业"><a href="#（2）提交作业" class="headerlink" title="（2）提交作业"></a>（2）提交作业</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run-application -t yarn-application    -Dyarn.provided.lib.dirs=<span class="string">&quot;hdfs://hadoop102:8020/flink-dist&quot;</span>    -c com.atguigu.wc.SocketStreamWordCount  hdfs://hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink的DataStreamAPI</title>
      <link href="/posts/pd14.html"/>
      <url>/posts/pd14.html</url>
      
        <content type="html"><![CDATA[<h5 id="DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："><a href="#DataStream-API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：" class="headerlink" title="DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成："></a>DataStream API是Flink的核心层API。一个Flink程序，其实就是对DataStream的各种转换。具体来说，代码基本上都由以下几部分构成：</h5><p><img src="https://pic1.imgdb.cn/item/67861fbed0e0a243d4f42964.png"></p><h5 id="在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法："><a href="#在Flink1-12以前，旧的添加source的方式，是调用执行环境的addSource-方法：" class="headerlink" title="在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法："></a>在Flink1.12以前，旧的添加source的方式，是调用执行环境的addSource()方法：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.addSource(...);</span><br></pre></td></tr></table></figure><h5 id="方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。"><a href="#方法传入的参数是一个“源函数”（source-function），需要实现SourceFunction接口。" class="headerlink" title="方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。"></a>方法传入的参数是一个“源函数”（source function），需要实现SourceFunction接口。</h5><h5 id="从Flink1-12开始，主要使用流批统一的新Source架构："><a href="#从Flink1-12开始，主要使用流批统一的新Source架构：" class="headerlink" title="从Flink1.12开始，主要使用流批统一的新Source架构："></a>从Flink1.12开始，主要使用流批统一的新Source架构：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStreamSource</span>&lt;<span class="type">String</span>&gt; stream = env.fromSource(…)</span><br></pre></td></tr></table></figure><h5 id="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"><a href="#Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。" class="headerlink" title="Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。"></a>Flink直接提供了很多预实现的接口，此外还有很多外部连接工具也帮我们实现了对应的Source，通常情况下足以应对我们的实际需求。</h5><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作:"></a>准备工作:</h2><h5 id="为了方便练习，这里使用WaterSensor作为数据模型。"><a href="#为了方便练习，这里使用WaterSensor作为数据模型。" class="headerlink" title="为了方便练习，这里使用WaterSensor作为数据模型。"></a>为了方便练习，这里使用WaterSensor作为数据模型。</h5><table><thead><tr><th><strong>id</strong></th><th><strong>String</strong></th><th><strong>水位传感器类型</strong></th></tr></thead><tbody><tr><td><strong>ts</strong></td><td><strong>Long</strong></td><td><strong>传感器记录时间戳</strong></td></tr><tr><td><strong>vc</strong></td><td><strong>Integer</strong></td><td><strong>水位记录</strong></td></tr></tbody></table><h4 id="具体代码如下："><a href="#具体代码如下：" class="headerlink" title="具体代码如下："></a>具体代码如下：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensor</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String id;</span><br><span class="line">    <span class="keyword">public</span> Long ts;</span><br><span class="line">    <span class="keyword">public</span> Integer vc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">WaterSensor</span><span class="params">(String id, Long ts, Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">getId</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setId</span><span class="params">(String id)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Long <span class="title function_">getTs</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setTs</span><span class="params">(Long ts)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.ts = ts;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">getVc</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setVc</span><span class="params">(Integer vc)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.vc = vc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;WaterSensor&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;id=&#x27;&quot;</span> + id + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, ts=&quot;</span> + ts +</span><br><span class="line">                <span class="string">&quot;, vc=&quot;</span> + vc +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span> == o) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (o == <span class="literal">null</span> || getClass() != o.getClass()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">WaterSensor</span> <span class="variable">that</span> <span class="operator">=</span> (WaterSensor) o;</span><br><span class="line">        <span class="keyword">return</span> Objects.equals(id, that.id) &amp;&amp;</span><br><span class="line">                Objects.equals(ts, that.ts) &amp;&amp;</span><br><span class="line">                Objects.equals(vc, that.vc);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Objects.hash(id, ts, vc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="这里需要注意，我们定义的WaterSensor，有这样几个特点："><a href="#这里需要注意，我们定义的WaterSensor，有这样几个特点：" class="headerlink" title="这里需要注意，我们定义的WaterSensor，有这样几个特点："></a>这里需要注意，我们定义的WaterSensor，有这样几个特点：</h4><ul><li>类是公有（public）的</li><li>有一个无参的构造方法</li><li>所有属性都是公有（public）的</li><li>所有属性的类型都是可以序列化的</li></ul><h6 id="Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"><a href="#Flink会把这样的类作为一种特殊的POJO（Plain-Ordinary-Java-Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。" class="headerlink" title="Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。"></a>Flink会把这样的类作为一种特殊的POJO（Plain Ordinary Java Object简单的Java对象，实际就是普通JavaBeans）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了toString方法，主要是为了测试输出显示更清晰。</h6><h6 id="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"><a href="#我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。" class="headerlink" title="我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。"></a>我们这里自定义的POJO类会在后面的代码中频繁使用，所以在后面的代码中碰到，把这里的POJO类导入就好了。</h6><h3 id="1-从集合、文件、元素中读取数据"><a href="#1-从集合、文件、元素中读取数据" class="headerlink" title="1.从集合、文件、元素中读取数据"></a>1.从集合、文件、元素中读取数据</h3><h5 id="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"><a href="#最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。" class="headerlink" title="最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。"></a>最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境的fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceBoundedTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">user: <span class="type">String</span>, url: <span class="type">String</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从元素中读取数据</span></span><br><span class="line"><span class="comment">//    val stream: DataStream[Int] = env.fromElements(1, 2, 3, 4, 5, 6)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从集合中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> clicks = <span class="type">List</span>(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> stream2: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromCollection(clicks)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从文件中读取数据</span></span><br><span class="line">    <span class="keyword">val</span> stream3: <span class="type">DataStream</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\click.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印输出</span></span><br><span class="line">    stream.print(<span class="string">&quot;从元素中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream2.print(<span class="string">&quot;从集合中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line">    stream3.print(<span class="string">&quot;从文件中&quot;</span>).setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2从Socket读取数据"><a href="#1-2从Socket读取数据" class="headerlink" title="1.2从Socket读取数据"></a>1.2从Socket读取数据</h3><ul><li><h6 id="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"><a href="#不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。" class="headerlink" title="不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。"></a>不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。</h6></li><li><h6 id="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"><a href="#我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。" class="headerlink" title="我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。"></a>我们之前用到的读取socket文本流，就是流处理场景。但是这种方式由于吞吐量小、稳定性较差，一般也是用于测试。</h6></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataStream</span>&lt;<span class="type">String</span>&gt; stream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>);</span><br></pre></td></tr></table></figure><h3 id="1-3从Kafka中读取数据"><a href="#1-3从Kafka中读取数据" class="headerlink" title="1.3从Kafka中读取数据"></a>1.3从Kafka中读取数据</h3><ul><li><h6 id="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"><a href="#Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。" class="headerlink" title="Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。"></a>Flink官方提供了连接工具flink-connector-kafka，直接帮我们实现了一个消费者FlinkKafkaConsumer，它就是用来读取Kafka数据的SourceFunction。</h6></li><li><h6 id="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。"><a href="#所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0-10-0版本以上的Kafka。这里我们需要导入的依赖如下。" class="headerlink" title="所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。"></a>所以想要以Kafka作为数据源获取数据，我们只需要引入Kafka连接器的依赖。Flink官方提供的是一个通用的Kafka连接器，它会自动跟踪最新版本的Kafka客户端。目前最新版本只支持0.10.0版本以上的Kafka。这里我们需要导入的依赖如下。</h6></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下："><a href="#代码如下：" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SourceKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用Properties保存kafka连接的配置</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line"></span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-从数据生成器读取数据"><a href="#1-4-从数据生成器读取数据" class="headerlink" title="1.4 从数据生成器读取数据"></a>1.4 从数据生成器读取数据</h3><h6 id="Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖："><a href="#Flink从1-11开始提供了一个内置的DataGen-连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1-17提供了新的Source写法，需要导入依赖：" class="headerlink" title="Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖："></a>Flink从1.11开始提供了一个内置的DataGen 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。1.17提供了新的Source写法，需要导入依赖：</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-datagen<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="代码如下：-1"><a href="#代码如下：-1" class="headerlink" title="代码如下："></a>代码如下：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DataGeneratorDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataGeneratorSource&lt;String&gt; dataGeneratorSource =</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">DataGeneratorSource</span>&lt;&gt;(</span><br><span class="line">                        <span class="keyword">new</span> <span class="title class_">GeneratorFunction</span>&lt;Long, String&gt;() &#123;</span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="keyword">public</span> String <span class="title function_">map</span><span class="params">(Long value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                                <span class="keyword">return</span> <span class="string">&quot;Number:&quot;</span>+value;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;,</span><br><span class="line">                        Long.MAX_VALUE,</span><br><span class="line">                        RateLimiterStrategy.perSecond(<span class="number">10</span>),</span><br><span class="line">                        Types.STRING</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env</span><br><span class="line">                .fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), <span class="string">&quot;datagenerator&quot;</span>)</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-5Flink支持的数据类型"><a href="#1-5Flink支持的数据类型" class="headerlink" title="1.5Flink支持的数据类型"></a>1.5Flink支持的数据类型</h3><ol><li><h5 id="Flink的类型系统"><a href="#Flink的类型系统" class="headerlink" title="Flink的类型系统"></a>Flink的类型系统</h5><p>Flink使用“类型信息”（TypeInformation）来统一表示数据类型。TypeInformation类是Flink中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。</p></li><li><h5 id="Flink支持的数据类型"><a href="#Flink支持的数据类型" class="headerlink" title="Flink支持的数据类型"></a>Flink支持的数据类型</h5><p>对于常见的Java和Scala数据类型，Flink都是支持的。Flink在内部，Flink对支持不同的类型进行了划分，这些类型可以在Types工具类中找到：</p></li></ol><ul><li><h5 id="（1）基本类型"><a href="#（1）基本类型" class="headerlink" title="（1）基本类型"></a>（1）基本类型</h5><p>所有Java基本类型及其包装类，再加上Void、String、Date、BigDecimal和BigInteger。</p></li><li><h5 id="（2）数组类型"><a href="#（2）数组类型" class="headerlink" title="（2）数组类型"></a>（2）数组类型</h5><p>包括基本类型数组（PRIMITIVE_ARRAY）和对象数组（OBJECT_ARRAY）。</p></li><li><h5 id="（3）复合数据类型"><a href="#（3）复合数据类型" class="headerlink" title="（3）复合数据类型"></a>（3）复合数据类型</h5><ol><li><p>Java元组类型（TUPLE）：这是Flink内置的元组类型，是Java API的一部分。最多25个字段，也就是从Tuple0~Tuple25，不支持空字段。</p></li><li><h5 id="Scala-样例类及Scala元组：不支持空字段。"><a href="#Scala-样例类及Scala元组：不支持空字段。" class="headerlink" title="Scala 样例类及Scala元组：不支持空字段。"></a>Scala 样例类及Scala元组：不支持空字段。</h5></li><li><h5 id="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"><a href="#行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。" class="headerlink" title="行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。"></a>行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。</h5></li><li><h5 id="POJO：Flink自定义的类似于Java-bean模式的类。"><a href="#POJO：Flink自定义的类似于Java-bean模式的类。" class="headerlink" title="POJO：Flink自定义的类似于Java bean模式的类。"></a>POJO：Flink自定义的类似于Java bean模式的类。</h5></li></ol></li></ul><h4 id="（4）辅助类型"><a href="#（4）辅助类型" class="headerlink" title="（4）辅助类型"></a>（4）辅助类型</h4><ul><li><h5 id="Option、Either、List、Map等。"><a href="#Option、Either、List、Map等。" class="headerlink" title="Option、Either、List、Map等。"></a>Option、Either、List、Map等。</h5></li></ul><h4 id="（5）泛型类型（GENERIC）"><a href="#（5）泛型类型（GENERIC）" class="headerlink" title="（5）泛型类型（GENERIC）"></a>（5）泛型类型（GENERIC）</h4><ul><li><h6 id="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"><a href="#Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。" class="headerlink" title="Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。"></a>Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。</h6></li><li><h6 id="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"><a href="#在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。" class="headerlink" title="在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。"></a>在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。</h6></li><li><h6 id="Flink对POJO类型的要求如下："><a href="#Flink对POJO类型的要求如下：" class="headerlink" title="Flink对POJO类型的要求如下："></a>Flink对POJO类型的要求如下：</h6><ol><li><h6 id="类是公有（public）的"><a href="#类是公有（public）的" class="headerlink" title="类是公有（public）的"></a>类是公有（public）的</h6></li><li><h6 id="有一个无参的构造方法"><a href="#有一个无参的构造方法" class="headerlink" title="有一个无参的构造方法"></a>有一个无参的构造方法</h6></li><li><h6 id="所有属性都是公有（public）的"><a href="#所有属性都是公有（public）的" class="headerlink" title="所有属性都是公有（public）的"></a>所有属性都是公有（public）的</h6></li><li><h6 id="所有属性的类型都是可以序列化的"><a href="#所有属性的类型都是可以序列化的" class="headerlink" title="所有属性的类型都是可以序列化的"></a>所有属性的类型都是可以序列化的</h6></li></ol></li></ul><h4 id="3）类型提示（Type-Hints）"><a href="#3）类型提示（Type-Hints）" class="headerlink" title="3）类型提示（Type Hints）"></a>3）类型提示（Type Hints）</h4><h6 id="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"><a href="#Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。" class="headerlink" title="Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。"></a>Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。</h6><h6 id="为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。"><a href="#为了解决这类问题，Java-API提供了专门的“类型提示”（type-hints）。" class="headerlink" title="为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。"></a>为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。</h6><h6 id="回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"><a href="#回忆一下之前的word-count流处理程序，我们在将String类型的每个词转换成（word，-count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2-lt-String-Long-gt-。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。" class="headerlink" title="回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。"></a>回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2&lt;String, Long&gt;。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.map(word -&gt; <span class="type">Tuple2</span>.of(word, <span class="number">1</span>L))</span><br><span class="line">.returns(<span class="type">Types</span>.<span class="type">TUPLE</span>(<span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">LONG</span>));</span><br></pre></td></tr></table></figure><h6 id="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。"><a href="#Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过-returns-方法，明确地指定转换之后的DataStream里元素的类型。" class="headerlink" title="Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。"></a>Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">returns(<span class="keyword">new</span> <span class="type">TypeHint</span>&lt;<span class="type">Tuple2</span>&lt;<span class="type">Integer</span>, <span class="type">SomeType</span>&gt;&gt;()&#123;&#125;)</span><br></pre></td></tr></table></figure><h3 id="转换算子（Transformation）"><a href="#转换算子（Transformation）" class="headerlink" title="转换算子（Transformation）"></a>转换算子（Transformation）</h3><h5 id="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"><a href="#数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。" class="headerlink" title="数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。"></a>数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个DataStream转换为新的DataStream。</h5><p><img src="https://pic1.imgdb.cn/item/678624a7d0e0a243d4f42b7c.png"></p><h3 id="1-基本转换算子（map-x2F-filter-x2F-flatMap）"><a href="#1-基本转换算子（map-x2F-filter-x2F-flatMap）" class="headerlink" title="1.基本转换算子（map&#x2F; filter&#x2F; flatMap）"></a>1.基本转换算子（map&#x2F; filter&#x2F; flatMap）</h3><h4 id="1-1-1映射（map）"><a href="#1-1-1映射（map）" class="headerlink" title="1.1.1映射（map）"></a>1.1.1映射（map）</h4><h6 id="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"><a href="#map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。" class="headerlink" title="map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。"></a>map是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素。</h6><p><img src="https://pic1.imgdb.cn/item/6786250bd0e0a243d4f42bac.png"></p><h6 id="我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"><a href="#我们只需要基于DataStream调用map-方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。" class="headerlink" title="我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。"></a>我们只需要基于DataStream调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction的实现；返回值类型还是DataStream，不过泛型（流中的元素类型）可能改变。</h6><h6 id="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"><a href="#下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。" class="headerlink" title="下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。"></a>下面的代码用不同的方式，实现了提取WaterSensor中的id字段的功能。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">MapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提取每次点击事件的用户名</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.map( _.user ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现mapFunction接口</span></span><br><span class="line">    stream.map(<span class="keyword">new</span> <span class="type">UserExtractor</span>).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserExtractor</span> <span class="keyword">extends</span> <span class="title">MapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"><a href="#上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map-方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。" class="headerlink" title="上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。"></a>上面代码中，MapFunction实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现MapFunction接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。</h6><h3 id="1-1-2过滤（filter）"><a href="#1-1-2过滤（filter）" class="headerlink" title="1.1.2过滤（filter）"></a>1.1.2过滤（filter）</h3><h5 id="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"><a href="#filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。" class="headerlink" title="filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。"></a>filter转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为true则元素正常输出，若为false则元素被过滤掉。</h5><p><img src="https://pic1.imgdb.cn/item/6786255dd0e0a243d4f42bd8.png"></p><h6 id="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。"><a href="#进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter-方法，就相当于一个返回布尔类型的条件表达式。" class="headerlink" title="进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。"></a>进行filter转换之后的新数据流的数据类型与原数据流是相同的。filter转换需要传入的参数需要实现FilterFunction接口，而FilterFunction内要实现filter()方法，就相当于一个返回布尔类型的条件表达式。</h6><h6 id="案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。"><a href="#案例需求：下面的代码会将数据流中传感器id为sensor-1的数据过滤出来。" class="headerlink" title="案例需求：下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。"></a><strong>案例需求：</strong>下面的代码会将数据流中传感器id为sensor_1的数据过滤出来。</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFilterTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过滤出用户为Marry的所有点击事件</span></span><br><span class="line">    <span class="comment">// 1.使用匿名函数</span></span><br><span class="line">    stream.filter( _.user == <span class="string">&quot;hkj&quot;</span> ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.实现FilterFunction</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">UserFilter</span> ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">UserFilter</span> <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.user == <span class="string">&quot;Bob&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-1-3-扁平映射（flatMap）"><a href="#1-1-3-扁平映射（flatMap）" class="headerlink" title="1.1.3 扁平映射（flatMap）"></a>1.1.3 扁平映射（flatMap）</h4><h6 id="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"><a href="#flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。" class="headerlink" title="flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。"></a>flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生0到多个元素。flatMap可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理。</h6><p><img src="https://pic1.imgdb.cn/item/6786259fd0e0a243d4f42bf4.png"></p><h6 id="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"><a href="#同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。" class="headerlink" title="同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。"></a>同map一样，flatMap也可以使用Lambda表达式或者FlatMapFunction接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流相同，也可以不同。</h6><h6 id="案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。"><a href="#案例需求：如果输入的数据是sensor-1，只打印vc；如果输入的数据是sensor-2，既打印ts又打印vc。" class="headerlink" title="案例需求：如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。"></a><strong>案例需求：</strong>如果输入的数据是sensor_1，只打印vc；如果输入的数据是sensor_2，既打印ts又打印vc。</h6><h6 id="实现代码如下："><a href="#实现代码如下：" class="headerlink" title="实现代码如下："></a>实现代码如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FlatMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformFlatMapTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试灵活输出形式</span></span><br><span class="line">    stream.flatMap(<span class="keyword">new</span> <span class="type">MyFlatMap</span>).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// lam表达式</span></span><br><span class="line">    <span class="keyword">val</span> stream1 = env.readTextFile(<span class="string">&quot;G:\\Flink学习\\FlinkStu\\data\\words.txt&quot;</span>)</span><br><span class="line">    stream1.flatMap( value =&gt; value.split(<span class="string">&quot;,&quot;</span>) ).map(value =&gt; (value, <span class="number">1</span>)).print(<span class="string">&quot;stream1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义实现flatMapFunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">FlatMapFunction</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>(value: <span class="type">Event</span>, out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 如果当前数据是hkj的点击事件那就直接输出user</span></span><br><span class="line">      <span class="keyword">if</span> (value.user == <span class="string">&quot;hkj&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 如果当前数据是Bob的点击事件，那么就输出user和url</span></span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (value.user == <span class="string">&quot;Bob&quot;</span>) &#123;</span><br><span class="line">        out.collect(value.user)</span><br><span class="line">        out.collect(value.url)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2聚合算子（Aggregation）"><a href="#1-2聚合算子（Aggregation）" class="headerlink" title="1.2聚合算子（Aggregation）"></a>1.2聚合算子（Aggregation）</h3><h5 id="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"><a href="#计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。" class="headerlink" title="计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。"></a>计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（Aggregation），类似于MapReduce中的reduce操作。</h5><h4 id="1-2-1-按键分区（keyBy）"><a href="#1-2-1-按键分区（keyBy）" class="headerlink" title="1.2.1 按键分区（keyBy）"></a>1.2.1 按键分区（keyBy）</h4><ul><li><h6 id="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"><a href="#对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。" class="headerlink" title="对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。"></a>对于Flink而言，DataStream是没有直接进行聚合的API的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在Flink中，要做聚合，需要先进行分区；这个操作就是通过keyBy来完成的。</h6></li><li><h6 id="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"><a href="#keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。" class="headerlink" title="keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。"></a>keyBy是聚合前必须要用到的一个算子。keyBy通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务。</h6></li><li><h6 id="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"><a href="#基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。" class="headerlink" title="基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。"></a>基于不同的key，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的key的数据，都将被发往同一个分区。</h6></li></ul><p><img src="https://pic1.imgdb.cn/item/67862611d0e0a243d4f42c2f.png"></p><h6 id="在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。"><a href="#在内部，是通过计算key的哈希值（hash-code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode-方法。" class="headerlink" title="在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。"></a>在内部，是通过计算key的哈希值（hash code），对分区数进行取模运算来实现的。所以这里key如果是POJO的话，必须要重写hashCode()方法。</h6><h6 id="keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"><a href="#keyBy-方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。" class="headerlink" title="keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。"></a>keyBy()方法需要传入一个参数，这个参数指定了一个或一组key。有很多不同的方法来指定key：比如对于Tuple数据类型，可以指定字段的位置或者多个位置的组合；对于POJO类型，可以指定字段的名称（String）；另外，还可以传入Lambda表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取key的逻辑。</h6><h6 id="我们可以以id作为key做一个分区操作，代码实现如下："><a href="#我们可以以id作为key做一个分区操作，代码实现如下：" class="headerlink" title="我们可以以id作为key做一个分区操作，代码实现如下："></a>我们可以以id作为key做一个分区操作，代码实现如下：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.<span class="type">KeySelector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformKeyByTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.keyBy( <span class="keyword">new</span> <span class="type">MyKeySelector</span> )</span><br><span class="line">      .maxBy(<span class="string">&quot;timestamp&quot;</span>)</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line"><span class="comment">//    stream.keyBy( _.user).print(&quot;lam&quot;)</span></span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyKeySelector</span> <span class="keyword">extends</span> <span class="title">KeySelector</span>[<span class="type">Event</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getKey</span></span>(value: <span class="type">Event</span>): <span class="type">String</span> = value.user</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"><a href="#需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。" class="headerlink" title="需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。"></a>需要注意的是，keyBy得到的结果将不再是DataStream，而是会将DataStream转换为KeyedStream。KeyedStream可以认为是“分区流”或者“键控流”，它是对DataStream按照key的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。</h6><h6 id="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"><a href="#KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream-API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。" class="headerlink" title="KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。"></a>KeyedStream也继承自DataStream，所以基于它的操作也都归属于DataStream API。但它跟之前的转换操作得到的SingleOutputStreamOperator不同，只是一个流的分区操作，并不是一个转换算子。KeyedStream是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。</h6><h4 id="1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）"><a href="#1-2-2-简单聚合（sum-x2F-min-x2F-max-x2F-minBy-x2F-maxBy）" class="headerlink" title="1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）"></a>1.2.2 简单聚合（sum&#x2F;min&#x2F;max&#x2F;minBy&#x2F;maxBy）</h4><h6 id="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："><a href="#有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：" class="headerlink" title="有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种："></a>有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：</h6><ol><li><h6 id="sum-：在输入流上，对指定的字段做叠加求和的操作。"><a href="#sum-：在输入流上，对指定的字段做叠加求和的操作。" class="headerlink" title="sum()：在输入流上，对指定的字段做叠加求和的操作。"></a>sum()：在输入流上，对指定的字段做叠加求和的操作。</h6></li><li><h6 id="min-：在输入流上，对指定的字段求最小值。"><a href="#min-：在输入流上，对指定的字段求最小值。" class="headerlink" title="min()：在输入流上，对指定的字段求最小值。"></a>min()：在输入流上，对指定的字段求最小值。</h6></li><li><h6 id="max-：在输入流上，对指定的字段求最大值。"><a href="#max-：在输入流上，对指定的字段求最大值。" class="headerlink" title="max()：在输入流上，对指定的字段求最大值。"></a>max()：在输入流上，对指定的字段求最大值。</h6></li><li><h6 id="minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。"><a href="#minBy-：与min-类似，在输入流上针对指定字段求最小值。不同的是，min-只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy-则会返回包含字段最小值的整条数据。" class="headerlink" title="minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。"></a>minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。</h6></li><li><h6 id="maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。"><a href="#maxBy-：与max-类似，在输入流上针对指定字段求最大值。两者区别与min-x2F-minBy-完全一致。" class="headerlink" title="maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。"></a>maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()&#x2F;minBy()完全一致。</h6></li></ol><h5 id="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"><a href="#简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。" class="headerlink" title="简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。"></a>简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。</h5><h6 id="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"><a href="#对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。" class="headerlink" title="对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。"></a>对于元组类型的数据，可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以f0、f1、f2、…来命名的。</h6><h6 id="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"><a href="#如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。" class="headerlink" title="如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。"></a>如果数据流的类型是POJO类，那么就只能通过字段名称来指定，不能通过位置来指定了。</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransAggregation</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;WaterSensor&gt; stream = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_2&quot;</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">new</span> <span class="title class_">WaterSensor</span>(<span class="string">&quot;sensor_3&quot;</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        stream.keyBy(e -&gt; e.id).max(<span class="string">&quot;vc&quot;</span>);    <span class="comment">// 指定字段名称</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"><a href="#简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。" class="headerlink" title="简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。"></a>简单聚合算子返回的，同样是一个SingleOutputStreamOperator，也就是从KeyedStream又转换成了常规的DataStream。所以可以这样理解：keyBy和聚合是成对出现的，先分区、后聚合，得到的依然是一个DataStream。而且经过简单聚合之后的数据流，元素的数据类型保持不变。</h5><h5 id="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"><a href="#一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。" class="headerlink" title="一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。"></a>一个聚合算子，会为每一个key保存一个聚合的值，在Flink中我们把它叫作“状态”（state）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个key的数据流上。</h5><h4 id="1-2-3-归约聚合（reduce）"><a href="#1-2-3-归约聚合（reduce）" class="headerlink" title="1.2.3 归约聚合（reduce）"></a>1.2.3 归约聚合（reduce）</h4><h5 id="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"><a href="#reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。" class="headerlink" title="reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。"></a>reduce可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。</h5><h5 id="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"><a href="#reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。" class="headerlink" title="reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。"></a>reduce操作也会将KeyedStream转换为DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。</h5><h5 id="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："><a href="#调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：" class="headerlink" title="调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下："></a>调用KeyedStream的reduce方法时，需要传入一个参数，实现ReduceFunction接口。接口在源码中的定义如下：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ReduceFunction</span>&lt;T&gt; <span class="keyword">extends</span> <span class="title class_">Function</span>, Serializable &#123;</span><br><span class="line">    T <span class="title function_">reduce</span><span class="params">(T value1, T value2)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"><a href="#ReduceFunction接口里需要实现reduce-方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。" class="headerlink" title="ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。"></a>ReduceFunction接口里需要实现reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。</h5><h5 id="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"><a href="#我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。" class="headerlink" title="我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。"></a>我们可以单独定义一个函数类实现ReduceFunction接口，也可以直接传入一个匿名类。当然，同样也可以通过传入Lambda表达式实现类似的功能。</h5><h5 id="为了方便后续使用，定义一个WaterSensorMapFunction："><a href="#为了方便后续使用，定义一个WaterSensorMapFunction：" class="headerlink" title="为了方便后续使用，定义一个WaterSensorMapFunction："></a>为了方便后续使用，定义一个WaterSensorMapFunction：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WaterSensorMapFunction</span> <span class="keyword">implements</span> <span class="title class_">MapFunction</span>&lt;String,WaterSensor&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> WaterSensor <span class="title function_">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] datas = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WaterSensor</span>(datas[<span class="number">0</span>],Long.valueOf(datas[<span class="number">1</span>]) ,Integer.valueOf(datas[<span class="number">2</span>]) );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="案例：使用reduce实现max和maxBy的功能。"><a href="#案例：使用reduce实现max和maxBy的功能。" class="headerlink" title="案例：使用reduce实现max和maxBy的功能。"></a>案例：使用reduce实现max和maxBy的功能。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">ReduceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformReduceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduce归约聚合,提取当前最活跃用户</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    data =&gt; true 的作用</span></span><br><span class="line"><span class="comment">//    1.keyBy 的作用：keyBy 是 Flink 中的一个算子，用于将数据流按照指定的键进行分组。</span></span><br><span class="line"><span class="comment">//                  分组后，相同键的数据会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    2.data =&gt; true 的含义：</span></span><br><span class="line"><span class="comment">//              这里的 data =&gt; true 是一个匿名函数，它对每条数据返回固定的 true 值。</span></span><br><span class="line"><span class="comment">//              由于所有数据的键都是 true，因此所有数据都会被分配到同一个分区中。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    3.为什么需要 data =&gt; true：</span></span><br><span class="line"><span class="comment">//          在统计最活跃用户时，需要将所有用户的活跃度数据集中到一个分区中，才能进行比较和筛选。</span></span><br><span class="line"><span class="comment">//          如果不使用 keyBy(data =&gt; true)，数据会分散在多个分区中，无法直接进行比较。</span></span><br><span class="line"></span><br><span class="line">    stream.map( data =&gt; (data.user, <span class="number">1</span>L) )</span><br><span class="line">      .keyBy(_._1)</span><br><span class="line">      .reduce( <span class="keyword">new</span> <span class="type">MySum</span> )  <span class="comment">// 统计每个用户的活跃度</span></span><br><span class="line">      .keyBy(data =&gt; <span class="literal">true</span>)  <span class="comment">// 将所有数据按照同样的key分到同一个组中</span></span><br><span class="line">      .reduce( (state, data) =&gt; <span class="keyword">if</span> (data._2 &gt;= state._2) data <span class="keyword">else</span> state ) <span class="comment">// 选取当前最活跃的用户</span></span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MySum</span> <span class="keyword">extends</span> <span class="title">ReduceFunction</span>[(<span class="type">String</span>, <span class="type">Long</span>)] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(value1: (<span class="type">String</span>, <span class="type">Long</span>), value2: (<span class="type">String</span>, <span class="type">Long</span>)): (<span class="type">String</span>, <span class="type">Long</span>) = (value1._1, value2._2 + value1._2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"><a href="#reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。" class="headerlink" title="reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。"></a>reduce同简单聚合算子一样，也要针对每一个key保存状态。因为状态不会清空，所以我们需要将reduce算子作用在一个有限key的流上。</h5><h3 id="1-3-3-用户自定义函数（UDF）"><a href="#1-3-3-用户自定义函数（UDF）" class="headerlink" title="1.3.3 用户自定义函数（UDF）"></a>1.3.3 用户自定义函数（UDF）</h3><h5 id="用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"><a href="#用户自定义函数（user-defined-function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。" class="headerlink" title="用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。"></a>用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。</h5><h5 id="用户自定义函数分为：函数类、匿名函数、富函数类。"><a href="#用户自定义函数分为：函数类、匿名函数、富函数类。" class="headerlink" title="用户自定义函数分为：函数类、匿名函数、富函数类。"></a>用户自定义函数分为：函数类、匿名函数、富函数类。</h5><h4 id="需求：用来从用户的点击数据中筛选包含“sensor-1”的内容："><a href="#需求：用来从用户的点击数据中筛选包含“sensor-1”的内容：" class="headerlink" title="需求：用来从用户的点击数据中筛选包含“sensor_1”的内容："></a><strong>需求：</strong>用来从用户的点击数据中筛选包含“sensor_1”的内容：</h4><h4 id="方式一：实现FilterFunction接口"><a href="#方式一：实现FilterFunction接口" class="headerlink" title="方式一：实现FilterFunction接口"></a><strong>方式一：</strong>实现FilterFunction接口</h4><h4 id="方式二：通过匿名类来实现FilterFunction接口："><a href="#方式二：通过匿名类来实现FilterFunction接口：" class="headerlink" title="方式二：通过匿名类来实现FilterFunction接口："></a><strong>方式二：</strong>通过匿名类来实现FilterFunction接口：</h4><h5 id="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"><a href="#方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。" class="headerlink" title="方式二的优化：为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。"></a><strong>方式二的优化：</strong>为了类可以更加通用，我们还可以将用于过滤的关键字”home”抽象出来作为类的属性，调用构造方法时传进去。</h5><h5 id="方式三：采用匿名函数（Lambda）"><a href="#方式三：采用匿名函数（Lambda）" class="headerlink" title="方式三：采用匿名函数（Lambda）"></a><strong>方式三：</strong>采用匿名函数（Lambda）</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">FilterFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformUDFTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试UDF的用法,筛选url中包含某个关键字hkjcpdd的Event事件</span></span><br><span class="line">    <span class="comment">// 1.实现一个自定义的函数类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">MyFilterFunction</span>(<span class="string">&quot;hkjcpdd&quot;</span>) ).print(<span class="string">&quot;1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 使用匿名类</span></span><br><span class="line">    stream.filter( <span class="keyword">new</span> <span class="type">FilterFunction</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(<span class="string">&quot;hkjmjj&quot;</span>)</span><br><span class="line">    &#125; ).print(<span class="string">&quot;2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 使用lambda表达式</span></span><br><span class="line">    stream.filter( _.url.contains(<span class="string">&quot;hkjmjj&quot;</span>) ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 实现自定义个filterfunction</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyFilterFunction</span>(<span class="params">keyWord: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">FilterFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// contains是指是否包含某些字段</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">Event</span>): <span class="type">Boolean</span> = value.url.contains(keyWord)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-2-富函数类（Rich-Function-Classes）"><a href="#1-3-2-富函数类（Rich-Function-Classes）" class="headerlink" title="1.3.2 富函数类（Rich Function Classes）"></a>1.3.2 富函数类（Rich Function Classes）</h4><h6 id="“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"><a href="#“富函数类”也是DataStream-API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。" class="headerlink" title="“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。"></a>“富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。</h6><h6 id="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"><a href="#与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。" class="headerlink" title="与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。"></a>与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。</h6><h6 id="Rich-Function有生命周期的概念。典型的生命周期方法有："><a href="#Rich-Function有生命周期的概念。典型的生命周期方法有：" class="headerlink" title="Rich Function有生命周期的概念。典型的生命周期方法有："></a>Rich Function有生命周期的概念。典型的生命周期方法有：</h6><ul><li><h6 id="open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。"><a href="#open-方法，是Rich-Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map-或者filter-方法被调用之前，open-会首先被调用。" class="headerlink" title="open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。"></a>open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。</h6></li><li><h6 id="close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"><a href="#close-方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。" class="headerlink" title="close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。"></a>close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。</h6></li></ul><h6 id="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。"><a href="#需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map-，在每条数据到来后都会触发一次调用。" class="headerlink" title="需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。"></a>需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。</h6><h6 id="来看一个例子说明："><a href="#来看一个例子说明：" class="headerlink" title="来看一个例子说明："></a>来看一个例子说明：</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RichMapFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformRichFunctionTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义一个RichMapFunction，测试复函数类的功能</span></span><br><span class="line">    stream.map( <span class="keyword">new</span> <span class="type">MyRichMap</span>() ).print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyRichMap</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">RichMapFunction</span>[<span class="type">Event</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务开始&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(value: <span class="type">Event</span>): <span class="type">Long</span> = value.timestamp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;索引号为：&quot;</span> + getRuntimeContext.getIndexOfThisSubtask + <span class="string">&quot;的任务结束&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-3-4-物理分区算子（Physical-Partitioning）"><a href="#5-3-4-物理分区算子（Physical-Partitioning）" class="headerlink" title="5.3.4 物理分区算子（Physical Partitioning）"></a>5.3.4 物理分区算子（Physical Partitioning）</h3><h5 id="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"><a href="#常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。" class="headerlink" title="常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。"></a>常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。</h5><h3 id="1-4-1-随机分区（shuffle）"><a href="#1-4-1-随机分区（shuffle）" class="headerlink" title="1.4.1 随机分区（shuffle）"></a>1.4.1 随机分区（shuffle）</h3><h5 id="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。"><a href="#最简单的重分区方式就是直接“洗牌”。通过调用DataStream的-shuffle-方法，将数据随机地分配到下游算子的并行任务中去。" class="headerlink" title="最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。"></a>最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。</h5><h5 id="随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。"><a href="#随机分区服从均匀分布（uniform-distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据-每次执行得到的结果也不会相同。" class="headerlink" title="随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。"></a>随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。</h5><p><img src="https://pic1.imgdb.cn/item/67862896d0e0a243d4f42d1b.png"></p><h5 id="经过随机分区之后，得到的依然是一个DataStream。"><a href="#经过随机分区之后，得到的依然是一个DataStream。" class="headerlink" title="经过随机分区之后，得到的依然是一个DataStream。"></a>经过随机分区之后，得到的依然是一个DataStream。</h5><h5 id="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"><a href="#我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。" class="headerlink" title="我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。"></a>我们可以做个简单测试：将数据读入之后直接打印到控制台，将输出的并行度设置为2，中间经历一次shuffle。执行多次，观察结果是否相同。</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShuffleExample</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"> env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; stream = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>);;</span><br><span class="line"></span><br><span class="line">        stream.shuffle().print()</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-4-2-轮询分区（Round-Robin）"><a href="#1-4-2-轮询分区（Round-Robin）" class="headerlink" title="1.4.2 轮询分区（Round-Robin）"></a>1.4.2 轮询分区（Round-Robin）</h4><h5 id="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"><a href="#轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的-rebalance-方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。" class="headerlink" title="轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。"></a>轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。</h5><p><img src="https://pic1.imgdb.cn/item/678628c7d0e0a243d4f42d3e.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rebalance()</span><br></pre></td></tr></table></figure><h4 id="1-4-3-重缩放分区（rescale）"><a href="#1-4-3-重缩放分区（rescale）" class="headerlink" title="1.4.3 重缩放分区（rescale）"></a>1.4.3 重缩放分区（rescale）</h4><h5 id="重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"><a href="#重缩放分区和轮询分区非常相似。当调用rescale-方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。" class="headerlink" title="重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。"></a>重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。</h5><p><img src="https://pic1.imgdb.cn/item/678628e9d0e0a243d4f42d59.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.rescale()</span><br></pre></td></tr></table></figure><h4 id="1-4-4-广播（broadcast）"><a href="#1-4-4-广播（broadcast）" class="headerlink" title="1.4.4 广播（broadcast）"></a>1.4.4 广播（broadcast）</h4><h5 id="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。"><a href="#这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast-方法，将输入数据复制并发送到下游算子的所有并行任务中去。" class="headerlink" title="这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。"></a>这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.broadcast()</span><br></pre></td></tr></table></figure><h4 id="1-4-5-全局分区（global）"><a href="#1-4-5-全局分区（global）" class="headerlink" title="1.4.5 全局分区（global）"></a>1.4.5 全局分区（global）</h4><h5 id="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"><a href="#全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用-global-方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。" class="headerlink" title="全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。"></a>全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.global()</span><br></pre></td></tr></table></figure><h4 id="1-4-6-自定义分区（Custom）"><a href="#1-4-6-自定义分区（Custom）" class="headerlink" title="1.4.6 自定义分区（Custom）"></a>1.4.6 自定义分区（Custom）</h4><h5 id="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。"><a href="#当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom-方法来自定义分区策略。" class="headerlink" title="当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。"></a>当Flink提供的所有分区策略都不能满足用户的需求时，我们可以通过使用partitionCustom()方法来自定义分区策略。</h5><h4 id="1-自定义分区器"><a href="#1-自定义分区器" class="headerlink" title="1)自定义分区器"></a>1)自定义分区器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.&#123;<span class="type">ParallelSourceFunction</span>, <span class="type">SourceFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Calendar</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//SourceFunction是并行度1的</span></span><br><span class="line"><span class="comment">//ParallelSourceFunction就是并行多的</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClickSource</span> <span class="keyword">extends</span> <span class="title">ParallelSourceFunction</span>[<span class="type">Event</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 标志位</span></span><br><span class="line">  <span class="keyword">var</span> running = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">Event</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 随机数生成器</span></span><br><span class="line">    <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line">    <span class="comment">// 定义数据随机选择的范围</span></span><br><span class="line">    <span class="keyword">val</span> users = <span class="type">Array</span>(<span class="string">&quot;Marry&quot;</span>, <span class="string">&quot;Hkj&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Cary&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> urls = <span class="type">Array</span>(<span class="string">&quot;./home&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="string">&quot;./fav&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>,<span class="string">&quot;./prod?id=3&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关键流程， 用标志位作为循环的判断条件，不停的发出数据</span></span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line">      <span class="keyword">val</span> event = <span class="type">Event</span>(</span><br><span class="line">        users(random.nextInt(users.length)),</span><br><span class="line">        urls(random.nextInt(users.length)),</span><br><span class="line">        <span class="type">Calendar</span>.getInstance.getTimeInMillis</span><br><span class="line">      )</span><br><span class="line"><span class="comment">//      // 为要发送的数据分配时间戳</span></span><br><span class="line"><span class="comment">//      ctx.collectWithTimestamp(event, event.timestamp)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//      // 向下游直接发送水位线</span></span><br><span class="line"><span class="comment">//      ctx.emitWatermark(new Watermark(event.timestamp - 1L))</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 调用ctx的方法向下游发送数据</span></span><br><span class="line">      ctx.collect(event)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 每隔一秒发送过一条数据</span></span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2）使用自定义分区器"><a href="#2）使用自定义分区器" class="headerlink" title="2）使用自定义分区器"></a>2）使用自定义分区器</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PartitionReblanceTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取自定义的数据流</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">ClickSource</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 轮询重分区之后打印输出</span></span><br><span class="line">    stream.rebalance.print(<span class="string">&quot;shuffle&quot;</span>).setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-5分流"><a href="#1-3-5分流" class="headerlink" title="1.3.5分流"></a>1.3.5分流</h3><h4 id="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"><a href="#所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。" class="headerlink" title="所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。"></a>所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。</h4><p><img src="https://pic1.imgdb.cn/item/6786298ad0e0a243d4f42daf.png"></p><h4 id="1-3-5-1-简单实现"><a href="#1-3-5-1-简单实现" class="headerlink" title="1.3.5.1 简单实现"></a>1.3.5.1 简单实现</h4><p>其实根据条件筛选数据的需求，本身非常容易实现：只要针对同一条流多次独立调用.filter()方法进行筛选，就可以得到拆分之后的流了。</p><p><strong>案例需求：</strong>读取一个整数数字流，将数据流划分为奇数流和偶数流。</p><p><strong>代码实现：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByFilter</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">      </span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                                                           .map(Integer::valueOf);</span><br><span class="line">        <span class="comment">//将ds 分为两个流 ，一个是奇数流，一个是偶数流</span></span><br><span class="line">        <span class="comment">//使用filter 过滤两次</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds1 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">0</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; ds2 = ds.filter(x -&gt; x % <span class="number">2</span> == <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;偶数&quot;</span>);</span><br><span class="line">        ds2.print(<span class="string">&quot;奇数&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"><a href="#这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？" class="headerlink" title="这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？"></a>这种实现非常简单，但代码显得有些冗余——我们的处理逻辑对拆分出的三条流其实是一样的，却重复写了三次。而且这段代码背后的含义，是将原始数据流stream复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢？</h5><h4 id="1-3-5-2-使用侧输出流"><a href="#1-3-5-2-使用侧输出流" class="headerlink" title="1.3.5.2 使用侧输出流"></a>1.3.5.2 使用侧输出流</h4><p>关于处理函数中侧输出流的用法，我们已经在7.5节做了详细介绍。简单来说，只需要调用上下文ctx的.output()方法，就可以输出任意类型的数据了。而侧输出流的标记和提取，都离不开一个“输出标签”（OutputTag），指定了侧输出流的id和类型。</p><p><strong>代码实现：</strong>将WaterSensor按照Id类型进行分流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SplitStreamByOutputTag</span> &#123;    </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">              .map(<span class="keyword">new</span> <span class="title class_">WaterSensorMapFunction</span>());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s1 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s1&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">        OutputTag&lt;WaterSensor&gt; s2 = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;&gt;(<span class="string">&quot;s2&quot;</span>, Types.POJO(WaterSensor.class))&#123;&#125;;</span><br><span class="line">       <span class="comment">//返回的都是主流</span></span><br><span class="line">        SingleOutputStreamOperator&lt;WaterSensor&gt; ds1 = ds.process(<span class="keyword">new</span> <span class="title class_">ProcessFunction</span>&lt;WaterSensor, WaterSensor&gt;()</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement</span><span class="params">(WaterSensor value, Context ctx, Collector&lt;WaterSensor&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (<span class="string">&quot;s1&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s1, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;s2&quot;</span>.equals(value.getId())) &#123;</span><br><span class="line">                    ctx.output(s2, value);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">//主流</span></span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        ds1.print(<span class="string">&quot;主流，非s1,s2的传感器&quot;</span>);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s1DS = ds1.getSideOutput(s1);</span><br><span class="line">        SideOutputDataStream&lt;WaterSensor&gt; s2DS = ds1.getSideOutput(s2);</span><br><span class="line"></span><br><span class="line">        s1DS.printToErr(<span class="string">&quot;s1&quot;</span>);</span><br><span class="line">        s2DS.printToErr(<span class="string">&quot;s2&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        env.execute();</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3-6-基本合流操作"><a href="#1-3-6-基本合流操作" class="headerlink" title="1.3.6 基本合流操作"></a><strong>1.3.6</strong> <strong>基本合流操作</strong></h3><h5 id="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"><a href="#在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。" class="headerlink" title="在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。"></a>在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以Flink中合流的操作会更加普遍，对应的API也更加丰富。</h5><h3 id="1-3-6-1"><a href="#1-3-6-1" class="headerlink" title="1.3.6.1"></a>1.3.6.1</h3><h4 id="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"><a href="#最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。" class="headerlink" title="最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。"></a>最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。</h4><p><img src="https://pic1.imgdb.cn/item/6786448fd0e0a243d4f434cf.png"></p><h5 id="在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："><a href="#在代码中，我们只要基于DataStream直接调用-union-方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：" class="headerlink" title="在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream："></a>在代码中，我们只要基于DataStream直接调用.union()方法，传入其他DataStream作为参数，就可以实现流的联合了；得到的依然是一个DataStream：</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream1.union(stream2, stream3, ...)</span><br></pre></td></tr></table></figure><h4 id="注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"><a href="#注意：union-的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。" class="headerlink" title="注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。"></a>注意：union()的参数可以是多个DataStream，所以联合操作可以实现多条流的合并。</h4><p><strong>代码实现：</strong>我们可以用下面的代码做一个简单测试：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UnionExample</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds1 = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;Integer&gt; ds2 = env.fromElements(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">        DataStreamSource&lt;String&gt; ds3 = env.fromElements(<span class="string">&quot;2&quot;</span>, <span class="string">&quot;2&quot;</span>, <span class="string">&quot;3&quot;</span>);</span><br><span class="line"></span><br><span class="line">        ds1.union(ds2,ds3.map(Integer::valueOf))</span><br><span class="line">           .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-6-2-连接（Connect）"><a href="#1-3-6-2-连接（Connect）" class="headerlink" title="1.3.6.2 连接（Connect）"></a>1.3.6.2 连接（Connect）</h4><h4 id="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"><a href="#流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。" class="headerlink" title="流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。"></a>流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（union），Flink还提供了另外一种方便的合流操作——连接（connect）。</h4><h5 id="1）连接流（ConnectedStreams）"><a href="#1）连接流（ConnectedStreams）" class="headerlink" title="1）连接流（ConnectedStreams）"></a>1）连接流（ConnectedStreams）</h5><p><img src="https://pic1.imgdb.cn/item/67864547d0e0a243d4f4350b.png"></p><p><strong>代码实现：</strong>需要分为两步：首先基于一条DataStream调用.connect()方法，传入另外一条DataStream作为参数，将两条流连接起来，得到一个ConnectedStreams；然后再调用同处理方法得到DataStream。这里可以的调用的同处理方法有.map()&#x2F;.flatMap()，以及.process()方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;Integer&gt; source1 = env.fromElements(1, 2, 3);</span></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;String&gt; source2 = env.fromElements(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);</span></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Integer&gt; source1 = env</span><br><span class="line">                .socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                .map(i -&gt; Integer.parseInt(i));</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; source2 = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">8888</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * TODO 使用 connect 合流</span></span><br><span class="line"><span class="comment">         * 1、一次只能连接 2条流</span></span><br><span class="line"><span class="comment">         * 2、流的数据类型可以不一样</span></span><br><span class="line"><span class="comment">         * 3、 连接后可以调用 map、flatmap、process来处理，但是各处理各的</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ConnectedStreams&lt;Integer, String&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connect.map(<span class="keyword">new</span> <span class="title class_">CoMapFunction</span>&lt;Integer, String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map1</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于数字流:&quot;</span> + value.toString();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">map2</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;来源于字母流:&quot;</span> + value;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码中，ConnectedStreams有两个类型参数，分别表示内部包含的两条流各自的数据类型；由于需要“一国两制”，因此调用.map()方法时传入的不再是一个简单的MapFunction，而是一个CoMapFunction，表示分别对两条流中的数据执行map操作。这个接口有三个类型参数，依次表示第一条流、第二条流，以及合并后的流中的数据类型。需要实现的方法也非常直白：.map1()就是对第一条流中数据的map操作，.map2()则是针对第二条流。</p><p>2）CoProcessFunction</p><p>与CoMapFunction类似，如果是调用.map()就需要传入一个CoMapFunction，需要实现map1()、map2()两个方法；而调用.process()时，传入的则是一个CoProcessFunction。它也是“处理函数”家族中的一员，用法非常相似。它需要实现的就是processElement1()、processElement2()两个方法，在每个数据到来时，会根据来源的流调用其中的一个方法进行处理。</p><p>值得一提的是，ConnectedStreams也可以直接调用.keyBy()进行按键分区的操作，得到的还是一个ConnectedStreams：</p><p>connectedStreams.keyBy(keySelector1, keySelector2);</p><p>这里传入两个参数keySelector1和keySelector2，是两条流中各自的键选择器；当然也可以直接传入键的位置值（keyPosition），或者键的字段名（field），这与普通的keyBy用法完全一致。ConnectedStreams进行keyBy操作，其实就是把两条流中key相同的数据放到了一起，然后针对来源的流再做各自处理，这在一些场景下非常有用。</p><p>案例需求：连接两条流，输出能根据id匹配上的数据（类似inner join效果）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConnectKeybyDemo</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Tuple2&lt;Integer, String&gt;&gt; source1 = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a1&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">1</span>, <span class="string">&quot;a2&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">2</span>, <span class="string">&quot;b&quot;</span>),</span><br><span class="line">                Tuple2.of(<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line">        );</span><br><span class="line">        DataStreamSource&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; source2 = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa1&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">1</span>, <span class="string">&quot;aa2&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple3.of(<span class="number">2</span>, <span class="string">&quot;bb&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple3.of(<span class="number">3</span>, <span class="string">&quot;cc&quot;</span>, <span class="number">1</span>)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connect = source1.connect(source2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 多并行度下，需要根据 关联条件 进行keyby，才能保证key相同的数据到一起去，才能匹配上</span></span><br><span class="line">        ConnectedStreams&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;&gt; connectKey = connect.keyBy(s1 -&gt; s1.f0, s2 -&gt; s2.f0);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; result = connectKey.process(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">CoProcessFunction</span>&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">                    <span class="comment">// 定义 HashMap，缓存来过的数据，key=id，value=list&lt;数据&gt;</span></span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple2&lt;Integer, String&gt;&gt;&gt; s1Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">                    Map&lt;Integer, List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt;&gt; s2Cache = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement1</span><span class="params">(Tuple2&lt;Integer, String&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s1数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple2&lt;Integer, String&gt;&gt; s1Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s1Values.add(value);</span><br><span class="line">                            s1Cache.put(id, s1Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s1Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s2的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple3&lt;Integer, String, Integer&gt; s2Element : s2Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + value + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + s2Element);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">processElement2</span><span class="params">(Tuple3&lt;Integer, String, Integer&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="type">Integer</span> <span class="variable">id</span> <span class="operator">=</span> value.f0;</span><br><span class="line">                        <span class="comment">// TODO 1.来过的s2数据，都存起来</span></span><br><span class="line">                        <span class="keyword">if</span> (!s2Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="comment">// 1.1 第一条数据，初始化 value的list，放入 hashmap</span></span><br><span class="line">                            List&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; s2Values = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">                            s2Values.add(value);</span><br><span class="line">                            s2Cache.put(id, s2Values);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            <span class="comment">// 1.2 不是第一条，直接添加到 list中</span></span><br><span class="line">                            s2Cache.get(id).add(value);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//TODO 2.根据id，查找s1的数据，只输出 匹配上 的数据</span></span><br><span class="line">                        <span class="keyword">if</span> (s1Cache.containsKey(id)) &#123;</span><br><span class="line">                            <span class="keyword">for</span> (Tuple2&lt;Integer, String&gt; s1Element : s1Cache.get(id)) &#123;</span><br><span class="line">                                out.collect(<span class="string">&quot;s1:&quot;</span> + s1Element + <span class="string">&quot;&lt;---------&gt;s2:&quot;</span> + value);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        result.print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4输出算子"><a href="#1-4输出算子" class="headerlink" title="1.4输出算子"></a>1.4输出算子</h3><h4 id="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"><a href="#Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。" class="headerlink" title="Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。"></a>Flink作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。</h4><p><img src="https://pic1.imgdb.cn/item/678645c8d0e0a243d4f43592.png"></p><h3 id="1-4-1-连接到外部系统"><a href="#1-4-1-连接到外部系统" class="headerlink" title="1.4.1 连接到外部系统"></a><strong>1.4.1</strong> <strong>连接到外部系统</strong></h3><h4 id="Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"><a href="#Flink的DataStream-API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。" class="headerlink" title="Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。"></a>Flink的DataStream API专门提供了向外部写入数据的方法：addSink。与addSource类似，addSink方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink程序中所有对外的输出操作，一般都是利用Sink算子完成的。</h4><h5 id="Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。"><a href="#Flink1-12以前，Sink算子的创建是通过调用DataStream的-addSink-方法实现的。" class="headerlink" title="Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。"></a>Flink1.12以前，Sink算子的创建是通过调用DataStream的.addSink()方法实现的。</h5><h5 id="stream-addSink-new-SinkFunction-…"><a href="#stream-addSink-new-SinkFunction-…" class="headerlink" title="stream.addSink(new SinkFunction(…));"></a>stream.addSink(new SinkFunction(…));</h5><h5 id="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"><a href="#addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke-，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。" class="headerlink" title="addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。"></a>addSink方法同样需要传入一个参数，实现的是SinkFunction接口。在这个接口中只需要重写一个方法invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用。</h5><h5 id="Flink1-12开始，同样重构了Sink架构，"><a href="#Flink1-12开始，同样重构了Sink架构，" class="headerlink" title="Flink1.12开始，同样重构了Sink架构，"></a>Flink1.12开始，同样重构了Sink架构，</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.sinkTo(…)</span><br></pre></td></tr></table></figure><h4 id="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："><a href="#当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：" class="headerlink" title="当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器："></a>当然，Sink多数情况下同样并不需要我们自己实现。之前我们一直在使用的print方法其实就是一种Sink，它表示将数据流写入标准控制台打印输出。Flink官方为我们提供了一部分的框架的Sink连接器。如下图所示，列出了Flink官方目前支持的第三方系统连接器：</h4><p><img src="https://pic1.imgdb.cn/item/678645f5d0e0a243d4f435e2.png"></p><h4 id="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"><a href="#我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source-x2F-sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。" class="headerlink" title="我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。"></a>我们可以看到，像Kafka之类流式系统，Flink提供了完美对接，source&#x2F;sink两端都能连接，可读可写；而对于Elasticsearch、JDBC等数据存储系统，则只提供了输出写入的sink连接器。</h4><h4 id="除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"><a href="#除Flink官方之外，Apache-Bahir框架，也实现了一些其他第三方系统与Flink的连接器。" class="headerlink" title="除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。"></a>除Flink官方之外，Apache Bahir框架，也实现了一些其他第三方系统与Flink的连接器。</h4><p><img src="https://pic1.imgdb.cn/item/6786460cd0e0a243d4f43614.png"></p><h4 id="除此以外，就需要用户自定义实现sink连接器了。"><a href="#除此以外，就需要用户自定义实现sink连接器了。" class="headerlink" title="除此以外，就需要用户自定义实现sink连接器了。"></a>除此以外，就需要用户自定义实现sink连接器了。</h4><h3 id="5-4-2-输出到文件"><a href="#5-4-2-输出到文件" class="headerlink" title="5.4.2 输出到文件"></a><strong>5.4.2</strong> <strong>输出到文件</strong></h3><h4 id="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"><a href="#Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。" class="headerlink" title="Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。"></a>Flink专门提供了一个流式文件系统的连接器：FileSink，为批处理和流处理提供了一个统一的Sink，它可以将分区文件写入Flink支持的文件系统。</h4><h4 id="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："><a href="#FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：" class="headerlink" title="FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法："></a>FileSink支持行编码（Row-encoded）和批量编码（Bulk-encoded）格式。这两种不同的方式都有各自的构建器（builder），可以直接调用FileSink的静态方法：</h4><ul><li>行编码： FileSink.forRowFormat（basePath，rowEncoder）。</li><li>批量编码： FileSink.forBulkFormat（basePath，bulkWriterFactory）。</li></ul><h4 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ch.qos.logback.core.util.<span class="type">TimeUtil</span></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringEncoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.<span class="type">Path</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.<span class="type">StreamingFileSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToFileTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 直接以文本形式分布式的写入到文件中</span></span><br><span class="line">    <span class="comment">// SimpleStringEncoder作用是将String转成char方便写入</span></span><br><span class="line">    <span class="keyword">val</span> fileSink = <span class="type">StreamingFileSink</span></span><br><span class="line">      .forRowFormat(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;./output&quot;</span>), <span class="keyword">new</span> <span class="type">SimpleStringEncoder</span>[<span class="type">String</span>](<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    stream.broadcast.map(_.toString).addSink(fileSink)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4-3-输出到Kafka"><a href="#1-4-3-输出到Kafka" class="headerlink" title="1.4.3 输出到Kafka"></a><strong>1.4.3</strong> 输出到Kafka</h3><h3 id="（1）添加Kafka-连接器依赖"><a href="#（1）添加Kafka-连接器依赖" class="headerlink" title="（1）添加Kafka 连接器依赖"></a>（1）添加Kafka 连接器依赖</h3><h3 id="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"><a href="#由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。" class="headerlink" title="由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。"></a>由于我们已经测试过从Kafka数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。</h3><h3 id="（2）启动Kafka集群"><a href="#（2）启动Kafka集群" class="headerlink" title="（2）启动Kafka集群"></a>（2）启动Kafka集群</h3><h3 id="（3）编写输出到Kafka的示例代码"><a href="#（3）编写输出到Kafka的示例代码" class="headerlink" title="（3）编写输出到Kafka的示例代码"></a>（3）编写输出到Kafka的示例代码</h3><h3 id="输出无key的record"><a href="#输出无key的record" class="headerlink" title="输出无key的record:"></a>输出无key的record:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.&#123;<span class="type">FlinkKafkaConsumer</span>, <span class="type">FlinkKafkaProducer</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToKafkaTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> stream = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>[<span class="type">String</span>](<span class="string">&quot;lhxcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line">      .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> fields = data.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        <span class="comment">// trim就是去空格</span></span><br><span class="line">        <span class="type">Event</span>(fields(<span class="number">0</span>).trim, fields(<span class="number">1</span>).trim, fields(<span class="number">2</span>).trim.toLong).toString</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 讲数据写入到kafka</span></span><br><span class="line">    stream.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer</span>[<span class="type">String</span>](<span class="string">&quot;master:9092&quot;</span>, <span class="string">&quot;hkjcpdd&quot;</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="然后开一个消费者查看是否到数据"><a href="#然后开一个消费者查看是否到数据" class="headerlink" title="然后开一个消费者查看是否到数据"></a>然后开一个消费者查看是否到数据</h3><h3 id="1-4-4-输出到MySQL（JDBC）"><a href="#1-4-4-输出到MySQL（JDBC）" class="headerlink" title="1.4.4 输出到MySQL（JDBC）"></a>1.4.4 输出到MySQL（JDBC）</h3><h3 id="写入数据的MySQL的测试步骤如下。"><a href="#写入数据的MySQL的测试步骤如下。" class="headerlink" title="写入数据的MySQL的测试步骤如下。"></a>写入数据的MySQL的测试步骤如下。</h3><h3 id="（1）添加依赖"><a href="#（1）添加依赖" class="headerlink" title="（1）添加依赖"></a>（1）添加依赖</h3><h3 id="添加MySQL驱动："><a href="#添加MySQL驱动：" class="headerlink" title="添加MySQL驱动："></a>添加MySQL驱动：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径："><a href="#官方还未提供flink-connector-jdbc的1-17-0的正式依赖，暂时从apache-snapshot仓库下载，pom文件中指定仓库路径：" class="headerlink" title="官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径："></a>官方还未提供flink-connector-jdbc的1.17.0的正式依赖，暂时从apache snapshot仓库下载，pom文件中指定仓库路径：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>apache-snapshots<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>apache snapshots<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.apache.org/content/repositories/snapshots/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="添加依赖："><a href="#添加依赖：" class="headerlink" title="添加依赖："></a>添加依赖：</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.17-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："><a href="#如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：" class="headerlink" title="如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容："></a>如果不生效，还需要修改本地maven的配置文件，mirrorOf中添加如下标红内容：</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">            &lt;id&gt;aliyunmaven&lt;/id&gt;</span><br><span class="line">            &lt;mirrorOf&gt;*,!apache-snapshots&lt;/mirrorOf&gt;</span><br><span class="line">            &lt;name&gt;阿里云公共仓库&lt;/name&gt;</span><br><span class="line">            &lt;url&gt;https:<span class="comment">//maven.aliyun.com/repository/public&lt;/url&gt;</span></span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><h4 id="（2）启动MySQL，在test库下建表ws"><a href="#（2）启动MySQL，在test库下建表ws" class="headerlink" title="（2）启动MySQL，在test库下建表ws"></a>（2）启动MySQL，在test库下建表ws</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span>     </span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `ws` (</span><br><span class="line">  `id` <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `ts` <span class="type">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `vc` <span class="type">int</span>(<span class="number">11</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (`id`)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8</span><br></pre></td></tr></table></figure><h4 id="（3）编写输出到MySQL的示例代码"><a href="#（3）编写输出到MySQL的示例代码" class="headerlink" title="（3）编写输出到MySQL的示例代码"></a>（3）编写输出到MySQL的示例代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.day02</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.day02.<span class="type">SourceBoundedTest</span>.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.connector.jdbc.&#123;<span class="type">JdbcConnectionOptions</span>, <span class="type">JdbcSink</span>, <span class="type">JdbcStatementBuilder</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">PreparedStatement</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SinkToMysqlTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;www.bing.com&quot;</span>, <span class="number">2000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjcpdd.com&quot;</span>, <span class="number">3000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;www.by123.com&quot;</span>, <span class="number">8000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Lisa&quot;</span>, <span class="string">&quot;www.hkjmjj.com&quot;</span>, <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.print()</span><br><span class="line"></span><br><span class="line">    stream.addSink(<span class="type">JdbcSink</span>.sink(</span><br><span class="line">      <span class="string">&quot;insert into shop (name, area, dizhi, price) values(?, ?, ?, ?)&quot;</span>, <span class="comment">// 定义写入Mysql的语句</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcStatementBuilder</span>[<span class="type">Event</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(t: <span class="type">PreparedStatement</span>, u: <span class="type">Event</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">          t.setString(<span class="number">1</span>, u.user)</span><br><span class="line">          t.setString(<span class="number">2</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">3</span>, u.url)</span><br><span class="line">          t.setString(<span class="number">4</span>, u.url)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">JdbcConnectionOptions</span>.<span class="type">JdbcConnectionOptionsBuilder</span>()</span><br><span class="line">        .withUrl(<span class="string">&quot;jdbc:mysql://master:3306/test?useSSL=false&quot;</span>)</span><br><span class="line">        .withDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">        .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">        .withPassword(<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">        .build()</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"><a href="#（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。" class="headerlink" title="（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。"></a>（4）运行代码，用客户端连接MySQL，查看是否成功写入数据。</h4><h3 id="1-4-5-自定义Sink输出"><a href="#1-4-5-自定义Sink输出" class="headerlink" title="1.4.5 自定义Sink输出"></a><strong>1.4.5</strong> <strong>自定义Sink输出</strong></h3><h4 id="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。"><a href="#如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的-addSink-方法就可以自定义写入任何外部存储。" class="headerlink" title="如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。"></a>如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkDunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.addSink(<span class="keyword">new</span> <span class="type">MySinkFunction</span>&lt;<span class="type">String</span>&gt;());</span><br></pre></td></tr></table></figure><h4 id="在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"><a href="#在实现SinkFunction的时候，需要重写的一个关键方法invoke-，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。" class="headerlink" title="在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。"></a>在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。</h4><h4 id="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"><a href="#这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。" class="headerlink" title="这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。"></a>这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。</h4>]]></content>
      
      
      <categories>
          
          <category> Flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据可视化学习路线</title>
      <link href="/posts/1083.html"/>
      <url>/posts/1083.html</url>
      
        <content type="html"><![CDATA[<h1 id="大数据数据可视化学习路线"><a href="#大数据数据可视化学习路线" class="headerlink" title="大数据数据可视化学习路线"></a>大数据数据可视化学习路线</h1><h5 id="前提：具备一定的Python基础"><a href="#前提：具备一定的Python基础" class="headerlink" title="前提：具备一定的Python基础"></a>前提：具备一定的Python基础</h5><h2 id="1-python的基础学习，以及进阶学习"><a href="#1-python的基础学习，以及进阶学习" class="headerlink" title="1.python的基础学习，以及进阶学习"></a>1.python的基础学习，以及进阶学习</h2><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href>https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="2-numpy、pandas的入门学习（先学numpy）"><a href="#2-numpy、pandas的入门学习（先学numpy）" class="headerlink" title="2.numpy、pandas的入门学习（先学numpy）"></a>2.numpy、pandas的入门学习（先学numpy）</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="3-数据可视化matplotlib库的基础绘图"><a href="#3-数据可视化matplotlib库的基础绘图" class="headerlink" title="3.数据可视化matplotlib库的基础绘图"></a>3.数据可视化matplotlib库的基础绘图</h3><p>【千锋教育python数据可视化Matplotlib绘图教程，Matplotlib柱状图｜Matplotlib动态图｜Matplotlib散点图】<a href>https://www.bilibili.com/video/BV1nM411m7Cf?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看"><a href="#4-数据可视化进阶绘图基于matplotlib的seaborn库-从p51开始看" class="headerlink" title="4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)"></a>4.数据可视化进阶绘图基于matplotlib的seaborn库  (从p51开始看)</h3><p>【【人工智能必备：Python数据分析】AI博士半天就教会我大学一直没学会的利用Python进行数据分析！怎么可以讲的如此通俗，太强了！】<a href>https://www.bilibili.com/video/BV1ru411U772?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="5-数据可视化pyecharts可交互式基础绘图"><a href="#5-数据可视化pyecharts可交互式基础绘图" class="headerlink" title="5.数据可视化pyecharts可交互式基础绘图"></a>5.数据可视化pyecharts可交互式基础绘图</h3><p>【千锋教育PyEcharts数据可视化快速入门教程，大数据分析Python交互绘图实用利器】<a href>https://www.bilibili.com/video/BV1nM411F7GT?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="6-数据可视化tableau图标绘图"><a href="#6-数据可视化tableau图标绘图" class="headerlink" title="6.数据可视化tableau图标绘图"></a>6.数据可视化tableau图标绘图</h3><p>【【Tableau教程】Tableau零基础教程，带你解锁当下最受欢迎的数据可视化软件】<a href>https://www.bilibili.com/video/BV1E4411B7ef?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="7-数据交互式可视化进阶绘图前提基础html"><a href="#7-数据交互式可视化进阶绘图前提基础html" class="headerlink" title="7.数据交互式可视化进阶绘图前提基础html"></a>7.数据交互式可视化进阶绘图前提基础html</h3><p>【黑马程序员pink老师前端入门教程，零基础必看的h5(html5)+css3+移动端前端视频教程】<a href>https://www.bilibili.com/video/BV14J4114768?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="8-数据可视化echarts可交互式进阶绘图"><a href="#8-数据可视化echarts可交互式进阶绘图" class="headerlink" title="8.数据可视化echarts可交互式进阶绘图"></a>8.数据可视化echarts可交互式进阶绘图</h3><p>【电商平台数据可视化实时监控系统-Echarts-vue项目综合练习-pink老师推荐(持续更新)素材已经更新】<a href>https://www.bilibili.com/video/BV1bh41197p8?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="9-使用vue进行数据可视化前提基础Node-js"><a href="#9-使用vue进行数据可视化前提基础Node-js" class="headerlink" title="9.使用vue进行数据可视化前提基础Node.js"></a>9.使用vue进行数据可视化前提基础Node.js</h3><p>【黑马程序员Node.js全套入门教程，nodejs新教程含es6模块化+npm+express+webpack+promise等_Nodejs实战案例详解】<a href>https://www.bilibili.com/video/BV1a34y167AZ?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p><h3 id="10-使用vue进行数据可视化"><a href="#10-使用vue进行数据可视化" class="headerlink" title="10.使用vue进行数据可视化"></a>10.使用vue进行数据可视化</h3><p>【千锋Echarts+Vue3.0数据可视化项目构建_入门必备前端项目实战教程】<a href>https://www.bilibili.com/video/BV14u411D7qK?vd_source&#x3D;90fd85d804fb97442fbceaa1b3402436</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习路线 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习路线 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集群搭建学习路线</title>
      <link href="/posts/1081.html"/>
      <url>/posts/1081.html</url>
      
        <content type="html"><![CDATA[<h1 id="大数据集群搭建学习路线"><a href="#大数据集群搭建学习路线" class="headerlink" title="大数据集群搭建学习路线"></a>大数据集群搭建学习路线</h1><h3 id="前提：熟练使用Linux的命令及其操作"><a href="#前提：熟练使用Linux的命令及其操作" class="headerlink" title="前提：熟练使用Linux的命令及其操作"></a>前提：熟练使用Linux的命令及其操作</h3><h3 id="0-CentosLinux的基操"><a href="#0-CentosLinux的基操" class="headerlink" title="0.CentosLinux的基操"></a>0.CentosLinux的基操</h3><p>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="1-Mysql数据库的入门学习"><a href="#1-Mysql数据库的入门学习" class="headerlink" title="1.Mysql数据库的入门学习"></a>1.Mysql数据库的入门学习</h3><p>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="2-Hadoop集群的搭建及其使用和Hive的使用"><a href="#2-Hadoop集群的搭建及其使用和Hive的使用" class="headerlink" title="2.Hadoop集群的搭建及其使用和Hive的使用"></a>2.Hadoop集群的搭建及其使用和Hive的使用</h3><p>【黑马程序员大数据入门到实战教程，大数据开发必会的Hadoop、Hive，云平台实战项目全套一网打尽】<a href="https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1WY4y197g7?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="3-Java的基础学习（上部）"><a href="#3-Java的基础学习（上部）" class="headerlink" title="3.Java的基础学习（上部）"></a>3.Java的基础学习（上部）</h3><p>【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="4-scala语言的学习"><a href="#4-scala语言的学习" class="headerlink" title="4.scala语言的学习"></a>4.scala语言的学习</h3><p>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="5-Python基础的学习"><a href="#5-Python基础的学习" class="headerlink" title="5.Python基础的学习"></a>5.Python基础的学习</h3><p>【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="6-zookeeper组件的学习"><a href="#6-zookeeper组件的学习" class="headerlink" title="6.zookeeper组件的学习"></a>6.zookeeper组件的学习</h3><p>【黑马程序员Zookeeper视频教程，快速入门zookeeper技术】<a href="https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1M741137qY?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="7-kafka组件的利用"><a href="#7-kafka组件的利用" class="headerlink" title="7.kafka组件的利用"></a>7.kafka组件的利用</h3><p>【尚硅谷Kafka教程，2024新版kafka视频，零基础入门到实战】<a href="https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1Gp421m7UN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="8-Flume的使用"><a href="#8-Flume的使用" class="headerlink" title="8.Flume的使用"></a>8.Flume的使用</h3><p>【【海牛大数据】Flume教程（学Flume看这个一套就够了，从基础到扩展到实战案例到组件配合全都有）】<a href="https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV18M4y1Q7fp?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="9-Sqoop的运用"><a href="#9-Sqoop的运用" class="headerlink" title="9.Sqoop的运用"></a>9.Sqoop的运用</h3><p>【【海牛大数据】Sqoop教程（命令详解、各组件融合、实战案例）】<a href="https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1724y137XU?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="10-Spark的搭建"><a href="#10-Spark的搭建" class="headerlink" title="10.Spark的搭建"></a>10.Spark的搭建</h3><h4 id="Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）"><a href="#Spark环境搭建（Local模式、StandAlone模式、Spark-On-Yarn模式、SPark-Ha）" class="headerlink" title="Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）"></a>Spark环境搭建（Local模式、StandAlone模式、Spark On Yarn模式、SPark Ha）</h4><p><a href="https://blog.csdn.net/JunLeon/article/details/123625680">(超详细) Spark环境搭建（Local模式、 StandAlone模式、Spark On Yarn模式）-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_44480968/article/details/119580363">Spark配置高可用（HA）_spark ha-CSDN博客</a></p><h3 id="11-Flink搭建及其使用-由于python版的使用还不是特别完整"><a href="#11-Flink搭建及其使用-由于python版的使用还不是特别完整" class="headerlink" title="11.Flink搭建及其使用(由于python版的使用还不是特别完整)"></a>11.Flink搭建及其使用(由于python版的使用还不是特别完整)</h3><h4 id="scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#scala版：【【尚硅谷】Flink1-13教程（Scala版）】https-www-bilibili-com-video-BV1zr4y157XV-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="scala版：【【尚硅谷】Flink1.13教程（Scala版）】https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>scala版：【【尚硅谷】Flink1.13教程（Scala版）】<a href="https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1zr4y157XV?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h4 id="java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854"><a href="#java版：【尚硅谷大数据Flink1-17实战教程从入门到精通】https-www-bilibili-com-video-BV1eg4y1V7AN-vd-source-df87f9e3a087dee96152d82cde6d8854" class="headerlink" title="java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854"></a>java版：【尚硅谷大数据Flink1.17实战教程从入门到精通】<a href="https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1eg4y1V7AN?vd_source=df87f9e3a087dee96152d82cde6d8854</a></h4><h3 id="11-Redis的搭建及其使用"><a href="#11-Redis的搭建及其使用" class="headerlink" title="11.Redis的搭建及其使用"></a>11.Redis的搭建及其使用</h3><p>【黑马程序员Redis入门到实战教程，深度透析redis底层原理+redis分布式锁+企业解决方案+黑马点评实战项目】<a href="https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1cr4y1671t?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="12-Hbase的搭建及其使用"><a href="#12-Hbase的搭建及其使用" class="headerlink" title="12.Hbase的搭建及其使用"></a>12.Hbase的搭建及其使用</h3><p>【【好程序员】大数据全新分布式存储HBase精品课程】<a href="https://www.bilibili.com/video/BV1RM411i7XM?p=15&vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1RM411i7XM?p=15&amp;vd_source=df87f9e3a087dee96152d82cde6d8854</a></p><h3 id="13-Azkaban的搭建及其使用"><a href="#13-Azkaban的搭建及其使用" class="headerlink" title="13.Azkaban的搭建及其使用"></a>13.Azkaban的搭建及其使用</h3><p>【尚硅谷大数据Azkaban 3.x教程（全新发布）】<a href="https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1y54y18713?vd_source=df87f9e3a087dee96152d82cde6d8854</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习路线 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习路线 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据处理学习路线</title>
      <link href="/posts/1082.html"/>
      <url>/posts/1082.html</url>
      
        <content type="html"><![CDATA[<ul><li><h1 id="大数据数据处理学习路线"><a href="#大数据数据处理学习路线" class="headerlink" title="大数据数据处理学习路线"></a>大数据数据处理学习路线</h1><p><strong>注意</strong>：下列无标注的全部要看</p><h2 id="第一阶段：基础部分"><a href="#第一阶段：基础部分" class="headerlink" title="第一阶段：基础部分"></a>第一阶段：基础部分</h2><ul><li>语言基础：python、java基础<ul><li>python：【黑马程序员python教程，8天python从入门到精通，学python看这套就够了】<a href="https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1qW4y1a7fU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注释：第一阶段</li><li>java：【黑马程序员Java零基础视频教程_上部(Java入门，含斯坦福大学练习题+力扣算法题和大厂java面试题）】<a href="https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV17F411T7Ao?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>数据库基础：mysql<ul><li>【黑马程序员 MySQL数据库入门到精通，从mysql安装到mysql高级、mysql优化全囊括】<a href="https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Kr4y1i7ru?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：基础篇</li></ul></li><li>linux：命令基础（）<ul><li>【黑马程序员新版Linux零基础快速入门到精通，全涵盖linux系统知识、常用软件环境部署、Shell脚本、云平台实践、大数据集群项目实战等】<a href="https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854">https://www.bilibili.com/video/BV1n84y1i7td?vd_source=df87f9e3a087dee96152d82cde6d8854</a></li></ul></li></ul><h2 id="第二部分：简单工具使用"><a href="#第二部分：简单工具使用" class="headerlink" title="第二部分：简单工具使用"></a>第二部分：简单工具使用</h2><ul><li><p>python第三方库：pandas、numpy、requests、bs4、jieba、snownlp</p><table><thead><tr><th>第三方库</th><th>链接</th></tr></thead><tbody><tr><td>pandas、numpy</td><td>【千锋教育python数据分析教程200集，Python数据分析师入门必备视频】<a href="https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E6%B3%A8%EF%BC%9Ap38-p117%EF%BC%89">https://www.bilibili.com/video/BV15V4y1f7Ju?p=117&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58（注：p38-p117）</a></td></tr><tr><td>requests、bs4</td><td>【尚硅谷Python爬虫教程小白零基础速通（含python基础+爬虫案例）】<a href="https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Db4y1m7Ho?p=52&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a> (注：p52-最后)</td></tr><tr><td>jieba</td><td>【Python Jieba 中文分词工具】<a href="https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1za4y117fE?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></td></tr><tr><td>snownlp</td><td>【Lecture 12 基于Snownlp的文本情感分析】<a href="https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58%EF%BC%88%E7%9C%8B%E5%AE%8C%E8%BF%99%E9%9B%86%E5%B0%B1%E8%A1%8C%E4%BA%86%EF%BC%89">https://www.bilibili.com/video/BV1DP4y1F7Mg?vd_source=3527d90a19fbfc4e630603c127d8bc58（看完这集就行了）</a></td></tr></tbody></table></li><li><p>excel：函数使用以及操作</p><ul><li>【2025必看！全网最新最细最实用Excel零基础入门到精通全套教程！专为零基础小白打造！内容富含Excel表格基础操作、实用函数讲解、项目实战等！】<a href="https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1wD4y1V7ZU?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li><p>hadoop：hadoop的基本使用命令</p><ul><li><a href="https://blog.csdn.net/m0_43405302/article/details/122243263">hadoop的HDFS的shell命令大全（一篇文章就够了）_shell统计hdfs-CSDN博客</a></li></ul></li></ul><h2 id="第三部分：集群工具使用"><a href="#第三部分：集群工具使用" class="headerlink" title="第三部分：集群工具使用"></a>第三部分：集群工具使用</h2><ul><li><p>scala：scala语言基础</p><ul><li>【黑马程序员Scala零基础入门到精通，大数据入门语言Scala精讲+案例】<a href="https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Q5411t74z?vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：看完前十章</li></ul></li><li><p>mapreduce：了解基本使用以及自定义方法</p><ul><li><p>【黑马程序员大数据Hadoop3.x全套教程，一套精通Hadoop的大数据入门教程】<a href="https://www.bilibili.com/video/BV11N411d7Zh?p=214&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV11N411d7Zh?p=214&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p171-214</p></li></ul></li><li><p>spark：掌握sparkcore sparksql</p><ul><li>【全网最全大数据Spark3.0教程 Spark3.0从入门到精通 黑马程序员大数据入门教程系列】<a href="https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1Xz4y1m7cv?p=20&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a><br>注：搭建部分不用看、只看sparkcore和sparksql</li></ul></li><li><p>hive：了解hive命令以及udf自定义函数</p><ul><li><p>【黑马程序员Hive全套教程，大数据Hive3.x数仓开发精讲到企业级实战应用】<a href="https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1L5411u7ae?vd_source=3527d90a19fbfc4e630603c127d8bc58</a></p><p>注：p0-p96</p></li></ul></li></ul><h2 id="第四部分：深入学习"><a href="#第四部分：深入学习" class="headerlink" title="第四部分：深入学习"></a>第四部分：深入学习</h2><ul><li>机器学习：了解机器学习原理以及sklearn使用<ul><li>【黑马程序员3天快速入门python机器学习】<a href="https://www.bilibili.com/video/BV1nt411r7tj?p=18&vd_source=3527d90a19fbfc4e630603c127d8bc58">https://www.bilibili.com/video/BV1nt411r7tj?p=18&amp;vd_source=3527d90a19fbfc4e630603c127d8bc58</a></li></ul></li><li>spark streaming：了解流式数据<ul><li>自己找视频或文档</li></ul></li><li>项目制作：尝试制作大数据项目</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习路线 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习路线 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop原理</title>
      <link href="/posts/3985.html"/>
      <url>/posts/3985.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784f8b3d0e0a243d4f3f05f.png"></p>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop实例命令</title>
      <link href="/posts/3986.html"/>
      <url>/posts/3986.html</url>
      
        <content type="html"><![CDATA[<h4 id="创建hive的表根据mysql上的表进行创建-create-hive-table"><a href="#创建hive的表根据mysql上的表进行创建-create-hive-table" class="headerlink" title="创建hive的表根据mysql上的表进行创建(create-hive-table)"></a><strong>创建hive的表根据mysql上的表进行创建(create-hive-table)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table --connect jdbc:mysql://master:3306/sqoop_db --username root --password 123456 --table city --hive-table hkjcpdd.city</span><br></pre></td></tr></table></figure><h4 id="查看mysql上有什么表-list-tables"><a href="#查看mysql上有什么表-list-tables" class="headerlink" title="查看mysql上有什么表(list-tables)"></a><strong>查看mysql上有什么表(list-tables)</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-tables --connect jdbc:mysql://master:3306/sys --username root --password 123456</span><br></pre></td></tr></table></figure><h3 id="查看mysql上有什么库-list-databases"><a href="#查看mysql上有什么库-list-databases" class="headerlink" title="查看mysql上有什么库(list-databases)"></a><strong>查看mysql上有什么库(list-databases)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://master:3306/ --username root --password 12345</span><br></pre></td></tr></table></figure><h3 id="使用sql进行操作-eval-query"><a href="#使用sql进行操作-eval-query" class="headerlink" title="使用sql进行操作(eval   query)"></a><strong>使用sql进行操作(eval   query)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">eval</span> --connect jdbc:mysql://master:3306/sqoop_db --username root --password  123456 --query <span class="string">&quot;select * from city limit 2;&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop多map条件查询导入hdfs.md</title>
      <link href="/posts/3992.html"/>
      <url>/posts/3992.html</url>
      
        <content type="html"><![CDATA[<h2 id="多map条件查询导入hdfs"><a href="#多map条件查询导入hdfs" class="headerlink" title="多map条件查询导入hdfs"></a>多map条件查询导入hdfs</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect 数据库连接字符串 \</span><br><span class="line">--username 数据库用户名 \</span><br><span class="line">--password 数据库密码 \</span><br><span class="line">--target-dir hdfs位置 \</span><br><span class="line">--delete-target-dir \  <span class="comment"># 这个就是把目录删了，不然mapreduce会执行失败</span></span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \ <span class="comment"># 使用什么分隔符</span></span><br><span class="line">--num-mappers 3 \</span><br><span class="line">--split-by 切分数依据 \</span><br><span class="line">--query <span class="string">&#x27; SQL语句 and $CONDITIONS &#x27;</span></span><br></pre></td></tr></table></figure><h3 id="–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"><a href="#–num-mappers-3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力" class="headerlink" title="–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力"></a><strong>–num-mappers 3这个是总共的Map数，Yarn会进行资源调度，看每台机的承受能力</strong></h3><h3 id="CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理"><a href="#CONDITIONS使用-CONDITIONS-的好处是，它允许-Sqoop-在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给-Sqoop-管理" class="headerlink" title="$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理"></a><strong>$CONDITIONS使用 $CONDITIONS 的好处是，它允许 Sqoop 在执行导入时动态地处理这些分片相关的条件，而无需你在查询中硬编码这些逻辑。这样，你可以专注于编写查询本身，而将分片和并发处理的细节留给 Sqoop 管理</strong></h3>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导入其他格式文件</title>
      <link href="/posts/3994.html"/>
      <url>/posts/3994.html</url>
      
        <content type="html"><![CDATA[<h2 id="导入其他格式文件"><a href="#导入其他格式文件" class="headerlink" title="导入其他格式文件"></a>导入其他格式文件</h2><h3 id="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式"><a href="#导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile-默认格式" class="headerlink" title="导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)"></a><strong>导入不同格式，支持as-avrodatafile、as-parquetfile、as-sequencefile、as-textfile(默认格式)</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /data/hkjcpdd \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--as-parquetfile \ <span class="comment">#文件格式</span></span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query <span class="string">&#x27;select * from city where id &lt; 10 and $CONDITIONS&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导入到Hbase</title>
      <link href="/posts/3988.html"/>
      <url>/posts/3988.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-table hkjcpdd:city \</span><br><span class="line">--column-family cf \</span><br><span class="line">--hbase-row-key <span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --hbase-row-key:要求mysql表必须有主见，将主键作为rowkey,表示一行</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop应用案例</title>
      <link href="/posts/3987.html"/>
      <url>/posts/3987.html</url>
      
        <content type="html"><![CDATA[<h3 id="创建一个执行文件然后给予权限然后执行"><a href="#创建一个执行文件然后给予权限然后执行" class="headerlink" title="创建一个执行文件然后给予权限然后执行"></a><strong>创建一个执行文件然后给予权限然后执行</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">batch_date=<span class="variable">$1</span></span><br><span class="line"></span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--usernmae root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir hive表hdfs目录 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span><br><span class="line">--split-by <span class="built_in">id</span> \</span><br><span class="line">--query select * from city</span><br><span class="line"></span><br><span class="line">result=$?</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [<span class="variable">$result</span> != 0];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;执行失败&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd</span><br><span class="line"><span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span>  <span class="string">&quot;执行成功&quot;</span> `<span class="built_in">date</span>` &gt;&gt; /home/hadoop/hkjcpdd </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop</title>
      <link href="/posts/3989.html"/>
      <url>/posts/3989.html</url>
      
        <content type="html"><![CDATA[<h2 id="数据的导入导出"><a href="#数据的导入导出" class="headerlink" title="数据的导入导出"></a>数据的导入导出</h2><p>导入：import</p><p>导出：export</p><p><img src="https://pic1.imgdb.cn/item/6784f9d1d0e0a243d4f3f083.png"></p>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop搭建</title>
      <link href="/posts/3984.html"/>
      <url>/posts/3984.html</url>
      
        <content type="html"><![CDATA[<h2 id="Sqoop搭建"><a href="#Sqoop搭建" class="headerlink" title="Sqoop搭建"></a>Sqoop搭建</h2><h3 id="1-解压"><a href="#1-解压" class="headerlink" title="1.解压"></a>1.解压</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /root/software</span><br></pre></td></tr></table></figure><h3 id="更改名字"><a href="#更改名字" class="headerlink" title="更改名字"></a>更改名字</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /root/software</span><br><span class="line"><span class="built_in">mv</span> sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop</span><br></pre></td></tr></table></figure><h3 id="2-添加环境变量"><a href="#2-添加环境变量" class="headerlink" title="2.添加环境变量"></a>2.添加环境变量</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">export</span> SQOOP_HOME=/root/software/sqoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SQOOP_HOME</span>/bin</span><br></pre></td></tr></table></figure><h3 id="3-配置-Sqoop-环境变量文件"><a href="#3-配置-Sqoop-环境变量文件" class="headerlink" title="3.配置 Sqoop 环境变量文件"></a>3.配置 Sqoop 环境变量文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切换到 Sqoop 配置文件目录</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$SQOOP_HOME</span>/conf</span><br><span class="line"><span class="comment"># 复制 Sqoop 环境变量模板文件</span></span><br><span class="line"><span class="built_in">cp</span> sqoop-env-template.sh sqoop-env.sh </span><br><span class="line"><span class="comment"># 编辑文件，指定相关路径</span></span><br><span class="line">vi sqoop-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最底下的配置加上，没有装的就不用去掉#号</span></span><br></pre></td></tr></table></figure><h3 id="4-MySQL-驱动"><a href="#4-MySQL-驱动" class="headerlink" title="4. MySQL 驱动"></a>4. MySQL 驱动</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拷贝 MySQL 驱动到 Sqoop 中的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> /opt/software/mysql-connector-java-5.1.37-bin.jar <span class="variable">$SQOOP_HOME</span>/lib</span><br></pre></td></tr></table></figure><h3 id="5-拷贝-Hive-文件"><a href="#5-拷贝-Hive-文件" class="headerlink" title="5. 拷贝 Hive 文件"></a>5. 拷贝 Hive 文件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了后续方便操作 Hive，我们需要将 Hive 的驱动放入 Sqoop 的 lib 目录中</span></span><br><span class="line"><span class="built_in">cp</span> hive/lib/hive-common-3.1.2.jar sqoop/lib/</span><br></pre></td></tr></table></figure><h3 id="6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）"><a href="#6-验证（输入-sqoop-version，出现如下版本信息表示安装成功）" class="headerlink" title="6.验证（输入 sqoop version，出现如下版本信息表示安装成功）"></a>6.验证（输入 <code>sqoop version</code>，出现如下版本信息表示安装成功）</h3><h3 id="7-展示Mysql中sys库下的所有表"><a href="#7-展示Mysql中sys库下的所有表" class="headerlink" title="7.展示Mysql中sys库下的所有表"></a>7.展示Mysql中sys库下的所有表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list<span class="operator">-</span>tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://localhost:3306/sys \</span></span><br><span class="line"><span class="comment">--username root \</span></span><br><span class="line"><span class="comment">--password 123456</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop导出</title>
      <link href="/posts/3990.html"/>
      <url>/posts/3990.html</url>
      
        <content type="html"><![CDATA[<h3 id="1-从hdfs导出到mysql里"><a href="#1-从hdfs导出到mysql里" class="headerlink" title="1.从hdfs导出到mysql里"></a><strong>1.从hdfs导出到mysql里</strong></h3><ol><li><h4 id="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"><a href="#要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表" class="headerlink" title="要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表"></a>要在mysql中创建相对应的表，因为他不会自动创建表，所以我我们在mysql中，根据hdfs文件中的数据，创建对应的表</h4></li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir hdfs路径指定到文件 \  <span class="comment"># 要导出的文件</span></span><br><span class="line">--table emp \    <span class="comment"># 导出到哪张Mysql表</span></span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--input-fields-terminated-by <span class="string">&#x27;|&#x27;</span>  <span class="comment"># 分隔符</span></span><br></pre></td></tr></table></figure><h3 id="2-从hive导出到Mysql中"><a href="#2-从hive导出到Mysql中" class="headerlink" title="2.从hive导出到Mysql中"></a><strong>2.从hive导出到Mysql中</strong></h3><h4 id="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"><a href="#sqoop的export命令支持insert、update到关系型数据库，但是不支持merge" class="headerlink" title="sqoop的export命令支持insert、update到关系型数据库，但是不支持merge"></a><strong>sqoop的export命令支持insert、update到关系型数据库，但是不支持merge</strong></h4><h4 id="1-hive表导入Mysql数据库insert（直接写入）"><a href="#1-hive表导入Mysql数据库insert（直接写入）" class="headerlink" title="1.hive表导入Mysql数据库insert（直接写入）"></a><strong>1.hive表导入Mysql数据库insert（直接写入）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--table cityhkjcpdd </span><br><span class="line">--num-mappers 4 </span><br><span class="line">--fields-terminated-by <span class="string">&#x27;,&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="2-从hive表导入mysql数据库update（追加替换相同）"><a href="#2-从hive表导入mysql数据库update（追加替换相同）" class="headerlink" title="2.从hive表导入mysql数据库update（追加替换相同）"></a><strong>2.从hive表导入mysql数据库update（追加替换相同）</strong></h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--export-dir /user/hive/warehouse/hkjcpdd.db/city1 \</span><br><span class="line">--update-key <span class="built_in">id</span> \    <span class="comment"># 根据id来进行去重</span></span><br><span class="line">--fields-terminated-by <span class="string">&#x27;\t&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全量导入hive表和增量导入hive表</title>
      <link href="/posts/3991.html"/>
      <url>/posts/3991.html</url>
      
        <content type="html"><![CDATA[<h3 id="全量导入："><a href="#全量导入：" class="headerlink" title="全量导入："></a><strong>全量导入：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/sqoop_db \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-import \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table hive的库.hive的表</span><br></pre></td></tr></table></figure><h4 id="增量导入："><a href="#增量导入：" class="headerlink" title="增量导入："></a><strong>增量导入：</strong></h4><h5 id="1-append方式"><a href="#1-append方式" class="headerlink" title="1.append方式"></a><strong>1.append方式</strong></h5><h5 id="2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"><a href="#2-lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）" class="headerlink" title="2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）"></a>2.lastmodified方式，必须要加–append（最加）或者–merge-key（合并，一般天主键）</h5><p><img src="https://pic1.imgdb.cn/item/6784fbd6d0e0a243d4f3f0e9.png"></p><h2 id="这个是第一种方式：incremental-append"><a href="#这个是第一种方式：incremental-append" class="headerlink" title="这个是第一种方式：incremental append"></a><strong>这个是第一种方式：incremental append</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line"></span><br><span class="line">--target-dir hdfs路径 \  这个路径是表的路径可以在hive中show create table city;就可以看到hdfs的位置了</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column <span class="built_in">id</span> \</span><br><span class="line">--last-value 1  这个就是看你的数字哪一行，如果是1那就从2开始增量数据</span><br></pre></td></tr></table></figure><p>参数解释：</p><p>1)incremental : append或lastmodified，使用lastmodified方式导入数据要指定增量数据是要 –append（追加）还是要 –merge-key（合并）</p><p>2)check-column&lt;字段&gt;: 作为增量导入判断的列名</p><p>3)last-value val : 指定某一个值，用于标记增量导入的位置，这个值的数据不会被导入列表中，只用于标记当前表中最后的值。</p><h3 id="第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）"><a href="#第二种导入方式–incremental-lastmodified-–append（例子：列入按照时间进行增量导入）" class="headerlink" title="第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）"></a><strong>第二种导入方式–incremental lastmodified –append（例子：列入按照时间进行增量导入）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--append</span><br><span class="line"></span><br><span class="line">--注意：last-value的设置把包括2020-10-10 13:00:00 时间的数据做增量导入</span><br></pre></td></tr></table></figure><h3 id="第三种导入方式–incremental-lastmodified-–append-（进行合并的）"><a href="#第三种导入方式–incremental-lastmodified-–append-（进行合并的）" class="headerlink" title="第三种导入方式–incremental lastmodified –append （进行合并的）"></a><strong>第三种导入方式–incremental lastmodified –append （进行合并的）</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306 \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table city \</span><br><span class="line">--num-mappers 4 \</span><br><span class="line">--target-dir hdfs路径 \</span><br><span class="line">--fields-terminated-by <span class="string">&quot;,&quot;</span> \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--check-column last_time \</span><br><span class="line">--last-value <span class="string">&#x27;2020-10-10 13:00:00&#x27;</span> \</span><br><span class="line">--merge-key <span class="built_in">id</span> <span class="comment"># 根据id来进行操作</span></span><br><span class="line"></span><br><span class="line">--incremental lastmodified --merge-key的作用：修改过得数据和新增的数据（前提是满足last-value的条件）都会导入尽力啊，并且重复的数据（不需要满足last-value的条件）都会进行合并</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark RDD</title>
      <link href="/posts/fa63.html"/>
      <url>/posts/fa63.html</url>
      
        <content type="html"><![CDATA[<h3 id="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心"><a href="#RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark-core的底层核心" class="headerlink" title="RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心"></a><strong>RDD叫做弹性分布式数据集，是Spark中最基本的数据抽象，他代表一个不可变、可分区、里面的元素可并行计算的集合。RDD是spark core的底层核心</strong></h3><h4 id><a href="#" class="headerlink" title></a></h4><h4 id="Dataset-数据集"><a href="#Dataset-数据集" class="headerlink" title="Dataset:(数据集)"></a><strong>Dataset</strong><strong>:(数据集)</strong></h4><ul><li><strong>RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数；</strong></li><li><strong>RDD 也可以缓存起来, 相当于存储具体数据。</strong></li></ul><h4 id="Distributed-："><a href="#Distributed-：" class="headerlink" title="Distributed****："></a><strong>Distributed****：</strong></h4><h4 id="RDD-支持分区-可以运行在集群中。"><a href="#RDD-支持分区-可以运行在集群中。" class="headerlink" title="RDD 支持分区, 可以运行在集群中。"></a><strong>RDD 支持分区, 可以运行在集群中。</strong></h4><h4 id="Resilient-："><a href="#Resilient-：" class="headerlink" title="Resilient****："></a><strong>Resilient****：</strong></h4><ul><li><strong>RDD 支持高效的容错；</strong></li><li><strong>RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中。</strong></li></ul><h3 id="1-RDD的特点："><a href="#1-RDD的特点：" class="headerlink" title="1.RDD的特点："></a><strong>1.RDD的特点：</strong></h3><ul><li><p>弹性</p><ul><li>容错的弹性:数据丢失可以自动恢复;</li><li>存储的弹性:内存与磁盘的自动切换;</li><li>计算的弹性:计算出错重试机制;</li><li>分片的弹性:可根据需要重新分片。</li></ul></li><li><p>分布式:数据存储在集群不同节点上&#x2F;计算分布式。</p></li><li><p>数据集: RDD封装了计算逻辑，并不保存数据。</p></li><li><p>数据抽象: RDD是一个抽象类，需要子类具体实现。</p></li><li><p>不可变: RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑。</p></li><li><p>可分区、并行计算。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark on yarn</title>
      <link href="/posts/fa64.html"/>
      <url>/posts/fa64.html</url>
      
        <content type="html"><![CDATA[<h1 id="Spark-on-Yarn"><a href="#Spark-on-Yarn" class="headerlink" title="Spark on Yarn"></a>Spark on Yarn</h1><h3 id="SparkOnYarn本质"><a href="#SparkOnYarn本质" class="headerlink" title="SparkOnYarn本质"></a><strong>SparkOnYarn本质</strong></h3><p>master角色由yarn的Resourcemanager担任</p><p>worker角色由yarn的nodemanager担任</p><p>deiver角色运行在Yarn容器内或提交任务的客户端进程中</p><p>真正干活的Executor运行在yarn提供的容器内</p><h3 id="部署："><a href="#部署：" class="headerlink" title="部署："></a><strong>部署：</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在spark-env.sh上只要添加这两个即可</span></span><br><span class="line">HADOOP_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/root/software/hadoop/etc/hadoop</span><br><span class="line"><span class="comment"># 添加环境变量之后还要添加一个</span></span><br><span class="line"><span class="built_in">which</span> python</span><br><span class="line"><span class="built_in">export</span> SPARK_PYTHON=<span class="built_in">which</span> python</span><br><span class="line"><span class="comment"># 然后启动，bin/pyspark --master yarn</span></span><br></pre></td></tr></table></figure><h3 id="Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"><a href="#Spark-On-Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式" class="headerlink" title="Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式"></a><strong>Spark On Yarn是有两种运行模式的，一种是Cluster模式也就是集群模式，另一种是Client模式也就是客户端模式</strong></h3><h3 id="这两种模式的区别就是Driver运行的位置"><a href="#这两种模式的区别就是Driver运行的位置" class="headerlink" title="这两种模式的区别就是Driver运行的位置"></a><strong>这两种模式的区别就是Driver运行的位置</strong></h3><h3 id="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"><a href="#集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内" class="headerlink" title="集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内"></a><strong>集群模式的Driver运行在Yarn容器内部，和ApplicationMaster在同一个容器内</strong></h3><h3 id="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"><a href="#客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中" class="headerlink" title="客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中"></a><strong>客户端模式的Driver运行在客户端进程中，比如Driver运行在Spark-submit程序的进程中</strong></h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端模式</span></span><br><span class="line">SPARK HOME=/export/server/spark<span class="variable">$&#123;SPARK HOME&#125;</span>/bin/spark-submit\--master yarn</span><br><span class="line">--deploy-mode client \（默认是客户端模式，不加也可以）</span><br><span class="line"><span class="comment"># 下面的参数可加可不加</span></span><br><span class="line">--driver-memory 512m \</span><br><span class="line">--executor-memory 512m \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line"><span class="variable">$&#123;SPARK HOME&#125;</span>/examples/src/main/python/pi.py 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集群模式</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark搭建</title>
      <link href="/posts/fa65.html"/>
      <url>/posts/fa65.html</url>
      
        <content type="html"><![CDATA[<h1 id="spark安装部署"><a href="#spark安装部署" class="headerlink" title="spark安装部署"></a>spark安装部署</h1><h3 id="先安装anacondea3然后再解压spark"><a href="#先安装anacondea3然后再解压spark" class="headerlink" title="先安装anacondea3然后再解压spark"></a><strong>先安装anacondea3然后再解压spark</strong></h3><h3 id="然后追加以下内容至-x2F-root-x2F-condarc"><a href="#然后追加以下内容至-x2F-root-x2F-condarc" class="headerlink" title="然后追加以下内容至&#x2F;root&#x2F; .condarc"></a><strong>然后追加以下内容至&#x2F;root&#x2F; .condarc</strong></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - defaults</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/fastai/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br></pre></td></tr></table></figure><h3 id="配置pyspark"><a href="#配置pyspark" class="headerlink" title="配置pyspark"></a><strong>配置pyspark</strong></h3><h3 id="conda-create-n-pyspark-python-x3D-版本号然后回车就下载了"><a href="#conda-create-n-pyspark-python-x3D-版本号然后回车就下载了" class="headerlink" title="conda create -n pyspark python&#x3D;版本号然后回车就下载了"></a><strong>conda create -n pyspark python&#x3D;版本号然后回车就下载了</strong></h3><h3 id="安装完之后conda-activate-pyspark切换虚拟环境"><a href="#安装完之后conda-activate-pyspark切换虚拟环境" class="headerlink" title="安装完之后conda activate pyspark切换虚拟环境"></a><strong>安装完之后conda activate pyspark切换虚拟环境</strong></h3><h1 id="Local环境部署"><a href="#Local环境部署" class="headerlink" title="Local环境部署"></a><strong>Local环境部署</strong></h1><h3 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a><strong>添加环境变量</strong></h3><h2 id="如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath"><a href="#如果是without版本需要配置spark-env-sh中添加export-SPARK-DIST-CLASSPATH-x3D-x2F-root-x2F-software-x2F-hadoop-x2F-bin-x2F-hadoop-classpath" class="headerlink" title="如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)"></a><strong>如果是without版本需要配置spark-env.sh中添加export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;root&#x2F;software&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)</strong></h2><h3 id="然后就可以启动了pysparkspark-shell"><a href="#然后就可以启动了pysparkspark-shell" class="headerlink" title="然后就可以启动了pysparkspark shell"></a><strong>然后就可以启动了pysparkspark shell</strong></h3><h3 id="运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动"><a href="#运行的时候可以加参数pyspark-–master-local-括号中如果给-号就是全部资源启动，给数字的话就是num个线程启动" class="headerlink" title="运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动"></a><strong>运行的时候可以加参数pyspark –master local[*]括号中如果给*号就是全部资源启动，给数字的话就是num个线程启动</strong></h3><h3 id="Spark集群搭建"><a href="#Spark集群搭建" class="headerlink" title="Spark集群搭建"></a>Spark集群搭建</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line">vi spark-env.sh</span><br><span class="line">    <span class="comment"># 指定 Java Home</span></span><br><span class="line">    <span class="built_in">export</span> JAVA_HOME=/export/servers/jdk1.8.0_221</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指定 Spark Master 地址</span></span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_HOST=node01</span><br><span class="line">    <span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"></span><br><span class="line"><span class="built_in">mv</span> works.template works</span><br><span class="line">    将Localhost删了换成master slave1 slave2</span><br><span class="line">    </span><br><span class="line">配置 HistoryServer</span><br><span class="line"><span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line">vi spark-defaults.conf (将这两个前面<span class="comment">#去掉)</span></span><br><span class="line">    spark.eventLog.enabled  <span class="literal">true</span></span><br><span class="line">    spark.eventLog.<span class="built_in">dir</span>      hdfs://node01:8020/spark_log</span><br><span class="line">vi spark-env.sh末尾添加</span><br><span class="line">    <span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:8020/spark_log&quot;</span></span><br><span class="line"> 创建hdfs目录</span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /spark_log</span><br><span class="line"></span><br><span class="line">修改log4j2.properties.template改名为log4j2.properties然后将19行的info改为WARN</span><br><span class="line"></span><br><span class="line">然后分发</span><br><span class="line">启动历史服务器：sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">最后启动</span><br><span class="line">sbin/start-all.sh</span><br><span class="line">sbin/start-history-server.sh</span><br><span class="line"></span><br><span class="line">连接，在webui的那个</span><br><span class="line">pyspark --master spark://master:7077</span><br></pre></td></tr></table></figure><h2 id="spark基于zookeeper实现HA"><a href="#spark基于zookeeper实现HA" class="headerlink" title="spark基于zookeeper实现HA"></a><strong>spark基于zookeeper实现HA</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">前提确保zookeeper和hdfs均已启动</span><br><span class="line">vi spark-env.sh文件</span><br><span class="line">将<span class="built_in">export</span> SPARK_MASTER_HOST=master和他的那个端口注释了</span><br><span class="line">追加SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181,hadoop03:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span></span><br><span class="line">然后重新启动spark并且再启动另外一台或者多台机的start-master.sh</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Put事务和Take事务</title>
      <link href="/posts/fe67.html"/>
      <url>/posts/fe67.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">在source和chanel的传输中是批量过去的，channels传输到sinks也是批量的</span><br><span class="line"></span><br><span class="line">在source到channel中呢就有一个缓冲区</span><br><span class="line"></span><br><span class="line">1.doPut操作，浆皮数据写入到临时缓冲区 Putlist中</span><br><span class="line"></span><br><span class="line">2.doCommit操作：检查channel队列中是否有足够空间用来存放数据 有的话就会执行doCommit操作</span><br><span class="line"></span><br><span class="line">3.如果channel空间不够就会执行回滚数据的操作（doRollBack）</span><br><span class="line"></span><br><span class="line">在channel到sink中的事务叫take事务，也是批量过去的</span><br><span class="line"></span><br><span class="line">1.doTake操作，拉取一批channel的数据，然后进入缓冲区</span><br><span class="line"></span><br><span class="line">2.takeList操作，临时缓冲</span><br><span class="line"></span><br><span class="line">3.doCommit操作，将这一批数据发送出去，发送失败就会回滚</span><br><span class="line"></span><br><span class="line">4.doRollBack发送失败就进行回滚</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume source的type</title>
      <link href="/posts/fe55.html"/>
      <url>/posts/fe55.html</url>
      
        <content type="html"><![CDATA[<h5 id="不支持断点续传"><a href="#不支持断点续传" class="headerlink" title="不支持断点续传"></a>不支持断点续传</h5><h1 id="spooling-directory-source"><a href="#spooling-directory-source" class="headerlink" title="spooling directory source"></a>spooling directory source</h1><h2 id="监听某一个目录，只要目录下有文件，文件中的数据就会收集"><a href="#监听某一个目录，只要目录下有文件，文件中的数据就会收集" class="headerlink" title="监听某一个目录，只要目录下有文件，文件中的数据就会收集"></a>监听某一个目录，只要目录下有文件，文件中的数据就会收集</h2><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; spooldir a1.sources.r1.spoolDir &#x3D; &#x2F;root&#x2F;spool</p><p>a1.channels.c1.type &#x3D; memory a1.channels.c1.capacity &#x3D; 10000 a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>注意Dir大写</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：Flume exec sources监听命令</title>
      <link href="/posts/fe64.html"/>
      <url>/posts/fe64.html</url>
      
        <content type="html"><![CDATA[<h2 id="exec-sources"><a href="#exec-sources" class="headerlink" title="exec sources"></a>exec sources</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; exec</p><p>a1.sources.r1.command &#x3D; tail -f &#x2F;root&#x2F;exec.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>那个命令就是需要监听的命令</p><p>他是不断的去监听你文件添加了什么</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：Flume file channel</title>
      <link href="/posts/fe63.html"/>
      <url>/posts/fe63.html</url>
      
        <content type="html"><![CDATA[<h2 id="file-channel"><a href="#file-channel" class="headerlink" title="file channel"></a>file channel</h2><p>需要的参数 type &#x3D; file</p><p>dataDirs &#x3D; &#x2F;roort</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.channels.c1.type &#x3D; file&#x3D;&#x3D;<br>a1.channels.c1.capaciry&#x3D;10000<br>a1.channels.c1.transactioncapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：Flume avro sink</title>
      <link href="/posts/fe65.html"/>
      <url>/posts/fe65.html</url>
      
        <content type="html"><![CDATA[<h2 id="avro-sink"><a href="#avro-sink" class="headerlink" title="avro sink"></a>avro sink</h2><p>需要配置的：type &#x3D; avro</p><p>hostname &#x3D; 主机名</p><p>port  &#x3D; 端口</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; avro<br>a1.sources.r1.bind &#x3D; 192.168.1.122<br>a1.sources.r1.port &#x3D; 55555&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1 c2<br>a1.sinks &#x3D; k1 k2</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100<br>a1.channels.c2.type &#x3D; memory<br>a1.channels.c2.capacity &#x3D; 10000<br>a1.channels.c2.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; avro<br>a1.sinks.k1.hostname &#x3D; 192.168.1.123<br>a1.sinks.k1.port &#x3D; 55555<br>a1.sinks.k2.type &#x3D; avro<br>=&#x3D;a1.sinks.k2.hostname &#x3D; 192.168.1.124<br>a1.sinks.k2.port &#x3D; 55555</p><p>a1.sources.r1.channels &#x3D; c1 c2<br>a1.sinks.k1.channel &#x3D; c1<br>a1.sinks.k2.channel &#x3D; c2</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume file_roll sink</title>
      <link href="/posts/fe62.html"/>
      <url>/posts/fe62.html</url>
      
        <content type="html"><![CDATA[<h2 id="File-roll-sink"><a href="#File-roll-sink" class="headerlink" title="File_roll sink"></a>File_roll sink</h2><p>必须的：type &#x3D; file_roll</p><p>sink.directory 保存在那个目录&amp;#x20;</p><p>非必须：sink.rollInterval &#x3D; 30 就是每过30s就会生成一个新的文件用来存储数据</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; file_roll</p><p>a1.sinks.k1.sink.directory &#x3D; &#x2F;root&#x2F;file_roll&amp;#x20;</p><p>a1.sinks.k1.sink.rollInterval &#x3D; 10</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume http source 监听Http</title>
      <link href="/posts/fe59.html"/>
      <url>/posts/fe59.html</url>
      
        <content type="html"><![CDATA[<h2 id="http-source-监听Http"><a href="#http-source-监听Http" class="headerlink" title="http source 监听Http"></a>http source 监听Http</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; http<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试办法curl -X POST -d ‘[{“headers”:{“key”:”Flume”},”body”:”TestEvent1”}]’ <a href="http://master:4141/">http://master:4141/</a></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Sink Processors sink处理器</title>
      <link href="/posts/fe66.html"/>
      <url>/posts/fe66.html</url>
      
        <content type="html"><![CDATA[<h2 id="Sink-Processors-sink处理器"><a href="#Sink-Processors-sink处理器" class="headerlink" title="Sink Processors sink处理器"></a>Sink Processors sink处理器</h2><h3 id="failover-sink-Processor故障转移处理器"><a href="#failover-sink-Processor故障转移处理器" class="headerlink" title="failover sink Processor故障转移处理器"></a><em><strong>failover sink Processor故障转移处</strong>理器</em></h3><h4 id="amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"><a href="#amp-x9-1-可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力" class="headerlink" title="&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力"></a>&amp;#x9;1)可以同时指定多个sink输出，按照优先级高低进行数据的分发，并具有故障转移能力</h4><p>&amp;#x9;2)需要指定: processor.type  &#x3D; failover</p><p>&amp;#x9;processor.priority.&lt;sinkName&gt; &#x3D; 数字（就是优先级，数字越大优先级越高）</p><p>&amp;#x9;processor.maxpenalty &#x3D; 10000（就是允许你宕机以后给你重连的时间）</p><h3 id="Load-balancing-Sink-Processor"><a href="#Load-balancing-Sink-Processor" class="headerlink" title="Load balancing Sink Processor"></a>Load balancing Sink Processor</h3><p>负载平衡处理器提供了在多个sink负载平衡流量的能力。支持两种模式：round<em>robin and random。round</em>_robin可以将数据负载均衡到多个sink上，random支持随机分发到不同的sink上</p><p>第一种就是随机给你发送（random随机）</p><p>第二种就是负载均衡（robin负载均衡）</p><blockquote><p>a1.sources&#x3D;r1 <br>a1.sinks&#x3D;k1 k2 <br>a1.channels&#x3D;c1 <br><br>a1.sources.r1.type&#x3D;netcat <br>a1.sources.r1.bind&#x3D;worker-1 <br>a1.sources.r1.port&#x3D;44444 <br><br>a1.channels.c1.type&#x3D;memory <br>a1.channels.c1.capacity&#x3D;100000 <br>a1.channels.c1.transactionCapacity&#x3D;100 <br><br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-1<br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinks.k1.type&#x3D;avro <br>a1.sinks.k1.hostname &#x3D; worke-2 <br>a1.sinks.k1.port &#x3D; 55555 <br>a1.sinkgroups &#x3D; g1 <br>a1.sinkgroups.g1.sinks &#x3D; k1 k2 <br>a1.sinkgroups.g1.processor.type &#x3D; failover(故障转移)   or    load_<em>balance(随机)<br>a1.sinkgroups.g1.processor.selector &#x3D; random or    round</em>_<em>robin# 默认是故障转移的不写就是故障转移，写了就是随机random是随机的意思，而round</em>_robin是轮询的意思<br><br>a1.sources.r1.channels&#x3D;c1 <br>a1.sinks.k1.channel&#x3D;c1 <br>a1.sinks.k2.channel&#x3D;c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume kafka sink</title>
      <link href="/posts/fe57.html"/>
      <url>/posts/fe57.html</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-sink"><a href="#kafka-sink" class="headerlink" title="kafka sink"></a>kafka sink</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>=&#x3D;a1.sinks.k1.type &#x3D; org.apache.flume.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.server &#x3D; master<br>a1.sinks.k1.kafka.topic &#x3D; hkjcpdd&#x3D;&#x3D;</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>flume监听nginx的access.log文件给kafka消费</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; taildir<br>a1.sources.r1.filegroups &#x3D; f1<br>a1.sources.r1.filegroups.f1 &#x3D; &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;access.log</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; org.apache.flume.sink.kafka.KafkaSink<br>a1.sinks.k1.kafka.bootstrap.servers &#x3D; master:9092<br>a1.sinks.k1.kafka.topic&#x3D;hkjcpdd</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume source的type</title>
      <link href="/posts/fe56.html"/>
      <url>/posts/fe56.html</url>
      
        <content type="html"><![CDATA[<p>exec source 在启动时运行给定的unix命令，并期望该进程在标准输出上连续生成数据。</p><p>spooling directory source (spooldir) 这个source允许你把要手机的文件放入磁盘上的某个指定目录。他会监视这个目录中产生的新文件，并在新文件处显示从新文件中解析出来。 与exec source不同，spooling directory source是可靠的， 即使flume重新启动或被kill，也不会丢失数据，同时作为这种可靠的代价，指定目录中的被手机的文件必须是不可变的、唯一命名的。flume会自动检测避免这种情况发生，如果发现问题，则会抛出异常；</p><p>taildir source 监控指定的一些文件，并在检测新的一行数据残生的时候几乎实时的读取他们，如果新的一行数据还没写完，taildir source 等到这行写完后读取</p><p>kafka source 就是一个apache kafka消费者， 他从kafka的topic中读取消息，如果运行了多个Kafka source 则可以把他们配置到同一个消费者组，以便每个source都读取一组唯一的topic分区</p><p>syslog sources 针对系统日志</p><p>http sources 发送get协议请求的</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume的netcat 监听端口</title>
      <link href="/posts/fe61.html"/>
      <url>/posts/fe61.html</url>
      
        <content type="html"><![CDATA[<h1 id="netcat-agent"><a href="#netcat-agent" class="headerlink" title="netcat agent"></a>netcat agent</h1><p>配置文件如下</p><blockquote><p>a1.sources &#x3D; r1&amp;#x20;</p><p>a1.channels &#x3D; c1&amp;#x20;</p><p>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat&amp;#x20;</p><p>=&#x3D;a1.sources.r1.bind &#x3D; localhost &#x3D;&#x3D;</p><p>=&#x3D;a1.sources.r1.port &#x3D; 44444&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory&amp;#x20;</p><p>a1.channels.c1.capacity &#x3D; 10000&amp;#x20;</p><p>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1 a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>测试方法：telnet localhost 44444</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume kafka channel</title>
      <link href="/posts/fe58.html"/>
      <url>/posts/fe58.html</url>
      
        <content type="html"><![CDATA[<h2 id="kafka-channel"><a href="#kafka-channel" class="headerlink" title="kafka channel"></a>kafka channel</h2><p>需要指定的</p><blockquote><p>type &#x3D; org.apache.flume.channel.kafka.KafkaChannel</p><p>kafka.bootstrap.server &#x3D; hostname:port</p><p>kafka.topic &#x3D; hkjcpdd</p></blockquote><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; localhost<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; org.apache.flume.channel.kafka.KafkaChannel <br>a1.channels.c1.kafka.bootstrap.servers &#x3D; master:9092</p><p>a1.channels.c1.kafka.topic&#x3D;hkjmjj</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume</title>
      <link href="/posts/fe69.html"/>
      <url>/posts/fe69.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Channel Selectors通道选择器</title>
      <link href="/posts/fe69.html"/>
      <url>/posts/fe69.html</url>
      
        <content type="html"><![CDATA[<h3 id="Channel-Selectors通道选择器"><a href="#Channel-Selectors通道选择器" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><h3 id="Channel-Selectors通道选择器-1"><a href="#Channel-Selectors通道选择器-1" class="headerlink" title="Channel Selectors通道选择器"></a><strong>Channel Selectors通道选择器</strong></h3><p>多路复用通道选择器，source是通过event header来决定传输到哪一个channel。source是通过event header来决定传输到哪一个channel</p><p><img src="https://pic1.imgdb.cn/item/6784c37dd0e0a243d4f3d664.png"></p><p><strong>replicating type是复制选择器</strong></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume hdfs sink</title>
      <link href="/posts/fe60.html"/>
      <url>/posts/fe60.html</url>
      
        <content type="html"><![CDATA[<h2 id="hdfs-sink"><a href="#hdfs-sink" class="headerlink" title="hdfs sink"></a>hdfs sink</h2><p>hdfs sink是将flume收集到的数据写入到hdfs中，方便数据可靠的保存</p><p>sink输出到hdfs中，默认每10个event生成一个hdfs文件，hdfs文件目录会根据hdfs.path的配置自动创建</p><p>配置参数：</p><p>hdfs.pathhdfs目录路径</p><p>hdsf.filePrefix文件前缀，默认值是FlumeData</p><p>hdfs.fileSuffix文件后缀</p><p>hdfs.rollnterval就是滚动，设置多长时间创建一个新文件进行存储，默认是30s</p><p>hdfs.rollSize文件大小超过一定值后，然后再创建一个新文件进行存储，默认是1024</p><p>hdfs.rollCount写入了多少个事件然后再创建一个新文件进行存储，默认是10个，设置为0的话表示不基于事件个数</p><p>hdfs.file.Type文件格式，有三种格式可选择：SequenceFile（默认，二进制），DataStream(不压缩,以文本的形式【方便观察】)，CompressedStream（可压缩）</p><p>hdfs.batchSize批数次，HDFS Sink每次从Channel中拿的事件个数。默认值100（就是每100个事件就从sink中传到hdfs上一次）## 一般不设置</p><p>hdfs.maxOpenFiles允许最多打开的文件数，默认是5000。如果超过了这个值，越早的文件会被关闭 ## 一般不设置</p><p>hdfs.callTimeouthdfs允许操作的事件，比如hdfs文件的open, write, flush, close操作，单位是ms，默认值10000</p><p>hdfs.codeC压缩编解码器。以下之一：gzip, bzip2, lzo, lzop, snappy</p><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; hdfs <br>a1.sinks.k1.hdfs.path &#x3D; &#x2F;data&#x2F;hkjcpdd&#x2F;%Y-%m-%d <br>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true <br>a1.sinks.k1.hdfs.rollInterval&#x3D; 10 <br>a1.sinks.k1.hdfs.fileType &#x3D; DataStream</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true这个是使用本地的事件戳</p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume Interceptor</title>
      <link href="/posts/fe68.html"/>
      <url>/posts/fe68.html</url>
      
        <content type="html"><![CDATA[<h4 id="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"><a href="#就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理" class="headerlink" title="就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理"></a>就是拦截器，拦截器可以将flume收集到的event进行拦截，并使用对应的拦截器，对event进行简单修改，过滤。同时可以配置多个拦截器实现不同的功能，按照配置的先后顺序进行拦截处理</h4><p>timestamp Interceptor给event的头信息中添加时间戳</p><p>Static Interceptor 给event的头信息中添加自定义键值</p><p>Host Interceptor给event的头信息中添加主机名或者ip信息</p><p>Search and Replace Interceptor拦截信息进行匹配和替换</p><p>Regex File</p><h2 id="timestamp-interceptor-添加时间戳"><a href="#timestamp-interceptor-添加时间戳" class="headerlink" title="timestamp interceptor(添加时间戳)"></a>timestamp interceptor(添加时间戳)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; timestamp&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="host-interceptor（添加主机信息）"><a href="#host-interceptor（添加主机信息）" class="headerlink" title="host interceptor（添加主机信息）"></a>host interceptor（添加主机信息）</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1 &#x3D;&#x3D;<br>=&#x3D;a1.sources.r1.interceptors.i1.type &#x3D; host&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Static-Interceptor"><a href="#Static-Interceptor" class="headerlink" title="Static Interceptor"></a>Static Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>a1.sources.r1.interceptors  &#x3D; i1 i2 i3</p><p>a1.sources.r1.interceptors.i3.type &#x3D; static<br>a1.sources.r1.interceptors.i3.key &#x3D; name<br>a1.sources.r1.interceptors.i3.value &#x3D; zhangsan</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Search-and-Replace-Interceptor"><a href="#Search-and-Replace-Interceptor" class="headerlink" title="Search and Replace Interceptor"></a>Search and Replace Interceptor</h3><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; search_replace<br>a1.sources.r1.interceptors.i1.searchPattern&#x3D;[a-z]<br>a1.sources.r1.interceptors.i1.replaceString&#x3D;*&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h2 id="Regex-Filering-Interceptor-过滤的用正则"><a href="#Regex-Filering-Interceptor-过滤的用正则" class="headerlink" title="Regex Filering Interceptor(过滤的用正则)"></a>Regex Filering Interceptor(过滤的用正则)</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_filter<br>a1.sources.r1.interceptors.i1.regex&#x3D;^jp.*<br>a1.sources.r1.interceptors.i1.excludeEvents&#x3D;true&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><h3 id="Regex-Extractor-Interceptor-通过正则对event进行捕获"><a href="#Regex-Extractor-Interceptor-通过正则对event进行捕获" class="headerlink" title="Regex Extractor Interceptor(通过正则对event进行捕获)"></a>Regex Extractor Interceptor(通过正则对event进行捕获)</h3><blockquote><p>a1.sources &#x3D; r1 a1.channels &#x3D; c1 a1.sinks &#x3D; k1</p><p>a1.sources.r1.type &#x3D; netcat<br>a1.sources.r1.bind &#x3D; master<br>a1.sources.r1.port &#x3D; 44444</p><p>=&#x3D;a1.sources.r1.interceptors  &#x3D; i1<br>a1.sources.r1.interceptors.i1.type &#x3D; regex_extractor<br>a1.sources.r1.interceptors.i1.regex&#x3D;(^[a-zA-Z]&#x3D;&#x3D;<em>&#x3D;&#x3D;)\s([0-9]&#x3D;&#x3D;</em>&#x3D;&#x3D;$)<br>a1.sources.r1.interceptors.i1.serializers&#x3D;s1 s2<br>a1.sources.r1.interceptors.i1.serializers.s1.name&#x3D;word<br>a1.sources.r1.interceptors.i1.serializers.s2.name&#x3D;num&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume taildir agent监听一个或者多个文件</title>
      <link href="/posts/fe54.html"/>
      <url>/posts/fe54.html</url>
      
        <content type="html"><![CDATA[<h2 id="taildir-agent"><a href="#taildir-agent" class="headerlink" title="taildir agent"></a>taildir agent</h2><blockquote><p>a1.sources &#x3D; r1<br>a1.channels &#x3D; c1<br>a1.sinks &#x3D; k1</p><p>=&#x3D;a1.sources.r1.type &#x3D; taildir<br># 这个是用于断点续传，确保不被重复消费<br>a1.sources.r1.positionFile&#x3D;&#x2F;data&#x2F;flume&#x2F;position.json<br>a1.sources.r1.filegroups &#x3D; f1 a1.sources.r1.filegroups.f1 &#x3D; &#x2F;root&#x2F;taildir&#x2F;hkjcpdd.log&#x3D;&#x3D;</p><p>a1.channels.c1.type &#x3D; memory<br>a1.channels.c1.capacity &#x3D; 10000<br>a1.channels.c1.transactionCapacity &#x3D; 100</p><p>a1.sinks.k1.type &#x3D; logger</p><p>a1.sources.r1.channels &#x3D; c1<br>a1.sinks.k1.channel &#x3D; c1</p></blockquote><p>与之exev不同的是，他是监听的是文件内容，而exec是监听命令运行后的结果</p><p>监听多个文件的话那就多写几个</p><p>例如:</p><blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume</title>
      <link href="/posts/fe52.html"/>
      <url>/posts/fe52.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://pic1.imgdb.cn/item/6784c673d0e0a243d4f3d73f.png"></p>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例：将nginx的日志实时传输到hdfs和kafka上</title>
      <link href="/posts/fe66.html"/>
      <url>/posts/fe66.html</url>
      
        <content type="html"><![CDATA[<p>案例：将nginx的日志实时传输到hdfs和kafka上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type=taildir</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1=/usr/local/nginx/logs/access.log</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type=memory</span><br><span class="line">a1.channels.c1.capacity=10000</span><br><span class="line">a1.channels.c1.transactionCapacity=100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type=memory</span><br><span class="line">a1.channels.c2.capacity=10000</span><br><span class="line">a1.channels.c2.transactionCapacity=100</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=/data/hkjcpdd/%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=true</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval=10</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k2.kafka.bootstrap.servers = master:9092</span><br><span class="line">a1.sinks.k2.kafka.topic = hkjcpdd</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AzkabanAzkaban搭建</title>
      <link href="/posts/968.html"/>
      <url>/posts/968.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D-1024M并重启mysql服务"><a href="#1-在搭建之前得有mysql环境-以及my-cnf文件中添加max-allowed-packet-x3D-1024M并重启mysql服务" class="headerlink" title="1.在搭建之前得有mysql环境,以及my.cnf文件中添加max_allowed_packet&#x3D;1024M并重启mysql服务"></a>1.在搭建之前得有mysql环境,以及my.cnf文件中添加max_allowed_packet&#x3D;1024M并重启mysql服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置mysql相关部分(db)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入Mysql然后创建azkaban库</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入库并<span class="built_in">source</span>进来azkaban-db/create-all-sql-3.84.4.sql</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改以下配置项(<span class="built_in">exec</span>)</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">executor.port=12321</span><br><span class="line">jetty.port=8061</span><br><span class="line">azkaban.webserver.url=http://master:8061</span><br><span class="line"></span><br><span class="line">mysql.host=master</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进行分发到其他机器，然后每台都要启动</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">最后进行激活</span></span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;master:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;slave1:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br><span class="line">[root@master azkaban-exec-server]# curl -G &quot;slave2:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">&#123;&quot;status&quot;:&quot;success&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改一下配置项(web)</span></span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">jetty.port=8061</span><br><span class="line"></span><br><span class="line">mysql.host=master</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置azkaban-users.xml</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加一行即可</span></span><br><span class="line">&lt;user password=&quot;123456&quot; roles=&quot;admin&quot; username=&quot;hkjcpdd&quot;/&gt;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后启动即可</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban</title>
      <link href="/posts/969.html"/>
      <url>/posts/969.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> demo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的最简单案例</title>
      <link href="/posts/967.html"/>
      <url>/posts/967.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># first.project</span><br><span class="line">azkaban-flow-version: <span class="number">2</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># first.flow</span><br><span class="line"><span class="function">nodes:</span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobA</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">hkjcpdd</span>&quot;</span></span><br></pre></td></tr></table></figure><p>然后将这两个打包成zip然后上传至webui界面提交即可</p>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban的JavaProcess作业案例</title>
      <link href="/posts/966.html"/>
      <url>/posts/966.html</url>
      
        <content type="html"><![CDATA[<h2 id="JavaProcess作业类型案例"><a href="#JavaProcess作业类型案例" class="headerlink" title="JavaProcess作业类型案例"></a>JavaProcess作业类型案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JavaProcess类型可以运行一个自定义主类方法，<span class="built_in">type</span>类型为javaprocess,可用的配置为：</span></span><br><span class="line"></span><br><span class="line">​Xms: 最小堆</span><br><span class="line">​Xmx: 最大堆</span><br><span class="line">​classpath: 类路径</span><br><span class="line">​java.class: 要运行的Java对象，其中必须包含Main方法</span><br><span class="line">​main.args: main方法的参数</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">案例：</span></span><br><span class="line"></span><br><span class="line">1)新建一个azkaban的maven工程</span><br><span class="line">2）创建包名：com.hkjcpdd</span><br><span class="line">3)创建AzTest类</span><br><span class="line">4)打包jar包azkaban.jar</span><br><span class="line">5)新建testJava.flow</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">five.flow</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: test_java</span><br><span class="line">    type: javaprocess</span><br><span class="line">    config:</span><br><span class="line">      Xms: 96M</span><br><span class="line">      Xmx: 200M</span><br><span class="line">      java.class: com.hkjcpdd.TestJavaProcess</span><br><span class="line">      </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后将flow和jar和project打包在一起上传即可</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban作业依赖</title>
      <link href="/posts/965.html"/>
      <url>/posts/965.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># second.flow</span><br><span class="line"><span class="function">nodes:</span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobA</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">aaa</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobB</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">bbb</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">  - <span class="title">name</span>: <span class="title">jobC</span></span></span><br><span class="line"><span class="function">    <span class="title">type</span>: <span class="title">command</span></span></span><br><span class="line"><span class="function">    <span class="title">dependsOn</span>:</span></span><br><span class="line"><span class="function">        - <span class="title">jobA</span></span></span><br><span class="line"><span class="function">        - <span class="title">jobB</span></span></span><br><span class="line"><span class="function">    <span class="title">config</span>:</span></span><br><span class="line"><span class="function">      <span class="title">command</span>: <span class="title">echo</span> &quot;<span class="title">ccc</span>&quot;</span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function"> # 添加<span class="title">dependsOn</span>就可以了，这个用处就是等<span class="title">AB</span>完成后再执行<span class="title">C</span></span></span><br><span class="line"><span class="function"> # 然后将<span class="title">second.fow</span>和<span class="title">first.project</span>打包<span class="title">zip</span>上传至<span class="title">webui</span>即可</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban失败重试</title>
      <link href="/posts/958.html"/>
      <url>/posts/958.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">自动失败重试</span></span><br><span class="line">nodes:</span><br><span class="line">  - name: jobA</span><br><span class="line">  type: command</span><br><span class="line">  config:</span><br><span class="line">    command: sh /not exists.sh</span><br><span class="line">    retries: 3</span><br><span class="line">    retry.backoff: 10000</span><br><span class="line">    </span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">参数说明：</span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">retries: 重试次数</span></span><br><span class="line"><span class="meta prompt_"> # </span><span class="language-bash">retry.backoff: 重试的时间间隔</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">手动失败重试</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">理想的就是从成功的地方跳过，失败的地方重试</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">需求：JobA =&gt; JobB(依赖于A) =&gt; JobC =&gt; JobD =&gt; JobE =&gt; JobF。生产环境中，任何Job都有可能挂掉，可以根据需求执行想要的Job</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">直接从webui操作</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.<span class="built_in">history</span>-&gt;flow-&gt;Prepare execution-&gt;execute</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2.就是从头开始，但是把成功的<span class="built_in">disable</span>就可以了</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban定时执行案例</title>
      <link href="/posts/959.html"/>
      <url>/posts/959.html</url>
      
        <content type="html"><![CDATA[<h2 id="定时执行案例"><a href="#定时执行案例" class="headerlink" title="定时执行案例"></a>定时执行案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">需求：JobA每间隔一分钟执行一次</span></span><br><span class="line">就是执行任务的那个绿色按钮</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban条件工作流案例</title>
      <link href="/posts/962.html"/>
      <url>/posts/962.html</url>
      
        <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.运行时参数案例</span></span><br><span class="line">1) 基本原理</span><br><span class="line">  （1）父Job将参数写入JOB_OUTPUT_PROP_FILE环境变量所指向的文件</span><br><span class="line">  （2）子Job使用$&#123;jobName:param&#125;来获取父Job输出的参数并定义执行条件</span><br><span class="line">2）支持的条件运算符</span><br></pre></td></tr></table></figure><p><img src="https://pic1.imgdb.cn/item/6775fc45d0e0a243d4ed9ee4.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">案例需求：</span></span><br><span class="line">jobA执行一个shell脚本</span><br><span class="line">jobB执行一个shell脚本，但jobBJ不需要每天都执行，而只需要每个周一执行</span><br><span class="line"></span><br><span class="line">(1)新建jobA.sh</span><br><span class="line">(2)新建jobB.sh</span><br><span class="line">(3)condition.flow</span><br><span class="line">nodes:</span><br><span class="line">  - name: JobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh JobA.sh  </span><br><span class="line">  - name: JobB</span><br><span class="line">    type: command</span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobA</span><br><span class="line">    config:</span><br><span class="line">      command: sh JobB.sh</span><br><span class="line">    condition: $&#123;JobA:wk&#125;==1</span><br><span class="line">(4)将JobA.sh、JobB.sh、condition.flow和azkaban.project打包成condition.zip</span><br><span class="line">(5)创建condition项目=&gt;上传condition.zip文件=&gt;执行作业=&gt;观察结果</span><br><span class="line">(6)按照我们设定的条件，JobB会根据当日日期决定是否执行</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">预定义宏案例</span></span><br><span class="line"></span><br><span class="line">Azkaban中预置了几个特殊的判断条件，称为预定义宏</span><br><span class="line">预定义宏会根据所有父Job的完成情况进行判断，再决定是否执行。可用的预定义宏如下：</span><br><span class="line"></span><br><span class="line">- (1)all_success:表示父Job全部成功才执行（默认）</span><br><span class="line">- (2)all_done: 表示父Job全部完成才执行</span><br><span class="line">- (3)all_failed:表示父Job全部失败才执行</span><br><span class="line">- (4)one_success:表示父Job至少一个成功才执行</span><br><span class="line">- (5)one_failed: 表示父Job至少一个失败才执行</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">案例 需求：</span></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">JobA执行一个shell脚本</span><br><span class="line">JobB执行一个shell脚本</span><br><span class="line">JobC执行一个shell脚本</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">1.新建JobA.sh</span><br><span class="line">2.新建JobC.sh</span><br><span class="line">3.新建marco.flow</span><br><span class="line">nodes:</span><br><span class="line">  - name: JobA</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh aaa.sh</span><br><span class="line">  - name: JobB</span><br><span class="line">    type: command</span><br><span class="line">    config:</span><br><span class="line">      command: sh jobB.sh</span><br><span class="line">  - name: JobC</span><br><span class="line">    type: command</span><br><span class="line">    dependsOn:</span><br><span class="line">      - JobA</span><br><span class="line">      - JobB</span><br><span class="line">    config:</span><br><span class="line">      command: sh bbb.sh</span><br><span class="line">    condition: one_success</span><br><span class="line">4.打包上传，可以看到只要一个成功就可以执行jobC</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban邮件报警案例</title>
      <link href="/posts/964.html"/>
      <url>/posts/964.html</url>
      
        <content type="html"><![CDATA[<h2 id="邮件报警案例"><a href="#邮件报警案例" class="headerlink" title="邮件报警案例"></a>邮件报警案例</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">得先从邮箱那里开启STMP的服务，拿到授权码</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进入<span class="variable">$azkaban</span>-web/conf/azkaban.properties</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改22和23行</span></span><br><span class="line">mail.sender=邮箱地址</span><br><span class="line">mail.host=smtp.qq.com</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">加上</span></span><br><span class="line">mail.user=邮箱地址</span><br><span class="line">mail.password=授权码</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新启动azkaban-web</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动任务的地方左侧有个Notification（通知）</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后将邮箱地址放进那两个框，一个是失败发送给谁，一个是成功发送给谁，可以指定多个</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-匿名、柯里化、闭包</title>
      <link href="/posts/b9m1.html"/>
      <url>/posts/b9m1.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-匿名函数"><a href="#1-匿名函数" class="headerlink" title="1.匿名函数"></a>1.匿名函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 作为值的函数</span></span><br><span class="line"><span class="comment">// 定义列表，记录1-10之间的数据</span></span><br><span class="line"><span class="keyword">val</span> list1 = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="comment">// 创建函数对象，用来将int -&gt; String</span></span><br><span class="line"><span class="keyword">val</span> func = (x: <span class="type">Int</span>) =&gt; <span class="string">&quot;*&quot;</span> * x</span><br><span class="line"><span class="comment">//调用map方法，将第一步的列表转换成目标列表</span></span><br><span class="line"><span class="keyword">val</span> list2 = list1.map(func)</span><br><span class="line"><span class="comment">// 打印</span></span><br><span class="line">println(list2)</span><br><span class="line"><span class="comment">// 匿名函数</span></span><br><span class="line"><span class="keyword">val</span> list3 = list1.map((x: <span class="type">Int</span>) =&gt; <span class="string">&quot;*&quot;</span> * x)</span><br><span class="line"><span class="keyword">val</span> list3 = list1.map(_ =&gt; <span class="string">&quot;*&quot;</span> * _)</span><br><span class="line">println(list3)</span><br></pre></td></tr></table></figure><h2 id="2-柯里化"><a href="#2-柯里化" class="headerlink" title="2.柯里化"></a>2.柯里化</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需求，定义方法，完成两个字符串的拼接</span></span><br><span class="line"><span class="comment">// 方式1：普通方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergel</span></span>(s1: <span class="type">String</span>, s2: <span class="type">String</span>) = s1.toUpperCase + s2.toUpperCase</span><br><span class="line"><span class="comment">// 方式2：柯里化写法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge2</span></span>(s1: <span class="type">String</span>, s2: <span class="type">String</span>)(f1: (<span class="type">String</span>, <span class="type">String</span>) =&gt; <span class="type">String</span>) = f1(s1, s2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 柯里化</span></span><br><span class="line"><span class="comment">// 方式一：普通方法</span></span><br><span class="line"><span class="keyword">val</span> str1 = mergel(<span class="string">&quot;hkj&quot;</span>, <span class="string">&quot;cpdd&quot;</span>)</span><br><span class="line">println(str1)</span><br><span class="line"><span class="comment">// 方式2：柯里化方法</span></span><br><span class="line"><span class="keyword">val</span> str2 = merge2(<span class="string">&quot;hkj&quot;</span>,<span class="string">&quot;cpdd&quot;</span>)(_.toUpperCase + _.toUpperCase)</span><br></pre></td></tr></table></figure><h2 id="3-闭包"><a href="#3-闭包" class="headerlink" title="3.闭包"></a>3.闭包</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 闭包</span></span><br><span class="line"><span class="comment">// 需求： 定义一个函数，用来获取两个整数的和，通过闭包的形式实现</span></span><br><span class="line"><span class="comment">// 闭包指的是可以访问不在当前作用于范围数据的一个函数</span></span><br><span class="line"><span class="comment">// 定义变量x, 初始值为10</span></span><br><span class="line"><span class="keyword">val</span> x = <span class="number">10</span></span><br><span class="line"><span class="comment">// 定义函数getSum(), 用来获取两个整数的和</span></span><br><span class="line"><span class="keyword">val</span> getSum = (y: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">  x + y</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用getSum(), 并打印</span></span><br><span class="line">println(getSum(<span class="number">20</span>))</span><br></pre></td></tr></table></figure><h2 id="4-控制抽象"><a href="#4-控制抽象" class="headerlink" title="4.控制抽象"></a>4.控制抽象</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 控制抽象</span></span><br><span class="line"><span class="comment">// 控制抽象也是函数的一种，它可以让我们更加灵活的使用函数，假设函数a的参数列表需要接受一个函数b，且函数b没有输入值也没有返回值，那门函数a就被称之为控制抽象函数</span></span><br><span class="line"><span class="comment">// 定义一个函数myShop, 该函数接受一个无参数无返回值的函数</span></span><br><span class="line"><span class="keyword">val</span> myShop = (f1: () =&gt; <span class="type">Unit</span>) =&gt; &#123;</span><br><span class="line">  println(<span class="string">&quot;hkjcpdd&quot;</span>)</span><br><span class="line">  <span class="comment">// 在myShop函数中调用f1函数</span></span><br><span class="line">  f1() <span class="comment">//表示顾客具体购买的商品</span></span><br><span class="line">  println(<span class="string">&quot;hkjmjj&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 调用myShop函数</span></span><br><span class="line">myShop&#123;</span><br><span class="line">  () =&gt; &#123;</span><br><span class="line">    println(<span class="string">&quot;我要买hkj&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;我要买hkjcpdd&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;我要买hkjmjj&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;我要买hkjyjj&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-案例：计算器"><a href="#5-案例：计算器" class="headerlink" title="5.案例：计算器"></a>5.案例：计算器</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个方法，用来完成两个int类型数字的计算（加减乘除）</span></span><br><span class="line"><span class="comment">// 方式一：普通写法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a + b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a -b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a * b</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>) = a / b</span><br><span class="line"><span class="comment">// 方式二：通过柯里化方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>)(func: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>) = func(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 案例：计算器</span></span><br><span class="line">println(add(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">println(subtract(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">println(multiply(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">println(divide(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment">// 柯里化计算器</span></span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ + _))</span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ - _))</span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ * _))</span><br><span class="line">println(calculate(<span class="number">10</span>, <span class="number">5</span>)(_ / _))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-泛型</title>
      <link href="/posts/b0a7.html"/>
      <url>/posts/b0a7.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-泛型方法"><a href="#1-泛型方法" class="headerlink" title="1.泛型方法"></a>1.泛型方法</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛型-意思是指某种具体的数据类型</span></span><br><span class="line"><span class="comment">// 方式1：不采用泛型，即：普通的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMiddleElement</span></span>(arr: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Int</span> = arr(arr.length / <span class="number">2</span>)</span><br><span class="line"><span class="comment">// 方式二：采用自定义的泛型方法来实现</span></span><br><span class="line"><span class="comment">// T就是type单词的缩写</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMiddleElement1</span></span>[<span class="type">T</span>](arr: <span class="type">Array</span>[<span class="type">T</span>]) = arr(arr.length / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 泛型-意思是指某种具体的数据类型</span></span><br><span class="line"><span class="comment">// 测试getMiddleElement（）方法</span></span><br><span class="line">println(getMiddleElement(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)))</span><br><span class="line">println(getMiddleElement(<span class="type">Array</span>(<span class="string">&quot;hkjcpdd&quot;</span>, <span class="string">&quot;hkjmjj&quot;</span>, <span class="string">&quot;hkjljj&quot;</span>)))</span><br><span class="line">println(getMiddleElement1(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)))</span><br><span class="line">println(getMiddleElement1(<span class="type">Array</span>(<span class="string">&quot;hkjcpdd&quot;</span>, <span class="string">&quot;hkjmjj&quot;</span>, <span class="string">&quot;hkjljj&quot;</span>)))</span><br></pre></td></tr></table></figure><h2 id="2-泛型类"><a href="#2-泛型类" class="headerlink" title="2.泛型类"></a>2.泛型类</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛型类</span></span><br><span class="line"><span class="comment">// 定义一个Pair泛型类，该类包含两个字段，且两个字段的类型不固定</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pair</span>[<span class="type">T</span>](<span class="params">val message: <span class="type">T</span></span>) </span>&#123;</span><br><span class="line">  println(message)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 泛型类</span></span><br><span class="line"><span class="comment">// 创建不同类型的Pair泛型类对象，并打印</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">Pair</span>[<span class="type">Int</span>](<span class="number">1234</span>)</span><br><span class="line"><span class="keyword">new</span> <span class="type">Pair</span>[<span class="type">String</span>](<span class="string">&quot;hkjcpdd&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="3-泛型特质"><a href="#3-泛型特质" class="headerlink" title="3.泛型特质"></a>3.泛型特质</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛型特质</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Logger</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> a: <span class="type">T</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">show</span></span>() = println(a)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ConsoleLogger</span> <span class="keyword">extends</span> <span class="title">Logger</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> a: <span class="type">String</span> = <span class="string">&quot;hkjcpdd&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 泛型特质</span></span><br><span class="line"><span class="type">ConsoleLogger</span>.show()</span><br></pre></td></tr></table></figure><h2 id="4-泛型上下界之上界"><a href="#4-泛型上下界之上界" class="headerlink" title="4.泛型上下界之上界"></a>4.泛型上下界之上界</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">使用t &lt; 类型名表示给类型添加一个上界，表示泛型参数必须从该列（或本身）继承.</span><br><span class="line">格式：</span><br><span class="line">[<span class="type">T</span> &lt;: 类型]</span><br><span class="line">例如：[<span class="type">T</span> &lt;: <span class="type">Person</span>]的意思是，泛型<span class="type">T</span>的数据类型必须是<span class="type">Person</span>类型或者<span class="type">Person</span>的子类型</span><br><span class="line"></span><br><span class="line"><span class="comment">// 上界上界是包子不包父</span></span><br><span class="line"><span class="comment">// 1.定义一个Person类</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name: <span class="type">String</span></span><br><span class="line">  <span class="keyword">val</span> age: <span class="type">Int</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 2. 定义一个Student类，继承Person类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> name: <span class="type">String</span> = <span class="string">&quot;hkjcpdd&quot;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> age: <span class="type">Int</span> = <span class="number">12</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 3. 定义一个泛型方法demo(),该方法接受一个Array参数</span></span><br><span class="line"><span class="comment">// 4.限定Demo方法的Array元素类型只能是Person或者Person的子类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span></span>[<span class="type">T</span> &lt;: <span class="type">Person</span>](array: <span class="type">Array</span>[<span class="type">T</span>]): <span class="type">Unit</span> = println(array)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 上界</span></span><br><span class="line"><span class="keyword">val</span> s1 = <span class="keyword">new</span> <span class="type">Student</span>()</span><br><span class="line">demo(<span class="type">Array</span>(s1))</span><br></pre></td></tr></table></figure><h2 id="5-泛型上下界之下界"><a href="#5-泛型上下界之下界" class="headerlink" title="5.泛型上下界之下界"></a>5.泛型上下界之下界</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">使用<span class="type">T</span> &gt;: 数据类型 表示给类型添加一个下界，表示泛型参数必须是从类型本身或该类型的父类型.</span><br><span class="line">格式：</span><br><span class="line">[<span class="type">T</span> &gt;: 类型]</span><br><span class="line">注意：</span><br><span class="line"><span class="number">1.</span>例如：[<span class="type">T</span> &gt;: <span class="type">Person</span>]的意思是，泛型<span class="type">T</span>的数据烈性必须是<span class="type">Person</span>类型或者<span class="type">Person</span>的父类型</span><br><span class="line"><span class="number">2.</span>如果泛型既有上界又有下界。下界写在前面，上界写在后面，即：[<span class="type">T</span> &gt;:类型<span class="number">1</span> &lt; 类型<span class="number">2</span>]</span><br><span class="line"><span class="comment">// 下界</span></span><br><span class="line"><span class="comment">// 1.定义一个Person类</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person1</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name: <span class="type">String</span></span><br><span class="line">  <span class="keyword">val</span> age: <span class="type">Int</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 2. 定义一个Policeman类，继承Person类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policeman</span> <span class="keyword">extends</span> <span class="title">Person1</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> name: <span class="type">String</span> = <span class="string">&quot;hkjmjj&quot;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> age: <span class="type">Int</span> = <span class="number">14</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 3.定义一个Superman类，继承Policeman类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Superman</span> <span class="keyword">extends</span> <span class="title">Policeman</span></span></span><br><span class="line"><span class="comment">// 4.定义一个demo泛型方法，该方法接受一个Array参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo1</span></span>[<span class="type">T</span> &gt;: <span class="type">Policeman</span>](arr: <span class="type">Array</span>[<span class="type">T</span>]) = println(arr)</span><br><span class="line"><span class="comment">// 5.测试调用demo,传入不同元素类型的Array</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 下界</span></span><br><span class="line">demo1(<span class="type">Array</span>(<span class="keyword">new</span> <span class="type">Policeman</span>))</span><br><span class="line"><span class="comment">// 以下就不可以了，因为设置了下界，下界包父不包子</span></span><br><span class="line"><span class="comment">// demo1(Array(new Superman))</span></span><br></pre></td></tr></table></figure><h2 id="6-泛型注意点"><a href="#6-泛型注意点" class="headerlink" title="6.泛型注意点"></a>6.泛型注意点</h2><blockquote><p>[!NOTE]</p><p>如果泛型既有上界又有下界。下界写在前面，上界写在后面，即：[T &gt;:类型1 &lt; 类型2]</p><p>如果是同一个同事设置了上下界，其实就是直接固定类型，和直接在里面写类型没区别</p></blockquote><h2 id="7-协变、逆变、非变"><a href="#7-协变、逆变、非变" class="headerlink" title="7.协变、逆变、非变"></a>7.协变、逆变、非变</h2><blockquote><p>[!IMPORTANT]</p><ul><li>非变：类A和类B之间是父子类关系，但是Pair[A]和Pair[B]之间没有任何关系.</li><li>协变：类A和类B之间是父子类关系，Pair[A]和Pair[B]之间也有父子类关系.</li><li>逆变：类A和类B之间是父子类关系，但是Pair[A]和Pair[B]之间是子父关系.</li></ul></blockquote><p><img src="https://pic.imgdb.cn/item/674582b8d0e0a243d4d12c3e.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.定义一个Super类、以及一个Sub类继承自Super类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Super</span> <span class="comment">// 父类</span></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sub</span> <span class="keyword">extends</span> <span class="title">Super</span></span></span><br><span class="line"><span class="comment">// 2.使用协变、逆变、非变分别定义三个泛型类</span></span><br><span class="line"><span class="comment">// 非变</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Temp1</span>[<span class="type">T</span>]</span></span><br><span class="line"><span class="comment">// 协变</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Temp2</span>[+<span class="type">T</span>]</span></span><br><span class="line"><span class="comment">// 逆变</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Temp3</span>[-<span class="type">T</span>]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 分别创建泛型类对象来掩饰协、逆变、非变</span></span><br><span class="line"><span class="comment">// 3.1测试非变</span></span><br><span class="line"><span class="keyword">val</span> t1: <span class="type">Temp1</span>[<span class="type">Sub</span>] = <span class="keyword">new</span> <span class="type">Temp1</span>[<span class="type">Sub</span>]</span><br><span class="line">  <span class="keyword">val</span> t2: <span class="type">Temp1</span>[<span class="type">Super</span>] = t1 <span class="comment">// 编译报错，因为是非变，super和Sub有父子类关系，但是Temp1[Super]和Temp1[Sub]无关系</span></span><br><span class="line"><span class="comment">// 3.2测试协变</span></span><br><span class="line"><span class="keyword">val</span> t3:<span class="type">Temp2</span>[<span class="type">Sub</span>] = <span class="keyword">new</span> <span class="type">Temp2</span>[<span class="type">Sub</span>]</span><br><span class="line"><span class="keyword">val</span> t4:<span class="type">Temp2</span>[<span class="type">Super</span>] = t3  <span class="comment">// 协变，super和Sub有父子类关系，但是Temp2[Super]和Temp2[Sub]有父子类关系</span></span><br><span class="line"><span class="comment">// 3.3测试逆变</span></span><br><span class="line">  <span class="keyword">val</span> t5: <span class="type">Temp3</span>[<span class="type">Sub</span>] = <span class="keyword">new</span> <span class="type">Temp3</span>[<span class="type">Sub</span>]</span><br><span class="line">  <span class="keyword">val</span> t6: <span class="type">Temp3</span>[<span class="type">Super</span>] = t5 <span class="comment">// 编译报错，逆变是,super和Sub有父子类关系，但是Temp3[Super]和Temp3[Sub]变成子父类关系</span></span><br><span class="line"><span class="keyword">val</span> t7: <span class="type">Temp3</span>[<span class="type">Super</span>] = <span class="keyword">new</span> <span class="type">Temp3</span>[<span class="type">Super</span>]</span><br><span class="line"><span class="keyword">val</span> t8: <span class="type">Temp3</span>[<span class="type">Sub</span>] = t7 <span class="comment">// 逆变，不会报错，子父类</span></span><br></pre></td></tr></table></figure><h2 id="8-案例：列表去重排序"><a href="#8-案例：列表去重排序" class="headerlink" title="8.案例：列表去重排序"></a>8.案例：列表去重排序</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例：列表去重排序</span></span><br><span class="line"><span class="comment">// 一直当前项目下的data文件夹有一个1.txt文本文件</span></span><br><span class="line"><span class="comment">// 对上述数据去重排序后，重新写入到data文件夹下的2.txt文本文件中</span></span><br><span class="line"><span class="comment">// 1.定义数据源对象，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> source = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\123.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 2.从指定的文件中读取所有的对象</span></span><br><span class="line"><span class="keyword">val</span> list1 = source.mkString.split(<span class="string">&quot;\\s+&quot;</span>).toList</span><br><span class="line"><span class="comment">// 所有的数据 -&gt; 按照空白字符切割，Array(&quot;&quot;, &quot;&quot;...)</span></span><br><span class="line"><span class="comment">// println(list1)</span></span><br><span class="line"><span class="comment">// 3. 把List[String] -&gt; List[Int]</span></span><br><span class="line"><span class="keyword">val</span> list2: <span class="type">List</span>[<span class="type">Int</span>] = list1.map(_.toInt)</span><br><span class="line"><span class="comment">// 4. 把List[Int] -&gt; Set[Int], 对列表元素去重</span></span><br><span class="line"><span class="keyword">val</span> set1: <span class="type">Set</span>[<span class="type">Int</span>] = list2.toSet</span><br><span class="line"><span class="comment">// 5. Set[Int] -&gt; List[Int], 然后升序排列.</span></span><br><span class="line"><span class="keyword">val</span> list3: <span class="type">List</span>[<span class="type">Int</span>] = set1.toList.sorted</span><br><span class="line"><span class="comment">// 6. 把所有的数据写入到指定的目的地文件中</span></span><br><span class="line"><span class="comment">// 6.1 创建字符缓冲流，用来写入数据到自定的目的地文件中.</span></span><br><span class="line"><span class="keyword">val</span> bw = <span class="keyword">new</span> <span class="type">BufferedWriter</span>(<span class="keyword">new</span> <span class="type">FileWriter</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\123456.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 6.2 遍历list3列表，获取每一个数字</span></span><br><span class="line"><span class="keyword">for</span> (i &lt;- list3) &#123;</span><br><span class="line">  <span class="comment">// i 就表示列表中的每一个</span></span><br><span class="line">  <span class="comment">// 6.3 将获取到的数字转换成字符串在写入</span></span><br><span class="line">  bw.write(i.toString)</span><br><span class="line">  <span class="comment">// 6.4 记得加换行</span></span><br><span class="line">  bw.newLine()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 7. 释放资源.</span></span><br><span class="line">bw.close()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-隐式操作</title>
      <link href="/posts/n9a7.html"/>
      <url>/posts/n9a7.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-隐式转换-手动导入"><a href="#1-隐式转换-手动导入" class="headerlink" title="1.隐式转换-手动导入"></a>1.隐式转换-手动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 手动导入</span></span><br><span class="line"><span class="comment">// 定义 RichFile 类，用来丰富 File 类的功能</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile</span>(<span class="params">file: <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// 定义 read() 方法，用来讲数据读取到一个字符串中</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="type">Source</span>.fromFile(file).mkString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 定义单例对象 ImplicitDemo，该单例对象中有一个隐式转换方法</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImplicitDemo</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 隐式转换方法 file2RichFile，是用来将 File 对象转换成 RichFile 对象</span></span><br><span class="line">  <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">file2RichFile</span></span>(file: <span class="type">File</span>): <span class="type">RichFile</span> = <span class="keyword">new</span> <span class="type">RichFile</span>(file)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入隐式转换</span></span><br><span class="line"><span class="keyword">import</span> <span class="type">ImplicitDemo</span>.file2RichFile</span><br><span class="line"><span class="comment">// 创建普通的 File 对象，尝试调用其 read() 功能</span></span><br><span class="line"><span class="keyword">val</span> file = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line">  println(file.read())</span><br></pre></td></tr></table></figure><h2 id="2-隐式转换-自动导入"><a href="#2-隐式转换-自动导入" class="headerlink" title="2.隐式转换-自动导入"></a>2.隐式转换-自动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动导入隐式转换</span></span><br><span class="line"><span class="comment">// 定义一个RichFile类，里面定义一个read方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RichFile1</span>(<span class="params">file: <span class="type">File</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>() = <span class="type">Source</span>.fromFile(file).mkString</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自动导入隐式转换</span></span><br><span class="line"><span class="comment">// 需求：通过隐式转换，让file类的对象具有read功能</span></span><br><span class="line"><span class="comment">// 2.定义一个饮食转换方法，用来将普通的File对象-&gt;RichFile对象.</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">file2RichFile</span></span>(file: <span class="type">File</span>) = <span class="keyword">new</span> <span class="type">RichFile1</span>(file)</span><br><span class="line"><span class="comment">// 3. 创建file对象，尝试调用read（）方法.</span></span><br><span class="line"><span class="keyword">val</span> file1 = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="3-隐式参数-手动导入"><a href="#3-隐式参数-手动导入" class="headerlink" title="3.隐式参数-手动导入"></a>3.隐式参数-手动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 隐式参数: 如果方法的某个参数列表使用了implicit修饰，则该参数列表就是：隐式参数</span></span><br><span class="line"><span class="comment">// 手动导入</span></span><br><span class="line"><span class="comment">// 定义一个show 方法</span></span><br><span class="line">  delimit: (<span class="type">String</span>, <span class="type">String</span>)是个元组</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(name: <span class="type">String</span>)(<span class="keyword">implicit</span> delimit: (<span class="type">String</span>, <span class="type">String</span>)) = delimit._1 + name + delimit._2</span><br><span class="line"><span class="comment">// 定义一个单利对象，用来给隐式参数设置默认值</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ImplicitParam</span> </span>&#123;</span><br><span class="line">  <span class="keyword">implicit</span> <span class="keyword">val</span> delimit_default = (<span class="string">&quot;&lt;&lt;&lt;&quot;</span>, <span class="string">&quot;&gt;&gt;&gt;&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 隐式参数</span></span><br><span class="line"><span class="comment">// 1.手动导入隐式参数</span></span><br><span class="line"><span class="keyword">import</span> <span class="type">ImplicitParam</span>.delimit_default</span><br><span class="line"><span class="comment">// println(show(&quot;hkj&quot;))</span></span><br><span class="line">println(show(<span class="string">&quot;hkj&quot;</span>)(<span class="string">&quot;((&quot;</span>, <span class="string">&quot;))&quot;</span>))</span><br></pre></td></tr></table></figure><h2 id="4-隐式参数-自动导入"><a href="#4-隐式参数-自动导入" class="headerlink" title="4.隐式参数-自动导入"></a>4.隐式参数-自动导入</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动导入</span></span><br><span class="line"><span class="comment">// 定义show方法，接受一个姓名，再接受一个前缀和后缀</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show1</span></span>(name: <span class="type">String</span>)(<span class="keyword">implicit</span> delimit: (<span class="type">String</span>, <span class="type">String</span>)) = delimit._1 + name + delimit._2</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.自动导入隐式参数</span></span><br><span class="line"><span class="comment">// 通过隐式值，力给隐式参数设置初始值.</span></span><br><span class="line"><span class="comment">// 由程序自动导入的</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> delimit_default = <span class="string">&quot;&lt;&lt;&lt;&quot;</span> -&gt; <span class="string">&quot;&gt;&gt;&gt;&quot;</span></span><br><span class="line"><span class="comment">// 3. 调用show1（）方法，打印结果</span></span><br><span class="line">println(show1(<span class="string">&quot;hkj&quot;</span>))</span><br><span class="line">println(show1(<span class="string">&quot;hkj&quot;</span>)(<span class="string">&quot;&lt;&lt;&lt;&quot;</span> -&gt; <span class="string">&quot;&gt;&gt;&gt;&quot;</span>))</span><br></pre></td></tr></table></figure><h3 id="5-案例：获取列表元素平均值"><a href="#5-案例：获取列表元素平均值" class="headerlink" title="5.案例：获取列表元素平均值"></a>5.案例：获取列表元素平均值</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hkjcpdd.stu</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ClassDemo14_fin</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1.定义一个RichList类，用来给普通的List添加avg方法，用于获取列表元素的平均值</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">RichList</span>(<span class="params">list: <span class="type">List</span>[<span class="type">Int</span>]</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// 2.定义avg方法，用来获取List列表中的所有元素的平均值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">avg</span></span>() = &#123;</span><br><span class="line">      <span class="keyword">if</span> (list.isEmpty) <span class="type">None</span></span><br><span class="line">      <span class="keyword">else</span> <span class="type">Some</span>(list.sum / list.size)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 获取列表元素平均值</span></span><br><span class="line">    <span class="comment">// 3.定义隐式转换方法，用来将普通List独享转换为RichList对象</span></span><br><span class="line">    <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">list2RichList</span></span>(list:<span class="type">List</span>[<span class="type">Int</span>]) = <span class="keyword">new</span> <span class="type">RichList</span>(list)</span><br><span class="line">    <span class="comment">// 4. 定义List列表，获取其中所有元素的平均值</span></span><br><span class="line">    <span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">    println(list1.avg())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6.总结"></a>6.总结</h2><h3 id="隐式操作主要理解为："><a href="#隐式操作主要理解为：" class="headerlink" title="隐式操作主要理解为："></a>隐式操作主要理解为：</h3><h3 id="1-将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）"><a href="#1-将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）" class="headerlink" title="1.将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）"></a>1.将方法进行丰富（在需要时，编译器会自动将一种类型的元素转换为另一种类型，以便使用目标类型的功能）</h3><h3 id="2-将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）"><a href="#2-将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）" class="headerlink" title="2.将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）"></a>2.将元素进行元素转换（在函数调用时，编译器会自动查找并传递隐式参数，从而简化代码）</h3>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-递归</title>
      <link href="/posts/p9a7.html"/>
      <url>/posts/p9a7.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-递归"><a href="#1-递归" class="headerlink" title="1.递归"></a>1.递归</h2><h3 id="递归简介"><a href="#递归简介" class="headerlink" title="递归简介"></a>递归简介</h3><blockquote><p>[!IMPORTANT]</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 递归操作-是指方法自己调用自己的情况</span></span><br><span class="line"><span class="comment">// 递归必须有出口，否则容易造成死递归</span></span><br><span class="line"><span class="comment">// 递归必须要有规律</span></span><br><span class="line"><span class="comment">// 构造方法不能递归</span></span><br><span class="line"><span class="comment">// 递归方法必有返回值的数据类型</span></span><br></pre></td></tr></table></figure></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义一个show()方法，用来掩饰递归</span></span><br><span class="line"><span class="comment">// 定义变量，记录show()方法的调用次数</span></span><br><span class="line"><span class="keyword">var</span> count = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="comment">// 递归指的就是方法自己调用自己</span></span><br><span class="line">  <span class="comment">// 1.2打印show()方法的调用次数</span></span><br><span class="line">  println(<span class="string">s&quot;<span class="subst">$&#123;count&#125;</span>调用show()&quot;</span>)</span><br><span class="line">  <span class="comment">// 1.3修改count变量的值</span></span><br><span class="line">  count = count + <span class="number">1</span></span><br><span class="line">  show()</span><br><span class="line">  <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用show()方法</span></span><br><span class="line">show()</span><br></pre></td></tr></table></figure><h2 id="2-递归案例-阶乘"><a href="#2-递归案例-阶乘" class="headerlink" title="2.递归案例-阶乘"></a>2.递归案例-阶乘</h2><p> <img src="https://pic.imgdb.cn/item/67452c6788c538a9b5bbf858.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.递归案例：阶乘</span></span><br><span class="line"><span class="comment">// 1.定义方法factorial()，用来获取指定数字的阶乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span></span>(n: <span class="type">Int</span>):<span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="comment">// 出口</span></span><br><span class="line">  <span class="keyword">if</span> (n == <span class="number">1</span>) <span class="number">1</span></span><br><span class="line">  <span class="comment">// 规律</span></span><br><span class="line">  <span class="keyword">else</span> n * factorial(n - <span class="number">1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.递归案例：阶乘</span></span><br><span class="line"><span class="comment">// 2.调用factorial()方法，获取指定数字的阶乘</span></span><br><span class="line"><span class="keyword">val</span> result = factorial(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// 3. 打印结果</span></span><br><span class="line">println(result)</span><br></pre></td></tr></table></figure><h2 id="3-案列：斐波那契-不死神兔"><a href="#3-案列：斐波那契-不死神兔" class="headerlink" title="3.案列：斐波那契-不死神兔"></a>3.案列：斐波那契-不死神兔</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例二：斐波那契数列</span></span><br><span class="line"><span class="comment">// 已知数列1, 1, 2, 3, 5, 8, 13... 问第十二个是多少</span></span><br><span class="line"><span class="comment">// 出口：第一个月和第二个月的兔子对数都是 1</span></span><br><span class="line"><span class="comment">// 规律从第三个月开始，每月的兔子对数 = 它前两个月的兔子对数之和.</span></span><br><span class="line"><span class="comment">// 1.定义方法rabbit(), 用来获取兔子的对数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rabbit</span></span>(month: <span class="type">Int</span>): <span class="type">Int</span>= &#123;</span><br><span class="line">  <span class="comment">// 出口</span></span><br><span class="line">  <span class="keyword">if</span> (month == <span class="number">1</span> || month == <span class="number">2</span>) <span class="number">1</span></span><br><span class="line">  <span class="comment">// 规律</span></span><br><span class="line">  <span class="keyword">else</span> rabbit(month - <span class="number">1</span>) + rabbit(month - <span class="number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 案例二：斐波那契数列</span></span><br><span class="line"><span class="comment">// 2.调用方法，获取第12个月的兔子对数</span></span><br><span class="line"><span class="keyword">val</span> result1 = rabbit(<span class="number">12</span>)</span><br><span class="line"><span class="comment">// 3. 打印</span></span><br><span class="line">println(result1)</span><br></pre></td></tr></table></figure><h2 id="4-案例：打印目录文件"><a href="#4-案例：打印目录文件" class="headerlink" title="4.案例：打印目录文件"></a>4.案例：打印目录文件</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例三：打印目录文件</span></span><br><span class="line"><span class="comment">// 1.定义printFile(dir: File), 用来打印该目录下所有的文件路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printFile</span></span>(dir: <span class="type">File</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 1.1 判断用户传入的路径是否是文件夹路径，如果不是，直接提示</span></span><br><span class="line">  <span class="keyword">if</span> (!dir.isDirectory) &#123;</span><br><span class="line">    println(<span class="string">&quot;您录入的路径不合法，不是文件夹的路径.&quot;</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 1.2 如果用户录入的是文件夹路径，程序继续执行</span></span><br><span class="line">    <span class="comment">// listFile就是dir目录下的每一个文件或者是文件夹对象</span></span><br><span class="line">    <span class="comment">// 1.3 通过File#listFile()获取到该目录下所有的文件或者文件夹的file对象形式</span></span><br><span class="line">    <span class="keyword">val</span> listFiles = dir.listFiles()</span><br><span class="line">    <span class="comment">// 1.4 遍历，获取到上一部(1.3) 获取到的每一个File对象</span></span><br><span class="line">    <span class="keyword">for</span> (listFile &lt;- listFiles) &#123;</span><br><span class="line">      <span class="comment">// listFile：表示dir目录下每一个具体的文件或者是文件夹对象</span></span><br><span class="line">      <span class="comment">// 1.5 判断，如果是文件，就直接输出 就是出口</span></span><br><span class="line">      <span class="keyword">if</span> (listFile.isFile) println(listFile)</span><br><span class="line">      <span class="comment">// 1.6 如果是文件夹路径，就递归 就是规律</span></span><br><span class="line">      <span class="keyword">else</span> printFile(listFile)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 案例三：打印目录文件</span></span><br><span class="line"><span class="comment">// 2.调用printFile()方法</span></span><br><span class="line">println(printFile(<span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目&quot;</span>)))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala-读取写入序列化</title>
      <link href="/posts/b9a6.html"/>
      <url>/posts/b9a6.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-读取数据"><a href="#1-读取数据" class="headerlink" title="1.读取数据"></a>1.读取数据</h2><h3 id="1-1按行读取"><a href="#1-1按行读取" class="headerlink" title="1.1按行读取"></a>1.1按行读取</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 按行读取</span></span><br><span class="line"><span class="comment">// 创建Source对象，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> source = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 以行为单位，来读取数据</span></span><br><span class="line"><span class="keyword">val</span> lines: <span class="type">Iterator</span>[<span class="type">String</span>] = source.getLines()</span><br><span class="line"><span class="comment">// 将读取到的数据封装到list列表中</span></span><br><span class="line"><span class="keyword">val</span> list: <span class="type">List</span>[<span class="type">String</span>] = lines.toList</span><br><span class="line"><span class="comment">// 打印结果</span></span><br><span class="line"><span class="keyword">for</span> (data &lt;- list) println(data)</span><br><span class="line"><span class="comment">// 关闭Source对象</span></span><br><span class="line">source.close()</span><br></pre></td></tr></table></figure><h3 id="1-2按字符读取"><a href="#1-2按字符读取" class="headerlink" title="1.2按字符读取"></a>1.2按字符读取</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 按字符读取</span></span><br><span class="line"><span class="keyword">val</span> source1 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 拆干见source对象关联数据源</span></span><br><span class="line"><span class="keyword">val</span> iter: <span class="type">BufferedIterator</span>[<span class="type">Char</span>] = source1.buffered</span><br><span class="line"><span class="comment">// 已字符为单位来读取数据</span></span><br><span class="line"><span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">  <span class="comment">// 打印读取到的数据 hasNext(), next()</span></span><br><span class="line">  print(iter.next())</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 关闭source对象</span></span><br><span class="line">source.close()</span><br></pre></td></tr></table></figure><h3 id="1-3优化版（字符串版）"><a href="#1-3优化版（字符串版）" class="headerlink" title="1.3优化版（字符串版）"></a>1.3优化版（字符串版）</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 优化版，如果文件中的内容较少，我们可以直接把它读取到一个字符串中</span></span><br><span class="line"><span class="keyword">val</span> source2 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 将数据读取到一个字符串中</span></span><br><span class="line"><span class="keyword">val</span> str: <span class="type">String</span> = source2.mkString</span><br><span class="line"><span class="comment">// 打印</span></span><br><span class="line">println(str)</span><br><span class="line"><span class="comment">// 关闭source对象</span></span><br><span class="line">source2.close()</span><br></pre></td></tr></table></figure><h3 id="1-4读取词法单词和数字"><a href="#1-4读取词法单词和数字" class="headerlink" title="1.4读取词法单词和数字"></a>1.4读取词法单词和数字</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取词法单元和数字</span></span><br><span class="line"><span class="keyword">val</span> source3 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\2.txt&quot;</span>)</span><br><span class="line"><span class="comment">//将其所有数据封装到一个字符串中</span></span><br><span class="line"><span class="keyword">val</span> str1 = source3.mkString</span><br><span class="line"><span class="comment">// 按照空白字符进行切割，获取到字符串数组</span></span><br><span class="line"><span class="keyword">val</span> strArray: <span class="type">Array</span>[<span class="type">String</span>] = str1.split(<span class="string">&quot;\\s+&quot;</span>)</span><br><span class="line"><span class="comment">// 将上诉的字符串数组转换成int类型的数组</span></span><br><span class="line"><span class="keyword">val</span> intArray: <span class="type">Array</span>[<span class="type">Int</span>] = strArray.map(_.toInt)</span><br><span class="line"><span class="keyword">for</span> (data &lt;- intArray) print(data + <span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="comment">// 关闭source</span></span><br><span class="line">source3.close()</span><br></pre></td></tr></table></figure><h3 id="1-5从Url或者其他源中读取数据"><a href="#1-5从Url或者其他源中读取数据" class="headerlink" title="1.5从Url或者其他源中读取数据"></a>1.5从Url或者其他源中读取数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从url或者其他源读取数据</span></span><br><span class="line"><span class="comment">//读取传智播客官网的数据</span></span><br><span class="line"><span class="keyword">val</span> source4 = <span class="type">Source</span>.fromURL(<span class="string">&quot;https://www.itcast.cn&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> str2 = source4.mkString</span><br><span class="line">println(str2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 直接从字符串 黑马程序员中读取数据</span></span><br><span class="line"><span class="keyword">val</span> source5 = <span class="type">Source</span>.fromString(<span class="string">&quot;黑马程序员&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> str3 = source5.mkString</span><br><span class="line">println(str3)</span><br></pre></td></tr></table></figure><h3 id="1-6读取二进制文件"><a href="#1-6读取二进制文件" class="headerlink" title="1.6读取二进制文件"></a>1.6读取二进制文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取二进制文件</span></span><br><span class="line"><span class="comment">// 创建file对象，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> file = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\1732176591422.png&quot;</span>)</span><br><span class="line"><span class="comment">// 创建字节输入流，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> fis = <span class="keyword">new</span> <span class="type">FileInputStream</span>(file)</span><br><span class="line"><span class="comment">// 创建字节数组，用来存储读取到的内容</span></span><br><span class="line"><span class="keyword">val</span> bys = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Byte</span>](file.length().toInt)</span><br><span class="line"><span class="comment">// 开始读取，将读取到的数据存储到字节数组中，病返回读取到的有效字节数</span></span><br><span class="line"><span class="keyword">val</span> len = fis.read(bys)</span><br><span class="line"><span class="comment">// 打印结果</span></span><br><span class="line">println(<span class="string">&quot;读取到的有效字节数：&quot;</span> + len)</span><br><span class="line">println(<span class="string">&quot;字节数组的长度&quot;</span> + bys.length)</span><br><span class="line"><span class="comment">// 关闭</span></span><br><span class="line">fis.close()</span><br></pre></td></tr></table></figure><h2 id="2-写入数据"><a href="#2-写入数据" class="headerlink" title="2.写入数据"></a>2.写入数据</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 写入数据</span></span><br><span class="line"><span class="comment">// 创建字节输出流对象，关联目的地文件</span></span><br><span class="line"><span class="keyword">val</span> fos = <span class="keyword">new</span> <span class="type">FileOutputStream</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\4.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 直接往目的地文件中编写指定的内容</span></span><br><span class="line">fos.write(<span class="string">&quot;黄凯君cpdd\r\n 黄凯君mjj&quot;</span>.getBytes)</span><br><span class="line"><span class="comment">// 关闭字节输出流</span></span><br><span class="line">fos.close()</span><br></pre></td></tr></table></figure><h2 id="3-序列化和反序列化"><a href="#3-序列化和反序列化" class="headerlink" title="3.序列化和反序列化"></a>3.序列化和反序列化</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 序列化和反序列化</span></span><br><span class="line"><span class="comment">// 演示序列化操作，即：将对象写入到文件中.</span></span><br><span class="line"><span class="comment">// 创建Person类型的对象</span></span><br><span class="line"><span class="keyword">val</span> p = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">&quot;黄凯君&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="comment">// 创建序列哈流，用来讲对象写入到文件中</span></span><br><span class="line"><span class="keyword">val</span> oos = <span class="keyword">new</span> <span class="type">ObjectOutputStream</span>(<span class="keyword">new</span> <span class="type">FileOutputStream</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\5.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 调用writeObject()方法， 将对象写入到文件中</span></span><br><span class="line">oos.writeObject(p)</span><br><span class="line">oos.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 反序列化操作，即：从文件中直接读取对象</span></span><br><span class="line"><span class="comment">// 创建反序列化流，关联数据源文件</span></span><br><span class="line"><span class="keyword">val</span> ois = <span class="keyword">new</span> <span class="type">ObjectInputStream</span>(<span class="keyword">new</span> <span class="type">FileInputStream</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\5.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 调用readObject()方法，从数据源文件中读取指定的对象</span></span><br><span class="line"><span class="comment">// 细节：我们获取到的对象是AnyRef类型，所以需要转换成Person类型</span></span><br><span class="line"><span class="keyword">val</span> p1 = ois.readObject().asInstanceOf[<span class="type">Person</span>]</span><br><span class="line"><span class="comment">// 打印结果</span></span><br><span class="line">println(p1.name, p1.age)</span><br><span class="line"><span class="comment">// 关闭</span></span><br><span class="line">ois.close()</span><br></pre></td></tr></table></figure><h2 id="案例：学生成绩单"><a href="#案例：学生成绩单" class="headerlink" title="案例：学生成绩单"></a>案例：学生成绩单</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 案例：学院成绩表</span></span><br><span class="line"><span class="comment">// 创建source对象并且关联数据源</span></span><br><span class="line"><span class="keyword">val</span> source9 = <span class="type">Source</span>.fromFile(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\Student.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 讲数据原一行一行读出来，并且已空格为分割符存入数组</span></span><br><span class="line"><span class="keyword">val</span> stuArray = source9.getLines().map(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="comment">// 创建可变列表</span></span><br><span class="line"><span class="keyword">val</span> studentList = <span class="type">ListBuffer</span>[<span class="type">Student</span>]()</span><br><span class="line"><span class="comment">// 将数据数组中的每一个存入列表中 这里的转换是方便添加总成绩进去，list方便动态加入</span></span><br><span class="line"><span class="keyword">for</span> (s &lt;- stuArray) &#123;</span><br><span class="line">  studentList += <span class="keyword">new</span> <span class="type">Student</span>(s(<span class="number">0</span>), s(<span class="number">1</span>).toInt, s(<span class="number">2</span>).toInt, s(<span class="number">3</span>).toInt)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将列表中的数据按照总分降序排列</span></span><br><span class="line"><span class="keyword">val</span> sortList = studentList.sortBy(_.getSum).reverse.toList</span><br><span class="line"><span class="comment">// 将处理好的数据存入指定位置</span></span><br><span class="line"><span class="keyword">val</span> bw = <span class="keyword">new</span> <span class="type">BufferedWriter</span>(<span class="keyword">new</span> <span class="type">FileWriter</span>(<span class="string">&quot;J:\\大数据第一代项目\\scalaFlink\\data\\Student1.txt&quot;</span>))</span><br><span class="line"><span class="comment">// 将数据写入</span></span><br><span class="line"><span class="keyword">for</span> (s &lt;- sortList) &#123;</span><br><span class="line">  <span class="comment">// s 表示排序后每一个学生的信息</span></span><br><span class="line">  bw.write(<span class="string">s&quot;<span class="subst">$&#123;s.name&#125;</span>, <span class="subst">$&#123;s.Chinese&#125;</span>, <span class="subst">$&#123;s.Math&#125;</span>, <span class="subst">$&#123;s.English&#125;</span>, <span class="subst">$&#123;s.getSum&#125;</span>&quot;</span>)</span><br><span class="line">  <span class="comment">// 每写完一个学生进行换行</span></span><br><span class="line">  bw.newLine()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 关闭</span></span><br><span class="line">bw.close()</span><br><span class="line">source9.close()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala模式匹配</title>
      <link href="/posts/h2m8.html"/>
      <url>/posts/h2m8.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-模式匹配"><a href="#1-模式匹配" class="headerlink" title="1.模式匹配"></a>1.模式匹配</h2><h3 id="1-1简单模式匹配"><a href="#1-1简单模式匹配" class="headerlink" title="1.1简单模式匹配"></a>1.1简单模式匹配</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 简单模式匹配</span></span><br><span class="line">print(<span class="string">&quot;请输入一个字符串：&quot;</span>)</span><br><span class="line"><span class="comment">//    val UserStdIn: Any = StdIn.readLine()</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">UserStdIn</span>: <span class="type">Any</span> = <span class="string">&quot;hadoop&quot;</span></span><br><span class="line"><span class="type">UserStdIn</span> <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;hadoop&quot;</span> =&gt; println(<span class="string">&quot;你真是个天才&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;flume&quot;</span> =&gt; println(<span class="string">&quot;你也是个天才&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;flink&quot;</span> =&gt; println(<span class="string">&quot;人才&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;默认项&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-2匹配类型"><a href="#1-2匹配类型" class="headerlink" title="1.2匹配类型"></a>1.2匹配类型</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配类型</span></span><br><span class="line"><span class="type">UserStdIn</span> <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> _: <span class="type">String</span> =&gt; println(<span class="string">&quot;这是一个字符串&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _: <span class="type">Int</span> =&gt; println(<span class="string">&quot;这是一个整形&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _: <span class="type">Char</span> =&gt; println(<span class="string">&quot;这是一个字符&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;默认项&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-3守卫"><a href="#1-3守卫" class="headerlink" title="1.3守卫"></a>1.3守卫</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 守卫 所谓的守卫指的是在case语句中添加if条件判断，这样可以让我们的代码更简洁，更优雅</span></span><br><span class="line"><span class="comment">//    val NumStdin = StdIn.readInt()</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">NumStdin</span> = <span class="number">22</span></span><br><span class="line"><span class="type">NumStdin</span> <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> (x % <span class="number">2</span> == <span class="number">0</span>) =&gt; println(<span class="string">&quot;是个偶数&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> (x == <span class="number">0</span>) =&gt; println(<span class="string">&quot;是零&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;是别的数字&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-4匹配样例类"><a href="#1-4匹配样例类" class="headerlink" title="1.4匹配样例类"></a>1.4匹配样例类</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配样例类  可以快速拿到传入类的参数</span></span><br><span class="line"><span class="keyword">val</span> c: <span class="type">Any</span> = <span class="type">Customer</span>(<span class="string">&quot;张三&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="keyword">val</span> o: <span class="type">Any</span> = <span class="type">Order</span>(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">c <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Customer</span>(name, age) =&gt; println(<span class="string">s&quot;Customer类型的对象, name=<span class="subst">$&#123;name&#125;</span>, age=<span class="subst">$&#123;age&#125;</span>&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Order</span>(id) =&gt; println(<span class="string">s&quot;Order类型的对象, id=<span class="subst">$&#123;id&#125;</span>&quot;</span>)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;未匹配&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="1-5匹配数组、元祖、集合、列表、映射、集"><a href="#1-5匹配数组、元祖、集合、列表、映射、集" class="headerlink" title="1.5匹配数组、元祖、集合、列表、映射、集"></a>1.5匹配数组、元祖、集合、列表、映射、集</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配数组、元祖、集合、列表、映射、集</span></span><br><span class="line"><span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>)</span><br><span class="line"><span class="keyword">val</span> arr2 = <span class="type">Array</span>(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">val</span> arr3 = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">arr1 <span class="keyword">match</span> &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Array</span>(<span class="number">1</span>, x, y) =&gt; println(<span class="string">s&quot;匹配到数组：长度为3,首元素为1，剩下两个元素无所谓，这里剩下的两个元素分别是：<span class="subst">$&#123;x&#125;</span>, <span class="subst">$&#123;y&#125;</span>&quot;</span>)</span><br><span class="line">     <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>) =&gt; println(<span class="string">&quot;匹配到数组：长度为1，且只有一个元素0&quot;</span>)</span><br><span class="line">     <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>, _*) =&gt; println(<span class="string">&quot;匹配到数组：以元素0开头，后边的元素无所谓&quot;</span>)</span><br><span class="line">     <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;未匹配&quot;</span>)</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="1-6匹配列表"><a href="#1-6匹配列表" class="headerlink" title="1.6匹配列表"></a>1.6匹配列表</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配列表</span></span><br><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">val</span> list2 = (<span class="number">0</span> to <span class="number">100</span>).toList</span><br><span class="line"><span class="keyword">val</span> list3 = <span class="type">List</span>(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">list1 <span class="keyword">match</span> &#123;</span><br><span class="line">   <span class="keyword">case</span> <span class="number">0</span> :: <span class="type">Nil</span> =&gt; println(<span class="string">&quot;只包含一个0&quot;</span>)</span><br><span class="line">   <span class="keyword">case</span> <span class="number">0</span> :: tail =&gt; println(<span class="string">&quot;以0开头的列表，数量不固定&quot;</span>)</span><br><span class="line">   <span class="keyword">case</span> x :: y :: <span class="type">Nil</span> =&gt; println(<span class="string">s&quot;只有两个数字的列表，数字分别是: <span class="subst">$&#123;x&#125;</span>, <span class="subst">$&#123;y&#125;</span>&quot;</span>)</span><br><span class="line">   <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;不匹配&quot;</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="1-7匹配元组"><a href="#1-7匹配元组" class="headerlink" title="1.7匹配元组"></a>1.7匹配元组</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配元祖</span></span><br><span class="line"><span class="keyword">val</span> tuple1 = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> tuple2 = (<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> tuple3 = (<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">tuple1 <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> (<span class="number">1</span>, x, y) =&gt; println(<span class="string">s&quot;以1开头的元祖，一共三个元素，另外两个元素为<span class="subst">$&#123;x&#125;</span>, <span class="subst">$&#123;y&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> (x, y, <span class="number">5</span>) =&gt; println(<span class="string">&quot;一共三个元素，最后一个元素为5的元祖&quot;</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;不匹配&quot;</span>)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="1-8变量生命中的模式匹配"><a href="#1-8变量生命中的模式匹配" class="headerlink" title="1.8变量生命中的模式匹配"></a>1.8变量生命中的模式匹配</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 变量声明中的模式匹配</span></span><br><span class="line"><span class="keyword">val</span> arr4 = (<span class="number">0</span> to <span class="number">10</span>).toArray</span><br><span class="line"><span class="comment">// 获得第二个，第三个，第四个元素</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(_, x, y, z, _*) = arr4</span><br><span class="line">println(x, y, z)</span><br><span class="line"><span class="comment">// 获得第一个，第二个元素</span></span><br><span class="line"><span class="keyword">val</span> list5 = (<span class="number">0</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="keyword">val</span> <span class="type">List</span>(a, b, _*) = list5</span><br><span class="line">println(a, b)</span><br></pre></td></tr></table></figure><h3 id="1-9匹配for表达式"><a href="#1-9匹配for表达式" class="headerlink" title="1.9匹配for表达式"></a>1.9匹配for表达式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 匹配for表达式</span></span><br><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>(<span class="string">&quot;张三&quot;</span> -&gt; <span class="number">13</span>, <span class="string">&quot;李四&quot;</span> -&gt; <span class="number">34</span>, <span class="string">&quot;王五&quot;</span> -&gt; <span class="number">34</span>, <span class="string">&quot;赵柳&quot;</span> -&gt; <span class="number">44</span>)</span><br><span class="line"><span class="comment">// 方式1</span></span><br><span class="line"><span class="keyword">for</span> ((k, v) &lt;- map1 <span class="keyword">if</span> (v == <span class="number">34</span>)) println(k, v)</span><br><span class="line"><span class="comment">// 方式2</span></span><br><span class="line"><span class="keyword">for</span> ((k, <span class="number">34</span>) &lt;- map1) println(k, <span class="number">34</span>)</span><br></pre></td></tr></table></figure><h3 id="2-Option类型"><a href="#2-Option类型" class="headerlink" title="2.Option类型"></a>2.Option类型</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">divide</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>)= &#123;</span><br><span class="line">  <span class="keyword">if</span> (b == <span class="number">0</span>) <span class="type">None</span></span><br><span class="line">  <span class="keyword">else</span> <span class="type">Some</span>(a / b)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 思路一</span></span><br><span class="line"><span class="keyword">val</span> result1 = divide(<span class="number">10</span>, <span class="number">0</span>)</span><br><span class="line">println(result1)</span><br><span class="line"><span class="comment">// 思路二</span></span><br><span class="line">result1 <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Some</span>(x) =&gt; println(x)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">None</span> =&gt; println(<span class="string">&quot;不能为0&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 思路三</span></span><br><span class="line">println(result1.getOrElse(<span class="string">&quot;不能为0&quot;</span>))</span><br></pre></td></tr></table></figure><h3 id="3-偏函数"><a href="#3-偏函数" class="headerlink" title="3.偏函数"></a>3.偏函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 偏函数</span></span><br><span class="line"><span class="keyword">val</span> pf: <span class="type">PartialFunction</span>[<span class="type">Int</span>, <span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;一&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">2</span> =&gt; <span class="string">&quot;二&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">3</span> =&gt; <span class="string">&quot;三&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">4</span> =&gt; <span class="string">&quot;四&quot;</span></span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="string">&quot;未匹配&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(pf(<span class="number">1</span>))</span><br><span class="line">println(pf(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="3-1结合map函数使用偏函数"><a href="#3-1结合map函数使用偏函数" class="headerlink" title="3.1结合map函数使用偏函数"></a>3.1结合map函数使用偏函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 结合map函数使用偏函数</span></span><br><span class="line"><span class="keyword">val</span> list11 = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="comment">//核心： 偏函数 结合 集合的函数式编程来使用</span></span><br><span class="line"><span class="keyword">val</span> list12 = list11.map &#123;</span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> x &gt;= <span class="number">1</span> &amp;&amp; x &lt;= <span class="number">3</span> =&gt; <span class="string">&quot;[1-3]&quot;</span></span><br><span class="line">  <span class="keyword">case</span> x <span class="keyword">if</span> x &gt;= <span class="number">4</span> &amp;&amp; x &lt;= <span class="number">8</span> =&gt; <span class="string">&quot;[4-8]&quot;</span></span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="string">&quot;(8-*]&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(list12)</span><br></pre></td></tr></table></figure><h3 id="3-2正则表达式"><a href="#3-2正则表达式" class="headerlink" title="3.2正则表达式"></a>3.2正则表达式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 正则表达式</span></span><br><span class="line"><span class="keyword">val</span> email = <span class="string">&quot;hkjcpdd123@163.com&quot;</span></span><br><span class="line"><span class="keyword">val</span> regex = <span class="string">&quot;&quot;&quot;.+@.+\..+&quot;&quot;&quot;</span>.r</span><br><span class="line"><span class="keyword">if</span> (regex.findAllMatchIn(email).nonEmpty) &#123; <span class="comment">// 那个nonEmpty就是i.size != 0</span></span><br><span class="line">  <span class="comment">// 能走到这里， 说明是合法邮箱</span></span><br><span class="line">  println(<span class="string">s&quot;<span class="subst">$&#123;email&#125;</span>是合法的邮箱&quot;</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 走到这里，说明是非法的邮箱</span></span><br><span class="line">  println(<span class="string">s&quot;<span class="subst">$&#123;email&#125;</span>是合法的邮箱&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正则2：过滤所有不合法的邮箱</span></span><br><span class="line"><span class="keyword">val</span> list14 = <span class="type">List</span>(<span class="string">&quot;123456789@qq.com&quot;</span>, <span class="string">&quot;a1da7897689@gmail.com&quot;</span>, <span class="string">&quot;123afaadd.com&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regex1 = <span class="string">&quot;&quot;&quot;.+@.+\..+&quot;&quot;&quot;</span>.r</span><br><span class="line"><span class="keyword">val</span> filterList = list14.filter(regex1.findAllMatchIn(_).nonEmpty)</span><br><span class="line">println(filterList)</span><br><span class="line"><span class="comment">// 正则3：获取邮箱运营商</span></span><br><span class="line"><span class="keyword">val</span> list15 = <span class="type">List</span>(<span class="string">&quot;hkjcpdd@qq.com&quot;</span>, <span class="string">&quot;hkjmjj@gmail.com&quot;</span>, <span class="string">&quot;zhangsan@163.com&quot;</span>, <span class="string">&quot;123foshana.com&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regex2 = <span class="string">&quot;&quot;&quot;.+@(.+)\..+&quot;&quot;&quot;</span>.r</span><br><span class="line"><span class="keyword">val</span> filterList1 = list15.map &#123;</span><br><span class="line">  <span class="comment">//固定格式 表示要校验的邮箱，固定格式  正则对象（对应的是正则中的分组内容）  固定格式  邮箱-&gt;运营商</span></span><br><span class="line">  <span class="comment">// 这里@是匹配的意思，就是后者正则匹配成功的就赋值给x</span></span><br><span class="line">  <span class="keyword">case</span> x<span class="meta">@regex</span>2(company) =&gt; x -&gt; company</span><br><span class="line">  <span class="keyword">case</span> x =&gt; x -&gt; <span class="string">&quot;未匹配&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(filterList1)</span><br></pre></td></tr></table></figure><h3 id="4-异常处理"><a href="#4-异常处理" class="headerlink" title="4.异常处理"></a>4.异常处理</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 异常处理</span></span><br><span class="line"><span class="comment">// 方式一：补货异常</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">// 可能出现问题的代码</span></span><br><span class="line">  <span class="comment">//      val i = 10 / 0</span></span><br><span class="line">  <span class="keyword">val</span> i = <span class="number">10</span> / <span class="number">1</span></span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="comment">// 出现问题后的解决方案</span></span><br><span class="line">  <span class="comment">//      case ex:ArithmeticException =&gt; println(&quot;算术异常&quot;)</span></span><br><span class="line">  <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt; ex.printStackTrace()</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="comment">// 这里一般用来释放资源的</span></span><br><span class="line">  println(<span class="string">&quot;我是用来释放资源的&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 方式二：抛出异常</span></span><br><span class="line"><span class="comment">//    throw new Exception(&quot;我是一个异常&quot;)</span></span><br></pre></td></tr></table></figure><h3 id="5-提取器"><a href="#5-提取器" class="headerlink" title="5.提取器"></a>5.提取器</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 提取器</span></span><br><span class="line"><span class="comment">// 方式1 普通写法</span></span><br><span class="line"><span class="keyword">val</span> s1 = <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">&quot;张三&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="comment">// 方式2  免new</span></span><br><span class="line"><span class="keyword">val</span> s2 = <span class="type">Student</span>(<span class="string">&quot;李四&quot;</span>, <span class="number">23</span>)</span><br><span class="line"><span class="comment">// 获取对象中的各个属性值，然后打印</span></span><br><span class="line"><span class="comment">// 方式1 普通获取</span></span><br><span class="line">println(s1.name, s1.age, s2.name, s2.age)</span><br><span class="line"><span class="comment">// 方式2 直接调用unapply方法</span></span><br><span class="line"><span class="type">Student</span>.unapply(s1)</span><br><span class="line"><span class="comment">// 方式3 通过模式匹配获取</span></span><br><span class="line">s1 <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Student</span>(name, age) =&gt; println(name, age)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;未匹配&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6-案例：随机职业"><a href="#6-案例：随机职业" class="headerlink" title="6.案例：随机职业"></a>6.案例：随机职业</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 随机职业</span></span><br><span class="line"><span class="comment">// 1.提示用户录入整数，匹配对应的职业</span></span><br><span class="line">println(<span class="string">&quot;请录入一个整数1~5， 我来告诉你上辈子的职业:&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> num = <span class="type">StdIn</span>.readInt()</span><br><span class="line"><span class="keyword">val</span> occupation = num <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;人机&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;人才&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;黄凯君&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;黄开俊&quot;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;黄凯军&quot;</span></span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="string">&quot;未匹配&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">println(occupation)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala - 函数式编程</title>
      <link href="/posts/a3f7.html"/>
      <url>/posts/a3f7.html</url>
      
        <content type="html"><![CDATA[<h2 id="scala函数式编程"><a href="#scala函数式编程" class="headerlink" title="scala函数式编程"></a>scala函数式编程</h2><h3 id="1-foreach方法"><a href="#1-foreach方法" class="headerlink" title="1.foreach方法"></a>1.foreach方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 遍历列表、数组</span></span><br><span class="line"><span class="keyword">val</span> list1 = (<span class="number">1</span> to <span class="number">5</span>).toList</span><br><span class="line"><span class="comment">// foreach方法</span></span><br><span class="line">list1.foreach(println(_))</span><br></pre></td></tr></table></figure><h3 id="2-映射map"><a href="#2-映射map" class="headerlink" title="2.映射map"></a>2.映射map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将结果整理成想要的</span></span><br><span class="line"><span class="keyword">val</span> list2 = list1.map(<span class="string">&quot;*&quot;</span> * _)</span><br><span class="line">println(<span class="string">&quot;映射&quot;</span> + list2)</span><br></pre></td></tr></table></figure><h3 id="3-扁平化映射flatMap"><a href="#3-扁平化映射flatMap" class="headerlink" title="3.扁平化映射flatMap"></a>3.扁平化映射flatMap</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将结果先进行映射然后进行扁平化</span></span><br><span class="line"><span class="keyword">val</span> list3 = <span class="type">List</span>((<span class="string">&quot;hkjcpdd hkjmjj hkjppp&quot;</span>), (<span class="string">&quot;hkjcpdd hkjmjj hkjppp&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> list4 = list3.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(<span class="string">&quot;扁平化映射&quot;</span> + list4)</span><br></pre></td></tr></table></figure><h3 id="4-排序"><a href="#4-排序" class="headerlink" title="4.排序"></a>4.排序</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统排序sorted如果需要反转在用完sorted之后reverse就好了</span></span><br><span class="line"><span class="keyword">val</span> list6 = <span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">0</span>)</span><br><span class="line">println(<span class="string">&quot;传统排序&quot;</span> + list6.sorted.rever</span><br><span class="line">        </span><br><span class="line"><span class="comment">// 指定字段排序</span></span><br><span class="line"><span class="keyword">val</span> list7 = <span class="type">List</span>(<span class="string">&quot;01 hadoop&quot;</span>, <span class="string">&quot;02 flume&quot;</span>, <span class="string">&quot;03 spark&quot;</span>)</span><br><span class="line">println(<span class="string">&quot;指定字段排序&quot;</span> + list7.sortBy(_.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>))</span><br><span class="line">                                             </span><br><span class="line"><span class="comment">// 自定义排序</span></span><br><span class="line">println(<span class="string">&quot;自定义排序&quot;</span> + list1.sortWith((x, y) =&gt; x &gt; y))</span><br></pre></td></tr></table></figure><h3 id="5-分组"><a href="#5-分组" class="headerlink" title="5.分组"></a>5.分组</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分组</span></span><br><span class="line"><span class="keyword">val</span> list8 = <span class="type">List</span>(<span class="string">&quot;刘德华&quot;</span> -&gt; <span class="string">&quot;男&quot;</span>, <span class="string">&quot;黄凯军&quot;</span> -&gt; <span class="string">&quot;男&quot;</span>, <span class="string">&quot;黄凯俊&quot;</span> -&gt; <span class="string">&quot;女&quot;</span> ,<span class="string">&quot;黄凯君&quot;</span> -&gt; <span class="string">&quot;男&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> map1 = list8.groupBy(_._2)</span><br><span class="line"><span class="keyword">val</span> map2 = map1.map(x =&gt; x._1 -&gt; x._2.size)</span><br><span class="line"><span class="type">Console</span>.println(map2)</span><br></pre></td></tr></table></figure><h3 id="6-聚合操作"><a href="#6-聚合操作" class="headerlink" title="6.聚合操作"></a>6.聚合操作</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 聚合操作 reduce用来对集合元素进行聚合计算 fold用来对集合进行折叠计算</span></span><br><span class="line"><span class="keyword">val</span> list9 = (<span class="number">1</span> to <span class="number">10</span>).toList</span><br><span class="line"><span class="comment">// 使用reduce计算所有元素的和</span></span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;reduce聚合= &quot;</span> + list9.reduce((x, y) =&gt; x - y))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;reduceLeft聚合 = &quot;</span> + list9.reduceLeft((x, y) =&gt; x - y))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;reduceRight聚合 = &quot;</span> + list9.reduceRight((x, y) =&gt; x -y))</span><br><span class="line"></span><br><span class="line"><span class="comment">// fold与reduce很像,只不过多了一个指定初始值参数 100表示初始化值, 后面表示函数对象</span></span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;fold = &quot;</span> + list9.fold(<span class="number">100</span>)((x, y) =&gt; x + y))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;foldLeft&quot;</span> + list9.foldLeft(<span class="number">100</span>)(_ + _))</span><br><span class="line"><span class="type">Console</span>.println(<span class="string">&quot;foldRight&quot;</span> + list9.foldRight(<span class="number">100</span>)(_ + _))</span><br></pre></td></tr></table></figure><h3 id="7-具体案例（学生成绩单）"><a href="#7-具体案例（学生成绩单）" class="headerlink" title="7.具体案例（学生成绩单）"></a>7.具体案例（学生成绩单）</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 学生成绩单</span></span><br><span class="line"><span class="keyword">val</span> listStu = <span class="type">List</span>((<span class="string">&quot;李四&quot;</span>, <span class="number">21</span>, <span class="number">89</span> ,<span class="number">43</span>), (<span class="string">&quot;王五&quot;</span>, <span class="number">56</span>, <span class="number">89</span>, <span class="number">98</span>), (<span class="string">&quot;赵柳&quot;</span>, <span class="number">66</span>, <span class="number">33</span>, <span class="number">55</span>), (<span class="string">&quot;黄凯君&quot;</span>, <span class="number">23</span>, <span class="number">54</span>, <span class="number">22</span>))</span><br><span class="line"><span class="comment">// 获取所有语文成绩在60及以上的</span></span><br><span class="line"><span class="keyword">val</span> filterList = listStu.filter(_._2 &gt;= <span class="number">60</span>)</span><br><span class="line">println(filterList.map(_._1))</span><br><span class="line"><span class="comment">// 获取所有学生的总成绩</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">SumList</span> = listStu.map(x =&gt; x._1 -&gt; (x._2 + x._3 + x._4))</span><br><span class="line">println(<span class="string">&quot;所有学生的总成绩: &quot;</span> + <span class="type">SumList</span>)</span><br><span class="line"><span class="comment">// 按照总成绩降序排列</span></span><br><span class="line">println(<span class="string">&quot;按照总成绩进行排序&quot;</span> + <span class="type">SumList</span>.sortWith((x, y) =&gt; x._2 &lt; x._2))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala</title>
      <link href="/posts/b9a7.html"/>
      <url>/posts/b9a7.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scala位运算符</title>
      <link href="/posts/d591.html"/>
      <url>/posts/d591.html</url>
      
        <content type="html"><![CDATA[<h3 id="位运算符"><a href="#位运算符" class="headerlink" title="位运算符"></a>位运算符</h3><p>  <img src="https://pic.imgdb.cn/item/67242470d29ded1a8cd11909.png"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; print(a &amp; b)# 相同为<span class="number">1</span>=上下想乘</span><br><span class="line">scala&gt; print(a | b) # 有<span class="number">1</span>则<span class="number">1</span></span><br><span class="line">scala&gt; print(a ^ b) # 不同为<span class="number">1</span></span><br><span class="line">scala&gt; print(~a) # 按位取反 因为拿到的是补码，<span class="number">-1</span>然后的到反码，反码符号位不变，其他按位取反</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(第二套)ZZ052大数据应用与服务赛项赛题</title>
      <link href="/posts/b2b7.html"/>
      <url>/posts/b2b7.html</url>
      
        <content type="html"><![CDATA[<h2 id="模块一：平台搭建与运维"><a href="#模块一：平台搭建与运维" class="headerlink" title="模块一：平台搭建与运维"></a>模块一：平台搭建与运维</h2><h3 id="（一）任务一：大数据平台搭建"><a href="#（一）任务一：大数据平台搭建" class="headerlink" title="（一）任务一：大数据平台搭建"></a><strong>（一）任务一：大数据平台搭建</strong></h3><h4 id="1．子任务一：基础环境准备"><a href="#1．子任务一：基础环境准备" class="headerlink" title="1．子任务一：基础环境准备"></a><strong>1．子任务一：基础环境准备</strong></h4><p>（1）对三台环境更新主机名，配置hosts文件，以node01作为时钟源并进行时间同步；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">其他三台一样</span></span><br><span class="line">[root@localhost ~]# hostnamectl set-hostname master</span><br><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure><p>（2）执行命令生成公钥、私钥，实现三台机器间的免秘登陆；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">其他三台一样</span></span><br><span class="line">[root@localhost ~]# ssh-keygen</span><br><span class="line">[root@localhost ~]# ssh-copy-id master</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave1</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave2</span><br></pre></td></tr></table></figure><p>（3）从宿主机&#x2F;root 目录下将文件 jdk-8u212-linux-x64.tar.gz 复制到容器 node01 中的&#x2F;root&#x2F;software 路径中（若路径不存在，则需新建），将 node01 节点 JDK 安装包解压到&#x2F;root&#x2F;software 路径中(若路径不存在，则需新建)；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# mkdir /root/software</span><br><span class="line">[root@localhost software]# tar -zxvf /opt/software/jdk-8u391-linux-x64.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（4）修改容器中&#x2F;etc&#x2F;profile 文件，设置 JDK 环境变量并使其生效，配置完毕后在 node01 节点分别执行“java - version”和“javac”命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新增</span></span><br><span class="line">export JAVA_HOME=/root/software/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">[root@localhost software]# source /etc/profile</span><br><span class="line">[root@localhost software]# java -version</span><br><span class="line">java version &quot;1.8.0_391&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br><span class="line">[root@localhost software]# javac</span><br><span class="line">用法: javac &lt;options&gt; &lt;source files&gt;</span><br><span class="line">其中, 可能的选项包括:</span><br><span class="line">  -g                         生成所有调试信息</span><br><span class="line">  -g:none                    不生成任何调试信息</span><br><span class="line">  -g:&#123;lines,vars,source&#125;     只生成某些调试信息</span><br><span class="line">  -nowarn                    不生成任何警告</span><br><span class="line">  -verbose                   输出有关编译器正在执行的操作的消息</span><br><span class="line">  -deprecation               输出使用已过时的 API 的源位置</span><br><span class="line">  -classpath &lt;路径&gt;            指定查找用户类文件和注释处理程序的位置</span><br><span class="line">  -cp &lt;路径&gt;                   指定查找用户类文件和注释处理程序的位置</span><br><span class="line">  -sourcepath &lt;路径&gt;           指定查找输入源文件的位置</span><br><span class="line">  -bootclasspath &lt;路径&gt;        覆盖引导类文件的位置</span><br><span class="line">  -extdirs &lt;目录&gt;              覆盖所安装扩展的位置</span><br><span class="line">  -endorseddirs &lt;目录&gt;         覆盖签名的标准路径的位置</span><br><span class="line">  -proc:&#123;none,only&#125;          控制是否执行注释处理和/或编译。</span><br><span class="line">  -processor &lt;class1&gt;[,&lt;class2&gt;,&lt;class3&gt;...] 要运行的注释处理程序的名称; 绕过默认的搜索进程</span><br><span class="line">  -processorpath &lt;路径&gt;        指定查找注释处理程序的位置</span><br><span class="line">  -parameters                生成元数据以用于方法参数的反射</span><br><span class="line">  -d &lt;目录&gt;                    指定放置生成的类文件的位置</span><br><span class="line">  -s &lt;目录&gt;                    指定放置生成的源文件的位置</span><br><span class="line">  -h &lt;目录&gt;                    指定放置生成的本机标头文件的位置</span><br><span class="line">  -implicit:&#123;none,class&#125;     指定是否为隐式引用文件生成类文件</span><br><span class="line">  -encoding &lt;编码&gt;             指定源文件使用的字符编码</span><br><span class="line">  -source &lt;发行版&gt;              提供与指定发行版的源兼容性</span><br><span class="line">  -target &lt;发行版&gt;              生成特定 VM 版本的类文件</span><br><span class="line">  -profile &lt;配置文件&gt;            请确保使用的 API 在指定的配置文件中可用</span><br><span class="line">  -version                   版本信息</span><br><span class="line">  -help                      输出标准选项的提要</span><br><span class="line">  -A关键字[=值]                  传递给注释处理程序的选项</span><br><span class="line">  -X                         输出非标准选项的提要</span><br><span class="line">  -J&lt;标记&gt;                     直接将 &lt;标记&gt; 传递给运行时系统</span><br><span class="line">  -Werror                    出现警告时终止编译</span><br><span class="line">  @&lt;文件名&gt;                     从文件读取选项和文件名</span><br></pre></td></tr></table></figure><h4 id="2．子任务二：Hadoop-完全分布式安装配置"><a href="#2．子任务二：Hadoop-完全分布式安装配置" class="headerlink" title="2．子任务二：Hadoop 完全分布式安装配置"></a><strong>2．子任务二：Hadoop 完全分布式安装配置</strong></h4><p>本任务需要使用 root 用户完成相关配置，安装 Hadoop需要配置前置环境。命令中要求使用绝对路径，具体要求如下:</p><p>（1）在 node01 将 Hadoop 解压到&#x2F;root&#x2F;software(若路径不存在，则需新建)目录下，并将解压包分发至 node02、node03 中，其中三节点节点均作为 datanode，配置好相关环境，初始化 Hadoop 环境 namenode；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# tar -zxvf /opt/software/hadoop-3.3.6.tar.gz -C /root/software/</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">分发</span></span><br><span class="line">[root@localhost software]# scp -r jdk/ hadoop/ slave1:`pwd`</span><br><span class="line">[root@localhost software]# scp -r jdk/ hadoop/ slave2:`pwd`</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化</span></span><br><span class="line">[root@localhost software]# hadoop namenode -format</span><br></pre></td></tr></table></figure><p>（2）开启集群，查看各节点进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动</span></span><br><span class="line">[root@localhost software]# start-all.sh</span><br><span class="line">[root@localhost software]# jps</span><br><span class="line">47810 ResourceManager</span><br><span class="line">48597 WebAppProxyServer</span><br><span class="line">49205 JobHistoryServer</span><br><span class="line">47450 SecondaryNameNode</span><br><span class="line">49371 Jps</span><br><span class="line">48060 NodeManager</span><br><span class="line">46766 NameNode</span><br><span class="line">47134 DataNode</span><br></pre></td></tr></table></figure><h4 id="3．子任务三：Hive-安装配置"><a href="#3．子任务三：Hive-安装配置" class="headerlink" title="3．子任务三：Hive 安装配置"></a><strong>3．子任务三：Hive 安装配置</strong></h4><p>（1）从宿主机&#x2F;root 目录下将文件 apache-hive-3.1.2-bin.tar.gz、mysql-connector-java-5.1.37.jar 复制到容器 node03 中的&#x2F;root&#x2F;software 路径中（若路径不存在，则需新建），将 node03 节点 Hive 安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost conf]# tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2）设置 Hive 环境变量，并使环境变量生效，执行命令 hive –version 查看版本信息；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost conf]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/root/software/jdk</span><br><span class="line">export HADOOP_HOME=/root/software/hadoop</span><br><span class="line">export HIVE_HOME=/root/software/hive</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@master conf]# hive --version</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive 3.1.3</span><br><span class="line">Git git://MacBook-Pro.fios-router.home/Users/ngangam/commit/hive -r 4df4d75bf1e16fe0af75aad0b4179c34c07fc975</span><br><span class="line">Compiled by ngangam on Sun Apr 3 16:58:16 EDT 2022</span><br><span class="line">From source with checksum 5da234766db5dfbe3e92926c9bbab2af</span><br></pre></td></tr></table></figure><p>（3）修改相关配置，添加依赖包，将 MySQL 数据库作为Hive 元数据库，初始化 Hive 元数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置好hive-env.sh和hive-site.xml</span></span><br><span class="line">[root@master conf]# cp /opt/software/mysql-connector-java-5.1.34.jar /root/software/hive/lib/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化</span></span><br><span class="line">[root@master conf]# schematool -initSchema -dbType mysql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动</span></span><br><span class="line">[root@master hive]# mkdir logs</span><br><span class="line">[root@master hive]# nohup hive --service metastore &gt;&gt; logs/metastore.log 2&gt;&amp;1 &amp;</span><br><span class="line">[1] 128547</span><br><span class="line"></span><br><span class="line">[root@master hive]# hive</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">which: no hbase in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/software/jdk/bin:/root/software/hadoop/bin:/root/software/hadoop/sbin:/root/software/hive/bin:/root/bin)</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive Session ID = 7e262d28-438c-4922-af9c-1323adfa7f88</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/root/software/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true</span><br><span class="line">Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 195fb209-4ef9-4452-bbdd-90b81a8d04bc</span><br><span class="line"><span class="meta prompt_">hive&gt;</span></span><br></pre></td></tr></table></figure><p><strong>4．子任务四：Flume 安装配置</strong> </p><p>（1）从宿主机&#x2F;root 目录下将文件 apache-flume-1.11.0-bin.tar.gz复制到容器node03中的&#x2F;root&#x2F;software路径中（若路径不存在，则需新建），将 node03 节点 Flume安装包解压到&#x2F;root&#x2F;software 目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -zxvf /opt/software/apache-flume-1.11.0-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2）完善相关配置，配置 Flume 环境变量，并使环境变量生效，执行命令 flume-ng version。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新增</span></span><br><span class="line">export FLUME_HOME_HOME=/root/software/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[root@master software]# source /etc/profile</span><br><span class="line"></span><br><span class="line">[root@master software]# flume-ng version</span><br><span class="line">Flume 1.11.0</span><br><span class="line">Source code repository: https://git.apache.org/repos/asf/flume.git</span><br><span class="line">Revision: 1a15927e594fd0d05a59d804b90a9c31ec93f5e1</span><br><span class="line">Compiled by rgoers on Sun Oct 16 14:44:15 MST 2022</span><br><span class="line">From source with checksum bbbca682177262aac3a89defde369a37</span><br></pre></td></tr></table></figure><h3 id="（二）任务二：数据库配置维护"><a href="#（二）任务二：数据库配置维护" class="headerlink" title="（二）任务二：数据库配置维护"></a><strong>（二）任务二：数据库配置维护</strong></h3><h4 id="1．子任务一：数据库配置"><a href="#1．子任务一：数据库配置" class="headerlink" title="1．子任务一：数据库配置"></a><strong>1．子任务一：数据库配置</strong></h4><p>（1）在主机 node3 上安装 mysql-community-server，启动 mySQL 服务，根据临时密码进入数据库，并修改本地密码为“123456”；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> software]# rpm <span class="operator">-</span>ivh mysql<span class="operator">-</span>community<span class="operator">-</span>server<span class="number">-5.7</span><span class="number">.44</span><span class="number">-1.</span>el7.x86_64.rpm </span><br><span class="line">警告：mysql<span class="operator">-</span>community<span class="operator">-</span>server<span class="number">-5.7</span><span class="number">.44</span><span class="number">-1.</span>el7.x86_64.rpm: 头V4 RSA<span class="operator">/</span>SHA256 Signature, 密钥 ID <span class="number">3</span>a79bd29: NOKEY</span><br><span class="line">准备中...                          ################################# [<span class="number">100</span><span class="operator">%</span>]</span><br><span class="line">正在升级<span class="operator">/</span>安装...</span><br><span class="line">   <span class="number">1</span>:mysql<span class="operator">-</span>community<span class="operator">-</span>server<span class="number">-5.7</span><span class="number">.44</span><span class="number">-1.</span>e################################# [<span class="number">100</span><span class="operator">%</span>]</span><br><span class="line">[root<span class="variable">@master</span> software]# systemctl <span class="keyword">start</span> mysqld</span><br><span class="line">[root<span class="variable">@master</span> software]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">2</span></span><br><span class="line">Server version: <span class="number">5.7</span><span class="number">.44</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2023</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">user</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><p>（2）开启 MySQL 远程连接权限，所有 root 用户都可以使用 123456 进行登录连接。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">bind<span class="operator">-</span>address<span class="operator">=</span><span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;mysql.sys&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p><strong>2．子任务二：导入相关表</strong> </p><p>（1）将本地&#x2F;root&#x2F;eduhq&#x2F;equipment&#x2F;目录下的数据文件 root_sl_src.sql 导入 MySQL 对应数据库 root_sl_src；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">use root_sl_src</span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash"><span class="built_in">source</span> root_sl_src.sql</span></span><br></pre></td></tr></table></figure><p>（2）将本地&#x2F;root&#x2F;eduhq&#x2F;equipment&#x2F;目录下的数据文件 root_sl_ugoogds_src.sql 导 入 MySQL 对应数据库root_sl_ugoogds_src。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> use root_sl_ugoogds_src</span><br><span class="line">mysql<span class="operator">&gt;</span> source root_sl_ugoogds_src.sql</span><br></pre></td></tr></table></figure><p><strong>3．子任务三：维护数据表</strong> </p><p>结合已导入的两份 sql 数据，对其中的数据进行如下查询和操作。</p><p>（1）对‘root_sl_src’数据库中的‘province’数据表进行修改，修改字段 province_id 为 24 的记录的province_name，修改为‘内蒙古自治区’；</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> province <span class="keyword">set</span> province_name<span class="operator">=</span><span class="string">&#x27;内蒙古自治区&#x27;</span> <span class="keyword">where</span> province_id<span class="operator">=</span><span class="number">24</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.01</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br></pre></td></tr></table></figure><p>（2）对‘root_sl_src’数据库中的‘city’数据表进行删除，删除字段 city_id 为 142 的记录。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">delete</span> <span class="keyword">from</span> city <span class="keyword">where</span> city_id<span class="operator">=</span><span class="number">142</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.03</span> sec)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 国赛赛题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(第一套)ZZ052大数据应用与服务赛项赛题</title>
      <link href="/posts/b2b6.html"/>
      <url>/posts/b2b6.html</url>
      
        <content type="html"><![CDATA[<h2 id="模块一：平台搭建与运维"><a href="#模块一：平台搭建与运维" class="headerlink" title="模块一：平台搭建与运维"></a>模块一：平台搭建与运维</h2><p><strong>（一）任务一：大数据平台搭建</strong> </p><p>本模块需要使用 root 用户完成相关配置；所有组件均</p><p>在&#x2F;root&#x2F;software 目录下。</p><h2 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">三台机都要</span></span><br><span class="line">[root@localhost ~]# systemctl stop firewalld</span><br><span class="line">[root@localhost ~]# systemctl disable firewalld</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# vi /etc/hosts</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加：</span></span><br><span class="line">192.168.1.122 master</span><br><span class="line">192.168.1.123 slave1</span><br><span class="line">192.168.1.124 slave2</span><br><span class="line"></span><br><span class="line">[root@localhost ~]# ssh-keygen# 一直回车即可</span><br><span class="line">[root@localhost ~]# ssh-copy-id master# yes 然后密码</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave1</span><br><span class="line">[root@localhost ~]# ssh-copy-id slave2</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">改主机名</span></span><br><span class="line">[root@localhost software]# hostnamectl set-hostname master# 以此类推第二台slave1第三台slave2</span><br></pre></td></tr></table></figure><p><strong>1．子任务一：基础环境准备</strong> </p><p>master、slave1、slave2三台节点都需要安装JDK</p><p>（1） 将JDK安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# tar -zxvf /opt/software/jdk-8u391-linux-x64.tar.gz -C /root/software/</span><br><span class="line">[root@localhost software]# cd /root/software/</span><br><span class="line">[root@localhost software]# mv jdk1.8.0_391/ jdk</span><br></pre></td></tr></table></figure><p>（2） 在“&#x2F;etc&#x2F;profile”文件中配置JDK环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加</span></span><br><span class="line">export JAVA_HOME=/root/software/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">[root@localhost software]# source /etc/profile</span><br></pre></td></tr></table></figure><p>JAVA_HOME和PATH的值，并让配置文件立即生效；</p><p>（3） 查看JDK版本，检测JDK是否安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# java -version</span><br><span class="line">java version &quot;1.8.0_391&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_391-b13)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.391-b13, mixed mode)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记得分发jdk给slave1和slave2</span></span><br><span class="line">[root@master software]# scp -r jdk/ slave1:`pwd`</span><br><span class="line">[root@master software]# scp -r jdk/ slave2:`pwd`</span><br></pre></td></tr></table></figure><p>在master节点操作</p><p>（1） 在master上生成SSH密钥对；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">前置工作已做</span><br></pre></td></tr></table></figure><p>（2） 将master上的公钥拷贝到slave1和slave2上；在 master 上通过 SSH 连接 slave1 和 slave2 来验证。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost software]# ssh slave1</span><br><span class="line">Last login: Mon Oct 28 11:43:50 2024 from 192.168.1.166</span><br><span class="line">[root@slave1 ~]# exit;</span><br><span class="line">登出</span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@localhost software]# ssh slave1</span><br><span class="line">Last login: Mon Oct 28 11:58:39 2024 from master</span><br><span class="line">[root@slave1 ~]# exit</span><br><span class="line">登出</span><br><span class="line">Connection to slave1 closed.</span><br><span class="line">[root@localhost software]# ssh slave2</span><br><span class="line">Last login: Mon Oct 28 11:43:52 2024 from 192.168.1.166</span><br><span class="line">[root@slave2 ~]# exit</span><br><span class="line">登出</span><br><span class="line">Connection to slave2 closed.</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：Hadoop 完全分布式安装配置</strong> </p><p>master、slave1、slave2三台节点都需要安装Hadoop</p><p>（1） 在 主 节 点 将 Hadoop 安 装 包 解 压 到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf /opt/software/hadoop-3.3.6.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2） 依次配置hadoop-env.sh、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml和workers配置</p><p>文件；Hadoop集群部署规划如下表；</p><p>表1    Hadoop集群部署规划</p><p><img src="https://pic.imgdb.cn/item/6721a0cdd29ded1a8cdcb200.png"></p><p>（3） 在master节点的Hadoop安装目录下依次创建hadoopDatas&#x2F;tempDatas 、 hadoopDatas&#x2F;namenodeDatas 、hadoopDatas&#x2F;datanodeDatas、hadoopDatas&#x2F;dfs&#x2F;nn&#x2F;edits、hadoopDatas&#x2F;dfs&#x2F;snn&#x2F;name 和hadoopDatas&#x2F;dfs&#x2F;nn&#x2F;snn&#x2F;edits目录；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/tempDatas</span><br><span class="line">[root@localhost hadoop]# mkdir hadoopDatas/namenodeDatas </span><br><span class="line">[root@localhost hadoop]# mkdir hadoopDatas/datanodeDatas</span><br><span class="line">[root@localhost hadoop]# mkdir hadoopDatas/dfs/nn/edits</span><br><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/dfs/nn/edits</span><br><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/dfs/snn/name</span><br><span class="line">[root@localhost hadoop]# mkdir -p hadoopDatas/dfs/nn/snn/edits</span><br></pre></td></tr></table></figure><p>（4） 在master节点上使用scp命令将配置完的Hadoop安装目录直接拷贝至slave1和slave2；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /root/software/hadoop/ slave1:`pwd`</span><br><span class="line">scp -r /root/software/hadoop/ slave2:`pwd`</span><br></pre></td></tr></table></figure><p>（5） 三台节点的“&#x2F;etc&#x2F;profile”文件中配置Hadoop环境变量HADOOP_HOME和PATH的值，并让配置文件立即生效；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# scp /etc/profile slave1:/etc</span><br><span class="line">profile                                                        100% 1985     3.4MB/s   00:00    </span><br><span class="line">[root@master software]# scp /etc/profile slave2:/etc</span><br><span class="line">profile                                                        100% 1985     3.4MB/s   00:00</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">slave1</span></span><br><span class="line">[root@slave1 ~]# source /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">slave2</span></span><br><span class="line">[root@slave2 ~]# source /etc/profile</span><br></pre></td></tr></table></figure><p>（6） 在主节点格式化集群；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# hadoop namenode -format</span><br></pre></td></tr></table></figure><p>（7） 在主节点依次启动HDFS、YARN集群和历史服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@master /]# start-all.sh</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">上一次登录：一 10月 28 16:05:13 CST 2024pts/0 上</span><br><span class="line">Starting datanodes</span><br><span class="line">上一次登录：一 10月 28 16:05:25 CST 2024pts/0 上</span><br><span class="line">Starting secondary namenodes [master]</span><br><span class="line">上一次登录：一 10月 28 16:05:27 CST 2024pts/0 上</span><br><span class="line">Starting resourcemanager</span><br><span class="line">上一次登录：一 10月 28 16:05:33 CST 2024pts/0 上</span><br><span class="line">Starting nodemanagers</span><br><span class="line">上一次登录：一 10月 28 16:05:40 CST 2024pts/0 上</span><br><span class="line">上一次登录：一 10月 28 16:05:42 CST 2024pts/0 上</span><br><span class="line">[root@master /]# mapred --daemon start historyserver</span><br><span class="line">[root@master /]# jps</span><br><span class="line">70417 WebAppProxyServer</span><br><span class="line">68435 NameNode</span><br><span class="line">71267 JobHistoryServer</span><br><span class="line">69218 SecondaryNameNode</span><br><span class="line">71385 Jps</span><br><span class="line">69560 ResourceManager</span><br><span class="line">69816 NodeManager</span><br><span class="line">68718 DataNode</span><br></pre></td></tr></table></figure><p><strong>3．子任务三：MySQL 安装配置</strong> </p><p>只在master节点操作</p><p>（1） 将MySQL 5.7.25安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -xvf /opt/software/mysql-5.7.44-1.el7.x86_64.rpm-bundle.tar -C /root/software/</span><br><span class="line">mysql-community-client-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-common-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-devel-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-devel-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-server-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-test-5.7.44-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>（2） 使 用 rpm -ivh 依 次 安 装 mysql-community-common、mysql-community- libs、mysql-community-libscompat 、 mysql-community-client 和 mysql-communityserver包；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# rpm -ivh mysql-community-common-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-libs-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-libs-compat-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-client-5.7.44-1.el7.x86_64.rpm</span><br><span class="line">[root@master software]# rpm -ivh mysql-community-server-5.7.44-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>（3） 安装好MySQL后，使用mysql用户初始化和启动数据库；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# mysqld --initialize-insecure --user=mysql --datadir=/var/lib/mysql</span><br><span class="line">[root@master software]# systemctl start mysqld</span><br></pre></td></tr></table></figure><p>（4） 使用root用户无密码登录MySQL，然后将root用户的密码修改为123456，修改完成退出MySQL，重新登录验证密码是否修改成功；更改“mysql”数据库里的 user 表里的 host 项，从localhost 改成%即可实现用户远程登录；设置完成刷新配置信息，让其生效。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[root<span class="variable">@master</span> software]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">3</span></span><br><span class="line">Server version: <span class="number">5.7</span><span class="number">.44</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2023</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">user</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> exit;</span><br><span class="line">Bye</span><br><span class="line">[root<span class="variable">@master</span> software]# mysql <span class="operator">-</span>uroot <span class="operator">-</span>p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">4</span></span><br><span class="line">Server version: <span class="number">5.7</span><span class="number">.44</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2023</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">user</span>,host <span class="keyword">from</span> mysql.user;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">user</span>          <span class="operator">|</span> host      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> mysql.session <span class="operator">|</span> localhost <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql.sys     <span class="operator">|</span> localhost <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> root          <span class="operator">|</span> localhost <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;mysql.sys&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">update</span> mysql.user <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;mysql.session&#x27;</span>;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"><span class="keyword">Rows</span> matched: <span class="number">1</span>  Changed: <span class="number">1</span>  Warnings: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> flush privileges;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p><strong>4．子任务四：Hive 安装配置</strong> </p><p>只在master节点操作。</p><p>（1） 将Hive 3.1.2的安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2） 在“&#x2F;etc&#x2F;profile”文件中配置Hive环境变量HIVE_HOME和PATH的值，并让配置文件立即生效；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# cd /root/software/</span><br><span class="line">[root@master software]# mv apache-hive-3.1.3-bin/ hive</span><br><span class="line">[root@master software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加</span></span><br><span class="line">export HIVE_HOME=/root/software/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">[root@master software]# source /etc/profile</span><br></pre></td></tr></table></figure><p>（3） 查看Hive版本，检测Hive环境变量是否设置成功；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# hive --version</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding in [jar:file:/root/software/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.</span><br><span class="line">SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]</span><br><span class="line">Hive 3.1.3</span><br><span class="line">Git git://MacBook-Pro.fios-router.home/Users/ngangam/commit/hive -r 4df4d75bf1e16fe0af75aad0b4179c34c07fc975</span><br><span class="line">Compiled by ngangam on Sun Apr 3 16:58:16 EDT 2022</span><br><span class="line">From source with checksum 5da234766db5dfbe3e92926c9bbab2af</span><br></pre></td></tr></table></figure><p>（4） 切换到 $HIVE_HOME&#x2F;conf 目录下，将 hiveenv.sh.template文件复制一份并重命名为hive-env.sh；然后，使用vim编辑器进行编辑，在文件中配置HADOOP_HOME、HIVE_CONF_DIR以及HIVE_AUX_JARS_PATH参数的值，将原有值删除并将前面的注释符#去掉；配置完成，保存退出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# cp hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置这些值</span></span><br><span class="line">HADOOP_HOME=/root/software/hadoop</span><br><span class="line">export HIVE_CONF_DIR=/root/software/hive</span><br><span class="line">export HIVE_AUX_JARS_PATH=/root/software/hive/lib</span><br></pre></td></tr></table></figure><p>（5） 将 &#x2F;root&#x2F;software 目 录 下 的 MySQL 驱动包mysql-connector-java-5.1.47-bin.jar 拷 贝 到$HIVE_HOME&#x2F;lib目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# cp /opt/software/mysql-connector-java-5.1.34.jar /root/software/hive/lib/</span><br></pre></td></tr></table></figure><p>（6） 在$HIVE_HOME&#x2F;conf目录下创建一个名为hive-site.xml的文件，并使用vim编辑器进行编辑；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# vi hive-site.xml</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后进行配置</span></span><br></pre></td></tr></table></figure><p>配置如下内容：</p><p><img src="https://pic.imgdb.cn/item/6721a102d29ded1a8cdd10d5.png"></p><p>（7） 使用schematool命令，通过指定元数据库类型为“mysql”，来初始化源数据库的元数据；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure><p>（8） 使用CLI启动Hive，进入Hive客户端；在Hive默认数据库下创建一个名为student的管理表；</p><p><img src="https://pic.imgdb.cn/item/6721a127d29ded1a8cdd5fdd.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# hive</span><br><span class="line">hive&gt; create table student(</span><br><span class="line">    &gt; id int,</span><br><span class="line">    &gt; name string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.73 seconds</span><br></pre></td></tr></table></figure><p>（9） 通过insert语句往student表中插入一条测试数据。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into table student values(1, &#x27;hkj&#x27;);</span><br></pre></td></tr></table></figure><p><strong>5．子任务五：Flume 安装配置</strong> </p><p>只在 master 节点操作。</p><p>（1） 将 Flume 1.11.0 的安装包解压到&#x2F;root&#x2F;software目录下；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# tar -zxvf /opt/software/apache-flume-1.11.0-bin.tar.gz -C /root/software/</span><br></pre></td></tr></table></figure><p>（2） 在“&#x2F;etc&#x2F;profile”文件中配置Flume环境变量FLUME_HOME和PATH的值，并让配置文件立即生效；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master software]# cd /root/software/</span><br><span class="line">[root@master software]# mv apache-flume-1.11.0-bin/ flume</span><br><span class="line">[root@master software]# vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">追加</span></span><br><span class="line">export FLUME_HOME=/root/software/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line">[root@master software]# source /etc/profile</span><br></pre></td></tr></table></figure><p>（3） 使 用 cd 命 令 进 入 &#x2F;root&#x2F;software&#x2F;apache&#x2F;flume-1.11.0-bin&#x2F;conf 目 录 下 ， 使 用 cp 命令将 flumeenv.sh.template文件复制一份，并重命名为flume-env.sh；使 用 vim 命 令 打 开 “flume-env.sh” 配 置 文 件 ， 找 到JAVA_HOME参数位置，将前面的“#”去掉，将值修改为本机JDK的实际位置；修改完成，保存退出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# cd /root/software/flume/conf/</span><br><span class="line">[root@master conf]# cp flume-env.sh.template flume-env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将java_home配置项的<span class="comment">#去掉然后更换地址</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例：<span class="built_in">export</span> JAVA_HOME=/root/software/jdk</span></span><br></pre></td></tr></table></figure><p>（4） 查看Flume版本，检测Flume是否安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# flume-ng version</span><br><span class="line">Flume 1.11.0</span><br><span class="line">Source code repository: https://git.apache.org/repos/asf/flume.git</span><br><span class="line">Revision: 1a15927e594fd0d05a59d804b90a9c31ec93f5e1</span><br><span class="line">Compiled by rgoers on Sun Oct 16 14:44:15 MST 2022</span><br><span class="line">From source with checksum bbbca682177262aac3a89defde369a37</span><br></pre></td></tr></table></figure><p><strong>（二）任务二：数据库配置维护</strong> </p><p><strong>1．子任务一：数据库配置</strong> </p><p>在 Hive 中创建一个名为 comm 的数据库，如果数据库已经存在，则不进行创建。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master conf]# hive</span><br><span class="line">hive&gt; create database if not exist comm;</span><br><span class="line">FAILED: ParseException line 1:23 missing KW_EXISTS at &#x27;exist&#x27; near &#x27;&lt;EOF&gt;&#x27;</span><br><span class="line">line 1:29 extraneous input &#x27;comm&#x27; expecting EOF near &#x27;&lt;EOF&gt;&#x27;</span><br><span class="line">hive&gt; create database if not exists comm;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.297 seconds</span><br></pre></td></tr></table></figure><p><strong>2．子任务二：创建相关表</strong> </p><p>（1） 在 comm 数 据 库 下 创 建 一 个 名 为ods_behavior_log的外部表，如果表已存在，则先删除；分区字段为dt，即根据日期进行分区；同时，使用location关键 字 将 表 的 存 储 路 径 设 置 为 HDFS 的&#x2F;behavior&#x2F;ods&#x2F;ods_behavior_log目录；字段类型如下表所示；</p><p><img src="https://pic.imgdb.cn/item/6721a147d29ded1a8cddbd2e.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use comm;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.042 seconds</span><br><span class="line">hive&gt; CREATE TABLE IF NOT EXISTS ods_behavior_log (</span><br><span class="line">    &gt;   line STRING</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; PARTITIONED BY (dt STRING)</span><br><span class="line">    &gt; ROW FORMAT DELIMITED</span><br><span class="line">    &gt; FIELDS TERMINATED BY &#x27;\t&#x27;</span><br><span class="line">    &gt; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.244 seconds</span><br></pre></td></tr></table></figure><p>（2） 使 用 load data 子 句 将 本 地&#x2F;root&#x2F;eduhq&#x2F;data&#x2F;app_log&#x2F;behavior目录下的每个数据文件依次加载到外部表ods_behavior_log的对应分区中，按照日志文件对应日期定义静态分区（例如：dt&#x3D;’2023-01-01’）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-01.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-01&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.831 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-02.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-02&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.628 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-03.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-03&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.491 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-04.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-04&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.506 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-05.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-05&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.465 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-06.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-06&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.44 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/excel_log/behavior2023-01-07.xlsx&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-07&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.423 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-01.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-01&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.432 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-02.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-02&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.397 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-03.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-03&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.425 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-04.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-04&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.43 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-05.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-05&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.395 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-06.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-06&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.422 seconds</span><br><span class="line">hive&gt; LOAD DATA LOCAL INPATH &#x27;/root/eduhq/data/app_log/behavior/txt_log/behavior2023-01-07.log&#x27; INTO TABLE ods_behavior_log PARTITION (dt=&#x27;2023-01-07&#x27;);</span><br><span class="line">Loading data to table comm.ods_behavior_log partition (dt=2023-01-01)</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.773 seconds</span><br></pre></td></tr></table></figure><p>（3） 查看ods_behavior_log表的所有现有分区、前3行数据，并统计外部表ods_behavior_log数据总行数；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># 查看所有现有分区</span><br><span class="line">hive&gt; SHOW PARTITIONS ods_behavior_log;</span><br><span class="line">OK</span><br><span class="line">dt=2023-01-01</span><br><span class="line">dt=2023-01-02</span><br><span class="line">dt=2023-01-03</span><br><span class="line">dt=2023-01-04</span><br><span class="line">dt=2023-01-05</span><br><span class="line">dt=2023-01-06</span><br><span class="line">dt=2023-01-07</span><br><span class="line">Time taken: 0.089 seconds, Fetched: 7 row(s)</span><br><span class="line"></span><br><span class="line"># 前三行数据</span><br><span class="line">hive&gt; SELECT * FROM ods_behavior_log LIMIT 3;</span><br><span class="line">OK</span><br><span class="line">&#123;&quot;client_ip&quot;: &quot;222.86.237.202&quot;, &quot;device_type&quot;: &quot;pc&quot;, &quot;time&quot;: 1672580908000, &quot;type&quot;: &quot;WIFI&quot;, &quot;device&quot;: &quot;357c5dd65e834b8bafac861bdae5d76d&quot;, &quot;url&quot;: &quot;http://wan.baidu.com/home?idfrom=4087&quot;&#125;        2023-01-01</span><br><span class="line">&#123;&quot;client_ip&quot;: &quot;210.42.113.5&quot;, &quot;device_type&quot;: &quot;pc&quot;, &quot;time&quot;: 1672539082000, &quot;type&quot;: &quot;WIFI&quot;, &quot;device&quot;: &quot;6cab446d0c664de4b09c76bae90fc1bf&quot;, &quot;url&quot;: &quot;http://go.hao123.com/sites&quot;&#125;     2023-01-01</span><br><span class="line">&#123;&quot;client_ip&quot;: &quot;222.34.42.67&quot;, &quot;device_type&quot;: &quot;mobile&quot;, &quot;time&quot;: 1672524854000, &quot;type&quot;: &quot;5G&quot;, &quot;device&quot;: &quot;36b4213510e74c7e8455b61fb481b576&quot;, &quot;url&quot;: &quot;https://4366yy.381pk.com/1251/&quot;&#125;       2023-01-01</span><br><span class="line">Time taken: 1.805 seconds, Fetched: 3 row(s)</span><br><span class="line"></span><br><span class="line"># 统计数据总行数</span><br><span class="line">hive&gt; SELECT COUNT(*) FROM ods_behavior_log;</span><br><span class="line">Query ID = root_20241028190258_2de0fddd-343e-4256-b248-f252f3400a9b</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Starting Job = job_1730102747884_0003, Tracking URL = http://master:8089/proxy/application_1730102747884_0003/</span><br><span class="line">Kill Command = /root/software/hadoop/bin/mapred job  -kill job_1730102747884_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2024-10-28 19:03:31,071 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-10-28 19:03:42,782 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:04:43,115 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:05:43,251 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:06:44,246 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.55 sec</span><br><span class="line">2024-10-28 19:06:53,529 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.32 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 4 seconds 320 msec</span><br><span class="line">Ended Job = job_1730102747884_0003</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.32 sec   HDFS Read: 37824491 HDFS Write: 106 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 4 seconds 320 msec</span><br><span class="line">OK</span><br><span class="line">198413</span><br><span class="line">Time taken: 235.981 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p>（4） 在 comm 数 据 库 下 创 建 一 个 名 为dwd_behavior_log的外部表，如果表已存在，则先删除；分区字段为dt，即根据日期进行分区；另外，要求指定表的存储路径为HDFS的&#x2F;behavior&#x2F;dwd&#x2F;dwd_behavior_log目录，存储文件类型为“orc”，文件的压缩类型为“snappy”；字段类型如下表所示；</p><p><img src="https://pic.imgdb.cn/item/6721a16ad29ded1a8cde1313.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS comm.dwd_behavior_log (</span><br><span class="line">    &gt;   client_ip STRING,</span><br><span class="line">    &gt;   device_type STRING,</span><br><span class="line">    &gt;   type STRING,</span><br><span class="line">    &gt;   device STRING,</span><br><span class="line">    &gt;   url STRING,</span><br><span class="line">    &gt;   province STRING,</span><br><span class="line">    &gt;   city STRING,</span><br><span class="line">    &gt;   ts BIGINT</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; PARTITIONED BY (dt STRING)</span><br><span class="line">    &gt; STORED AS ORC</span><br><span class="line">    &gt; LOCATION &#x27;/behavior/dwd/dwd_behavior_log&#x27;</span><br><span class="line">    &gt; TBLPROPERTIES (&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.091 seconds</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 国赛赛题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker-dockerfile</title>
      <link href="/posts/d3ce.html"/>
      <url>/posts/d3ce.html</url>
      
        <content type="html"><![CDATA[<p>404</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker-compose</title>
      <link href="/posts/713b.html"/>
      <url>/posts/713b.html</url>
      
        <content type="html"><![CDATA[<ul><li>首先需要compose,yaml文件</li><li>上线：docker compose up -d</li><li>下线：docker compse down </li><li>启动：docker compose start x1 x2 x3</li><li>停止：docker compose stop x1 x3</li><li>扩容：docker compose scale x2&#x3D;3</li></ul><h4 id="compose-yaml例子："><a href="#compose-yaml例子：" class="headerlink" title="compose.yaml例子："></a>compose.yaml例子：</h4><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">name: myblog</span><br><span class="line">services:</span><br><span class="line">  mysql:</span><br><span class="line">    container_name: mysql</span><br><span class="line">    image: mysql:<span class="number">8.0</span></span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">&quot;3306:3306&quot;</span></span><br><span class="line">    environment:</span><br><span class="line">      MYSQL_ROOT_PASSWORD: <span class="string">&quot;123456&quot;</span></span><br><span class="line">      MYSQL_DATABASE: wordpress</span><br><span class="line">    volumes:</span><br><span class="line">      - mysql-data:/var/lib/mysql</span><br><span class="line">      - ./myconf:/etc/mysql/conf.d </span><br><span class="line">    restart: always</span><br><span class="line">    networks:</span><br><span class="line">      - blog</span><br><span class="line"></span><br><span class="line">  wordpress:</span><br><span class="line">    image: wordpress</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">&quot;8080:80&quot;</span></span><br><span class="line">    environment: </span><br><span class="line">      WORDPRESS_DB_HOST: mysql</span><br><span class="line">      WORDPRESS_DB_USER: root</span><br><span class="line">      WORDPRESS_DB_PASSWORD: <span class="string">&quot;123456&quot;</span> </span><br><span class="line">      WORDPRESS_DB_NAME: wordpress </span><br><span class="line">    volumes:</span><br><span class="line">      - wordpress:/var/www/html</span><br><span class="line">    restart: always</span><br><span class="line">    networks:</span><br><span class="line">      - blog</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  mysql-data:</span><br><span class="line">  wordpress:</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  blog:</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker安装</title>
      <link href="/posts/1fef.html"/>
      <url>/posts/1fef.html</url>
      
        <content type="html"><![CDATA[<h3 id="先移除旧版的docker如果没有就跳过"><a href="#先移除旧版的docker如果没有就跳过" class="headerlink" title="先移除旧版的docker如果没有就跳过"></a>先移除旧版的docker如果没有就跳过</h3><ul><li>移除旧版本docker \</li><li>sudo yum remove docker \</li><li>docker-client \</li><li>docker-client-latest \</li><li>docker-common \</li><li>docker-latest \</li><li>docker-latest-logrotate \</li><li>docker-logrotate \</li><li>docker-engine</li></ul><h3 id="配置docker-yum源"><a href="#配置docker-yum源" class="headerlink" title="配置docker yum源"></a>配置docker yum源</h3><ul><li>sudo yum install -y yum-utils</li><li>sudo yum-config-manager </li><li>–add-repo </li><li><a href="http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo">http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</a></li><li>如果已经配置好yum为阿里云的就不用了</li></ul><h2 id="安装-最新-docker"><a href="#安装-最新-docker" class="headerlink" title="安装 最新 docker"></a>安装 最新 docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</span><br></pre></td></tr></table></figure><h2 id="启动-amp-开机启动docker；-enable-start-二合一"><a href="#启动-amp-开机启动docker；-enable-start-二合一" class="headerlink" title="启动&amp; 开机启动docker； enable + start 二合一"></a>启动&amp; 开机启动docker； enable + start 二合一</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable docker --now</span><br></pre></td></tr></table></figure><h3 id="配置加速"><a href="#配置加速" class="headerlink" title="配置加速"></a>配置加速</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors那地址</span><br><span class="line">sudo mkdir -p /etc/docker</span><br><span class="line">sudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">&#123;</span><br><span class="line">&quot;registry-mirrors&quot;: [&quot;https://82m9ar63.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line">这个加速不太行使用另外一个</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker保存镜像</title>
      <link href="/posts/287f.html"/>
      <url>/posts/287f.html</url>
      
        <content type="html"><![CDATA[<h3 id="提交：docker-commit"><a href="#提交：docker-commit" class="headerlink" title="提交：docker commit"></a>提交：docker commit</h3><ul><li>docker commit [option] 名字就是用着的那个镜像的名字 新建镜像的名字:版本号</li><li>docker commit -m “update index.html” mynginx mynginx:1.0</li><li>-m 版本信息之类的</li></ul><h3 id="保存-docker-save"><a href="#保存-docker-save" class="headerlink" title="保存: docker save"></a>保存: docker save</h3><ul><li>docker save -o mynginx.tar mynginx:1.0</li><li>-o 保存出来就是本地</li></ul><h3 id="加载：docker-load"><a href="#加载：docker-load" class="headerlink" title="加载：docker load"></a>加载：docker load</h3><p>docker load -i mynginx.tar</p><p>-i就是读取本地的镜像</p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker分享社区</title>
      <link href="/posts/e3ea.html"/>
      <url>/posts/e3ea.html</url>
      
        <content type="html"><![CDATA[<h3 id="登录：docker-login"><a href="#登录：docker-login" class="headerlink" title="登录：docker login"></a>登录：docker login</h3><h3 id="命令-docker-tag"><a href="#命令-docker-tag" class="headerlink" title="命令:docker tag"></a>命令:docker tag</h3><ul><li>docker tag 原来的镜像名:版本号 新的镜像名:版本号</li><li>还可以docker tag mynginx:v1.0 lhx&#x2F;mynginx:v1.0 改名都是用户名&#x2F;镜像这样子的</li></ul><p>推荐在制作一个最新版本的镜像， 就是latest    docker tag mynginx:v1.0 lhx&#x2F;mynginx:latest然后在推送上去</p><h4 id="推送：docker-push-镜像名字"><a href="#推送：docker-push-镜像名字" class="headerlink" title="推送：docker push + 镜像名字"></a>推送：docker push + 镜像名字</h4><p><img src="/posts/e3ea.htm/Users\LHX\AppData\Roaming\Typora\typora-user-images\image-20241025115810209.png" alt="image-20241025115810209"></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker国内镜像源</title>
      <link href="/posts/ec39.html"/>
      <url>/posts/ec39.html</url>
      
        <content type="html"><![CDATA[<h3 id="进入配置文件registry-mirror：需要重启docker服务"><a href="#进入配置文件registry-mirror：需要重启docker服务" class="headerlink" title="进入配置文件registry mirror：需要重启docker服务"></a>进入配置文件registry mirror：需要重启docker服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker</span><br><span class="line">tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">&#123;</span><br><span class="line">&quot;registry-mirrors&quot;: [</span><br><span class="line">&quot;https://do.nark.eu.org&quot;,</span><br><span class="line">&quot;https://dc.j8.work&quot;,</span><br><span class="line">&quot;https://docker.m.daocloud.io&quot;,</span><br><span class="line">&quot;https://dockerproxy.com&quot;,</span><br><span class="line">&quot;https://docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">&quot;https://docker.nju.edu.cn&quot;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker存储</title>
      <link href="/posts/ade7.html"/>
      <url>/posts/ade7.html</url>
      
        <content type="html"><![CDATA[<ol><li>目录挂载：-v 外目录:内目录</li><li>例子：docker run -d -p 80:80 -v &#x2F;app&#x2F;nghtml:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html –name app01 nginx</li><li>卷映射 -v 卷名:内目录</li><li>例子：docker run -d -p 80:80 -v ngconf:&#x2F;etc&#x2F;nginx</li><li>卷的统一位置在：&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;</li><li>列出所有的卷docker volume ls</li><li>新建卷docker volume create hkjcpdd</li><li>查看卷的详情docker volume inspect ngconf</li></ol>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker架构</title>
      <link href="/posts/af13.html"/>
      <url>/posts/af13.html</url>
      
        <content type="html"><![CDATA[<h3 id="下载镜像的话就是，docker-pull-镜像-镜像-x3D-标签-版本-latest为最新"><a href="#下载镜像的话就是，docker-pull-镜像-镜像-x3D-标签-版本-latest为最新" class="headerlink" title="下载镜像的话就是，docker pull + 镜像  镜像&#x3D;标签+版本  latest为最新"></a>下载镜像的话就是，docker pull + 镜像  镜像&#x3D;标签+版本  latest为最新</h3><ul><li>下载别的版本的话就是去dockerhub上看</li><li>检索docker search + 名字</li><li>列表：docker images</li><li>删除：docker rmi + 完整标签或者id 如：docker rmi nginx:latest</li></ul><h3 id="运行镜像的话就是，docker-run-options-image-commend-arg…-后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加"><a href="#运行镜像的话就是，docker-run-options-image-commend-arg…-后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加" class="headerlink" title="运行镜像的话就是，docker run [options] image [commend] [arg…] 后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加"></a>运行镜像的话就是，docker run [options] image [commend] [arg…] 后面是启动命令但是一般的镜像都有自己的启动命令个所以可不加</h3><ul><li>后台运行-d</li><li>起个名字 –name</li><li>端口映射 -p 80:80</li></ul><ol><li><p>查看docker ps</p></li><li><p>停止 docker stop</p></li><li><p>启动docker start</p></li><li><p>重启 docker restart </p></li><li><p>状态 docker stats</p></li><li><p>日志 docker logs</p></li><li><p>进入 docker exec</p><p>docker exec -it mynginx &#x2F;bin&#x2F;bash</p><p>-it是交互模式</p><p>后面的路径是让他在控制台上显示</p></li><li><p>删除 docker rm</p></li><li><p>用docker构建自己的软件包，docker build + 镜像</p></li><li><p>发给应用市场让dock保存的话就是，dock push + 镜像</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker网络</title>
      <link href="/posts/5796.html"/>
      <url>/posts/5796.html</url>
      
        <content type="html"><![CDATA[<p>查看容器的细节</p><p>docker inspect app2</p><p>docker network + 功能</p><p><img src="/posts/5796.htm/Users\LHX\AppData\Roaming\Typora\typora-user-images\image-20241025120003627.png" alt="image-20241025120003627"></p><h4 id="环境变量就用-e"><a href="#环境变量就用-e" class="headerlink" title="环境变量就用-e"></a>环境变量就用-e</h4>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker保存镜像</title>
      <link href="/posts/287f.html"/>
      <url>/posts/287f.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yum配置</title>
      <link href="/posts/dfc4.html"/>
      <url>/posts/dfc4.html</url>
      
        <content type="html"><![CDATA[<h2 id="普通配置"><a href="#普通配置" class="headerlink" title="普通配置"></a>普通配置</h2><ul><li><p>关闭防火墙</p></li><li><p>然后关闭selinux</p></li><li><p>查看网卡 nmcli c s</p></li><li><p>刷新网卡 nmcli c u ens33</p></li><li><p>开启ssh远程连接 vi &#x2F;etc&#x2F;ssh&#x2F;sshd_config</p></li><li><p>刷新服务 systemctl restart sshd</p></li><li><p>先暂时挂载mount &#x2F;opt&#x2F;镜像 &#x2F;mnt&#x2F;</p></li><li><p>然后永久挂载echo &#x2F;opt&#x2F;镜像 &#x2F;mnt&#x2F; iso9660 defaults 0 0 &gt;&gt; &#x2F;etc&#x2F;fstab</p></li><li><p>然后进入&#x2F;etc&#x2F;yum.repo</p></li><li><p>全部删除</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/yum.repo</span><br><span class="line">[1]</span><br><span class="line">name=1</span><br><span class="line">baseurl=file:///mnt/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">yum repolist all</span><br></pre></td></tr></table></figure></li></ul><h3 id="阿里云源："><a href="#阿里云源：" class="headerlink" title="阿里云源："></a>阿里云源：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">先安装wget    yum install wget -y</span><br><span class="line">然后拿文件</span><br><span class="line">wget -O /etc/yum.repo.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">然后清理缓存还有生成新的缓存</span><br><span class="line">清理 yum clean all</span><br><span class="line">生成 yum makecache</span><br><span class="line">接下来下载epel库</span><br><span class="line">yum install epel-release -y</span><br><span class="line">然后重复一次清理和生成就可以了</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> yum配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 示例 </tag>
            
            <tag> yum配置 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
